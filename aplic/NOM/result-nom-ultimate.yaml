---
# Source: nomultimate/charts/idm/templates/idm-pdb.yaml
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: itom-idm
  namespace: default
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app: itom-idm-app
---
# Source: nomultimate/charts/itomdiminio/templates/minio-pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: itom-di-minio
  namespace: default
  labels:
    app: itomdiminio
spec:
  selector:
    matchLabels:
      app: itomdiminio
  maxUnavailable: 25%
---
# Source: nomultimate/charts/itomdipulsar/templates/bookkeeper/bookkeeper-pdb.yaml
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: "itomdipulsar-bookkeeper"
  namespace: default
  labels:
    app: itomdipulsar
    chart: itomdipulsar-2.8.1-31
    release: RELEASE-NAME
    heritage: Helm
    cluster: itomdipulsar
    app.kubernetes.io/name: itomdipulsar-bookkeeper
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: 2.8.1-31
    itom.microfocus.com/capability: itom-data-ingestion
    tier.itom.microfocus.com/backend: backend
    component: bookkeeper
spec:
  selector:
    matchLabels:
      app: itomdipulsar
      release: RELEASE-NAME
      component: bookkeeper
  maxUnavailable: 1
---
# Source: nomultimate/charts/itomdipulsar/templates/broker/broker-pdb.yaml
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: "itomdipulsar-broker"
  namespace: default
  labels:
    app: itomdipulsar
    chart: itomdipulsar-2.8.1-31
    release: RELEASE-NAME
    heritage: Helm
    cluster: itomdipulsar
    app.kubernetes.io/name: itomdipulsar-broker
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: 2.8.1-31
    itom.microfocus.com/capability: itom-data-ingestion
    tier.itom.microfocus.com/backend: backend
    component: broker
spec:
  selector:
    matchLabels:
      app: itomdipulsar
      release: RELEASE-NAME
      component: broker
  maxUnavailable: 1
---
# Source: nomultimate/charts/itomdipulsar/templates/proxy/proxy-pdb.yaml
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: "itomdipulsar-proxy"
  namespace: default
  labels:
    app: itomdipulsar
    chart: itomdipulsar-2.8.1-31
    release: RELEASE-NAME
    heritage: Helm
    cluster: itomdipulsar
    app.kubernetes.io/name: itomdipulsar-proxy
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: 2.8.1-31
    itom.microfocus.com/capability: itom-data-ingestion
    tier.itom.microfocus.com/backend: backend
    component: proxy
spec:
  selector:
    matchLabels:
      app: itomdipulsar
      release: RELEASE-NAME
      component: proxy
  maxUnavailable: 1
---
# Source: nomultimate/charts/itomdipulsar/templates/zookeeper/zookeeper-pdb.yaml
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

# deploy zookeeper only when `components.zookeeper` is true
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: "itomdipulsar-zookeeper"
  namespace: default
  labels:
    app: itomdipulsar
    chart: itomdipulsar-2.8.1-31
    release: RELEASE-NAME
    heritage: Helm
    cluster: itomdipulsar
    app.kubernetes.io/name: itomdipulsar-zookeeper
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: 2.8.1-31
    itom.microfocus.com/capability: itom-data-ingestion
    tier.itom.microfocus.com/backend: backend
    component: zookeeper
spec:
  selector:
    matchLabels:
      app: itomdipulsar
      release: RELEASE-NAME
      component: zookeeper
  maxUnavailable: 1
---
# Source: nomultimate/charts/autopass/templates/apls-rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: itom-autopass-lms
  namespace: default
imagePullSecrets:
- name: registrypullsecret
---
# Source: nomultimate/charts/bvd/templates/bvd-rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: bvd
  namespace: default
imagePullSecrets:
- name: registrypullsecret
---
# Source: nomultimate/charts/idm/templates/idm-rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: itom-idm
  namespace: default
imagePullSecrets:
- name: registrypullsecret
---
# Source: nomultimate/charts/itom-di-udx-scheduler/templates/scheduler-rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: itom-di-scheduler-sa
  namespace: default
imagePullSecrets:
- name: registrypullsecret
---
# Source: nomultimate/charts/itom-ingress-controller/templates/rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: itom-ingress-controller
  namespace: default
imagePullSecrets:
- name: registrypullsecret
---
# Source: nomultimate/charts/itom-reloader/templates/rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: itom-cdf-reloader
imagePullSecrets:
- name: registrypullsecret
---
# Source: nomultimate/charts/itom-vault/templates/vault-rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: itom-vault
  namespace: default
imagePullSecrets:
- name: registrypullsecret
---
# Source: nomultimate/charts/itomdiadministration/templates/di-administration-rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: itom-di-administration-sa
  namespace: default
imagePullSecrets:
- name: registrypullsecret
---
# Source: nomultimate/charts/itomdidataaccess/templates/data-access-rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: itom-di-data-access-sa
  namespace: default
imagePullSecrets:
- name: registrypullsecret
---
# Source: nomultimate/charts/itomdimetadataserver/templates/metadata-server-rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: itom-di-metadata-server-sa
  namespace: default
imagePullSecrets:
- name: registrypullsecret
---
# Source: nomultimate/charts/itomdiminio/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: itom-di-minio-sa
  labels:
    app: itomdiminio
    chart: itomdiminio-2.5.0-21
    release: RELEASE-NAME
    heritage: Helm
    cluster: itom-di-minio
    app.kubernetes.io/name:  itom-di-minio
    app.kubernetes.io/managed-by: RELEASE-NAME
    app.kubernetes.io/version: 2.5.0-21
    itom.microfocus.com/capability: minio
    tier.itom.microfocus.com/backend: backend
  namespace: "default"
imagePullSecrets:
- name: registrypullsecret
---
# Source: nomultimate/charts/itomdimonitoring/templates/gen-certs-rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata: 
  name: "itomdimonitoring-gen-certs-sa"
  namespace: default
imagePullSecrets:
  - name: registrypullsecret
---
# Source: nomultimate/charts/itomdimonitoring/templates/monitoring-rbac.yaml
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
apiVersion: v1
kind: ServiceAccount
metadata: 
  name: itom-di-monitoring-sa
  namespace: default
imagePullSecrets:
- name: registrypullsecret
---
# Source: nomultimate/charts/itomdipostload/templates/postload-rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: itom-di-postload-sa
  namespace: default
imagePullSecrets:
- name: registrypullsecret
---
# Source: nomultimate/charts/itomdipulsar/templates/broker/broker-service-account.yaml
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

apiVersion: v1
kind: ServiceAccount
imagePullSecrets:
- name: registrypullsecret
metadata:
  name: itomdipulsar-broker-sa
  namespace: default
  labels:
    app: itomdipulsar
    chart: itomdipulsar-2.8.1-31
    release: RELEASE-NAME
    heritage: Helm
    cluster: itomdipulsar
    app.kubernetes.io/name: itomdipulsar-broker
    app.kubernetes.io/managed-by: RELEASE-NAME
    app.kubernetes.io/version: 2.8.1-31
    itom.microfocus.com/capability: itom-data-ingestion
    tier.itom.microfocus.com/backend: backend
    component: broker
  annotations:
---
# Source: nomultimate/charts/itomdipulsar/templates/coso/pulsar-common-rbac.yaml
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
apiVersion: v1
kind: ServiceAccount
metadata:
  name: itomdipulsar-sa
  namespace: default
imagePullSecrets:
- name: registrypullsecret
---
# Source: nomultimate/charts/itomdipulsar/templates/proxy/proxy-service-account.yaml
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

apiVersion: v1
kind: ServiceAccount
metadata:
  name: itomdipulsar-proxy-sa
  namespace: default
  labels:
    app: itomdipulsar
    chart: itomdipulsar-2.8.1-31
    release: RELEASE-NAME
    heritage: Helm
    cluster: itomdipulsar
    app.kubernetes.io/name: itomdipulsar-proxy
    app.kubernetes.io/managed-by: RELEASE-NAME
    app.kubernetes.io/version: 2.8.1-31
    itom.microfocus.com/capability: itom-data-ingestion
    tier.itom.microfocus.com/backend: backend
    component: proxy
imagePullSecrets:
- name: registrypullsecret
---
# Source: nomultimate/charts/itomnomcosodataaccess/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: itom-nom-coso-dl
  namespace: default
imagePullSecrets:
  - name: registrypullsecret
---
# Source: nomultimate/charts/nomapiserver/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: itom-nom-api-server
  namespace: default
imagePullSecrets:
  - name: registrypullsecret
---
# Source: nomultimate/charts/nomcore/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nom-core
  namespace: default
imagePullSecrets:
  - name: registrypullsecret
---
# Source: nomultimate/charts/nomhttp/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: itom-nom-http-backend
  namespace: default
imagePullSecrets:
  - name: registrypullsecret
---
# Source: nomultimate/charts/nommetricstransform/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: itom-nom-metrics
  namespace: default
imagePullSecrets:
  - name: registrypullsecret
---
# Source: nomultimate/charts/nomreportingcontent/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: itom-nom-reporting
  namespace: default
imagePullSecrets:
  - name: registrypullsecret
---
# Source: nomultimate/charts/nomxui/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nom-ui
  namespace: default
imagePullSecrets:
  - name: registrypullsecret
---
# Source: nomultimate/charts/nomzk/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: itom-nom-zookeeper
  namespace: default
imagePullSecrets:
  - name: registrypullsecret
---
# Source: nomultimate/charts/bvd/templates/bvd-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  labels:
    app.kubernetes.io/managed-by: Helm
  annotations:
    meta.helm.sh/release-name : RELEASE-NAME
    meta.helm.sh/release-namespace: default
  name: bvd-secret
type: Opaque
---
# Source: nomultimate/charts/idm/templates/idm-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: itom-idm
  namespace: default
data:
  settings: IyBpZG0uc2FtbC5rZXlzdG9yZS5kZWZhdWx0S2V5LnBhc3N3b3JkOgojIGlkbS5zYW1sLmtleXN0b3JlLmRlZmF1bHRQcml2YXRlS2V5OgojIGlkbS5zYW1sLmtleXN0b3JlLmRlZmF1bHRDZXJ0aWZpY2F0ZToKCiMgbHdzc29Db25maWcuZG9tYWluLm1vZGU6CiMgbHdzc29Db25maWcuY29va2llTmFtZToKIyBsd3Nzb0NvbmZpZy5pbml0U3RyaW5nOgojIGx3c3NvQ29uZmlnLmV4cGlyYXRpb25QZXJpb2Q6CiMgbHdzc29Db25maWcuc2VjdXJlSFRUUENvb2tpZToKIyBsd3Nzb0NvbmZpZy5kb21haW46CgojIGlkbS50b2tlbi5saWZldGltZS5taW51dGVzOgojIGlkbS5yZXF1ZXN0X3Rva2VuLmxpZmV0aW1lLm1pbnV0ZXM6CiMgaWRtLnRva2VuLmxpZmV0aW1lLm1pbnV0ZXNGb3JNb2JpbGU6CiMgaWRtLnJlcXVlc3RfdG9rZW4ubGlmZXRpbWUubWludXRlc0Zvck1vYmlsZToK
---
# Source: nomultimate/charts/itom-di-udx-scheduler/templates/scheduler-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: scheduler-secret
data:
---
# Source: nomultimate/charts/itom-ingress-controller/templates/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: nginx-default-secret
  namespace: default
type: Opaque
---
# Source: nomultimate/charts/itom-vault/templates/vault-credential.yaml
apiVersion: v1
kind: Secret
metadata:
  name: vault-credential
  namespace: default
type: Opaque
---
# Source: nomultimate/charts/itom-vault/templates/vault-instance-id.yml
apiVersion: v1
kind: Secret
metadata:
  name: vault-instance-id
  namespace: default
type: Opaque
---
# Source: nomultimate/charts/itom-vault/templates/vault-passphrase.yaml
apiVersion: v1
kind: Secret
metadata:
  name: vault-passphrase
  namespace: default
type: Opaque
---
# Source: nomultimate/charts/itom-vault/templates/vault-root-cert.yml
apiVersion: v1
kind: Secret
metadata:
  name: vault-root-cert
  namespace: default
type: Opaque
---
# Source: nomultimate/charts/itomdiadministration/templates/di-administration-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: administration-secret
data:
---
# Source: nomultimate/charts/itomdiadministration/templates/di-configuration-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: configuration-secret
data:
---
# Source: nomultimate/charts/itomdidataaccess/templates/data-access-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: data-access-secret
data:
---
# Source: nomultimate/charts/itomdimetadataserver/templates/metadata-server-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: metadata-server-secret
data:
---
# Source: nomultimate/charts/itomdiminio/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: itom-di-minio-secret
  labels:
    app: itomdiminio
    chart: itomdiminio-2.5.0-21
    release: RELEASE-NAME
    heritage: Helm
type: Opaque
data:
  accesskey: "QUtJQUlPU0ZPRE5ON0VYQU1QTEU="
  secretkey: "d0phbHJYVXRuRkVNSS9LN01ERU5HL2JQeFJmaUNZRVhBTVBMRUtFWQ=="
---
# Source: nomultimate/charts/itomdimonitoring/templates/vertica-prom-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: vert-prom-secret
data:
---
# Source: nomultimate/charts/itomdipostload/templates/postload-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: itom-di-postload-secret
data:
---
# Source: nomultimate/charts/itomdipulsar/templates/tls/tls-certs-client-ca.yaml
apiVersion: v1
kind: Secret
metadata:
  name: itomdipulsar-client-ca-certs-secret
data:
---
# Source: nomultimate/templates/secret-test.yaml
apiVersion: v1
kind: Secret
metadata:
  name: nom-secret
type: kubernetes.io/service-account-token
data:
  # Você pode incluir pares chave-valor adicionais, da mesma forma que faria com
  # Secrets do tipo Opaque
  APLS_DB_PASSWD_KEY: dGVzdGUK
  BVD_DB_PASSWD_KEY: dGVzdGUK
  IDM_DB_PASSWD_KEY: dGVzdGUK
  NOM_DB_PASSWD_KEY: dGVzdGUK
  BTCD_DB_PASSWD_KEY: dGVzdGUK
  ITOMDI_DBA_PASSWORD_KEY: dGVzdGUK
  ITOMDI_ROUSER_PASSWORD_KEY: dGVzdGUK
  ITOMDI_MONITOR_PWD_KEY: dGVzdGUK
  idm_nom_admin_password: dGVzdGUK
  NNM_SYS_PASSWD_KEY: dGVzdGUK
---
# Source: nomultimate/charts/bvd/templates/bvd-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: bvd-config
  namespace: default
  labels:
    app: bvd
    app.kubernetes.io/name: bvd-config
    app.kubernetes.io/managed-by: bvd-config
    app.kubernetes.io/version: 11.8.11
    itom.microfocus.com/capability: bvd
    tier.itom.microfocus.com/config: config
data:
  bvd.approle: "default"
  bvd.approleid: "WhyIsThisRequired"
  
  bvd.defaultVerticaUser: "vertica_rouser"
  bvd.defaultVerticaPW.key: "ITOMDI_ROUSER_PASSWORD_KEY"
  bvd.defaultVerticaHost: "pxl0nnmi0022.disposistivos.bb.com.br"
  bvd.defaultVerticaDB: "verticadb"
  bvd.defaultVerticaPort: "5443"
  bvd.defaultVerticaTLS: "false"
  bvd.exploreContextRoot: ""
  bvd.bvdContextRoot: ""
  bvd.redisport: "6380"
  bvd.redishost: "bvd-redis"
  bvd.redispassword.key: "redis_pwd"
  bvd.adminname.key: "bvd_admin_login"
  bvd.adminpassword.key: "bvd_admin_password"
  bvd.adminrole: "bvd_admin"
  bvd.apikey.key: "bvd_api_key"
  bvd.smtppassword.key: "schedule_mail_password_key"
  bvd.smtpsecurity: "TLS"
  bvd.UseTLS: "true"
  bvd.collectPrometheusMetrics: "true"
  bvd.systemUID: "1999"
  bvd.systemGID: "1999"
  

  bvd.dbCa.base64: ""
  bvd.dbCa.base64.key: "bvd_db_cert"
  bvd.dbhost: "silo12-master.postgresql.bdh.servicos.bb.com.br"
  bvd.dbport: "5432"
  bvd.dbType: "Postgresql"
  bvd.dbname: "bvd"
  bvd.dbsid: ""
  bvd.dbConnectionString: ""
  bvd.createDB: "false"
  bvd.dbuser: "bvd"
  bvd.dbpassword.key: "BVD_DB_PASSWD_KEY"
  bvd.dbUseTLS: "false"
  bvd.timeFormat: "24"
  bvd.reqTimeout: ""
  suite.suitelogo: "data:image/svg+xml;base64,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"
  suite.suitefavicon: "/externalcomponents/nom/widgets/assets/favicon.ico"
  suite.suitename: "Network Operations Management"
  suite.logo_large: "/externalcomponents/nom/widgets/assets/nomlogo.svg"
  suite.release: "2021.11"
  bvd.externalname: bvd.homologacao.nuvem.hm.bb.com.br
  bvd.externalport: "443"
  bvd.featureToggles: "{\"DISCARD_CHANGES_DIALOG\":true}"
  bvd.defaultFoundationRoles: |
        [
          {
            "name": "__unowned",
            "description": "Default permission for all unowned pages.",
            "permission": []
          },
          {
            "name": "__ungrouped",
            "description": "Default permission for all un-grouped pages.",
            "permission": []
          },
          {
              "name": "__grouped",
              "description": "Default permission for all rbac group pages.",
              "permission": []
          }
        ]
---
# Source: nomultimate/charts/bvd/templates/bvd-services-configmap.yaml
# Config information of services that BVD/UIS is dependent on
apiVersion: v1
kind: ConfigMap
metadata:
  name: bvd-services-config
  namespace: default
  labels:
    app: bvd
    app.kubernetes.io/name: bvd-services-config
    app.kubernetes.io/managed-by: bvd-services-config
    app.kubernetes.io/version: 11.8.11
    itom.microfocus.com/capability: bvd
    tier.itom.microfocus.com/config: config
data:
  suite.namespace: default
  suite.vault_addr: https://itom-vault:8200
  suite.autopasslicenseserver: https://itom-autopass-lms:80
  suite.integration_user: "integration_admin"
  suite.integration_user_password_key: "idm_integration_admin_password"
  suite.idm_transport_key: "idm_transport_admin_password"
  suite.idm_transport_user: "transport_admin"
  suite.idm_organization: "Provider"
  suite.idm_addr: https://itom-idm-svc:80
  suite.idm_external_url: https://bvd.homologacao.nuvem.hm.bb.com.br:80
---
# Source: nomultimate/charts/itom-di-udx-scheduler/templates/scheduler-configmap.yaml
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

apiVersion: v1
kind: ConfigMap
metadata:
  name: "itom-di-udx-scheduler-scheduler"
  namespace: 
  labels:
    app: itom-di-udx-scheduler
    chart: itom-di-udx-scheduler-2.5.0-33
    release: RELEASE-NAME
    heritage: Helm
    component: scheduler
    cluster: itom-di-udx-scheduler
data:
  threads: "6"
  vertica.datasource.host: "pxl0nnmi0022.disposistivos.bb.com.br"
  vertica.datasource.port: "5443"
  vertica.datasource.database: "verticadb"
  vertica.datasource.username: "vertica_rwuser"
  vertica.datasource.resourcepool: itom_di_stream_respool_provider_default
  vertica.datasource.password.key: "ITOMDI_DBA_PASSWORD_KEY"
  vertica.datasource.readonly.username: "vertica_rouser"
  vertica.datasource.is.tls.enabled: "false"
  batch.duration: "4"
  batch.units: seconds
  copyRead.timeout: "500"
  copyRead.units: milliseconds
  receiverQueue.size: "2000"
  heartbeat.timeout: "1"
  #tenant and deployment for schema name
  di.tenant: provider  
  di.deployment: default
  scheduler.rpm.version: 2.5.0-33
  scheduler.enable.rpm.version.check: "true"
  scheduler.log.level: info
  scheduler.udx.log.level: error
  scheduler.enable.performance.test: "false"
  scheduler.jdbc.opt: ""
  scheduler.explicit.statement.execute: "false"
  scheduler.pulsar.topic.partition.auto.refresh: "false"
  scheduler.parser.parameters: "flatten_arrays=True,flatten_maps=True"

  scheduler.configserver.hostname: "itom-di-administration-svc"
  scheduler.configserver.port: "18443"
  scheduler.configserver.connection.retry.delay.ms: "30000"
  scheduler.configserver.client.heart.beat: "20000"
  scheduler.configserver.server.heart.beat: "30000"
  scheduler.configserver.message.buffer.size.limit.mb: "2"
  scheduler.failed.event.initial.delay.millis: "60000"
  scheduler.failed.event.retry.delay.millis: "60000"
  scheduler.enable.restart.scheduler.job: "false"
  scheduler.max.memory.usage.percentage: "65.0"
  scheduler.data.retention.job.cron.schedule: "0 0 0 * * *"
  scheduler.enable.data.retention.job: "true"
  scheduler.enable.udx.has.message.check: "false"
  scheduler.load.method: "AUTO"
  scheduler.enable.node.parallelism: "true"
  scheduler.enable.frame.backlog.check: "true"
  scheduler.udx.ack.grouping.time.millis: "0"
  scheduler.udx.max.message.count: "0"
  scheduler.udx.max.stream.size.bytes: "0"
  scheduler.staggered.sleep.duration.millis: "0"
  scheduler.supported.vertica.versions: "v10.1.0,v10.1.1"
  scheduler.max.minutes.past.last.batch.update: "5.0"
  scheduler.restart.time.interval.in.hours: "6"
  scheduler.frame.duration: "00:00:30"
  scheduler.udx.pulsar.client.cleanup: "false"
  scheduler.lane.worker.connection.close.threshold: "15"
  scheduler.vdb.connection.close.threshold: "100"
  scheduler.udx.pulsar.client.cleanup.threshold: "20"
  scheduler.alive.check.time.interval.seconds: "60"
  scheduler.enable.microbatch.backlog.check: "true"
  scheduler.enable.subscription.cleaner: "true"
  pulsar.tenant: public
  pulsar.namespace: default
  pulsar.broker.service.port: "6650"
  pulsar.broker.service.port.tls: "6651"
  pulsar.web.service.port: "8080"
  pulsar.web.service.port.tls: "8443"
  pulsar.tls.hostname.verification: "false"
  pulsar.service.name: "itomdipulsar-broker"
  pulsar.auth.class: "org.apache.pulsar.client.impl.auth.AuthenticationTls"
  pulsar.enable.message.acknowledgement: "true"

  pulsar.datasource.host: ""
  pulsar.datasource.port: "31051"
  pulsar.admin.datasource.host: itomdipulsar-proxy
  pulsar.admin.datasource.port: "8443"
  administration.datasource.host: itom-di-administration-svc
  administration.datasource.port: "18443"
  scheduler.retention: "90"
  SCHED_UID: "1999"
  SCHED_GID: "1999"
  SCHED_PULSAR_UDX_TABLE: microbatch_history
  SCHED_SMALL_TOPIC_SIZE: "0"

  scheduler.logconfig.from.configmap: "true"
  scheduler.logconfig.log.to.file: "true"
---
# Source: nomultimate/charts/itom-di-udx-scheduler/templates/scheduler-logback-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: scheduler-logback-cm
data:
  logback.xml: |-
        <?xml version="1.0" encoding="UTF-8"?>
        
        <configuration scan="true" scanPeriod="30 seconds">
        
            <property name="FILE_LOG_PATTERN"
                      value="%d{yyyy-MM-dd HH:mm:ss.SSS} %5p ${PID:- } [%t] [%X{ConfigurationDetails}]--- %-40.40logger{39} %X{Context}: %m%n"/>
        
            <appender name="ConsoleAppender" class="ch.qos.logback.core.ConsoleAppender">
        
                <encoder>
                    <pattern>${FILE_LOG_PATTERN}</pattern>
                </encoder>
            </appender>
        
            <appender name="LogFileAppender" class="ch.qos.logback.core.rolling.RollingFileAppender">
                <!--See also http://logback.qos.ch/manual/appenders.html#RollingFileAppender-->
                <Append>true</Append>
                <File>${SCHEDULER_NFS_LOG_DIR}/scheduler.log</File>
                <encoder>
                    <pattern>${FILE_LOG_PATTERN}</pattern>
                </encoder>
                <rollingPolicy class="ch.qos.logback.core.rolling.FixedWindowRollingPolicy">
                    <maxIndex>5</maxIndex>
                    <FileNamePattern>${SCHEDULER_NFS_LOG_DIR}/scheduler.log.%i</FileNamePattern>
                </rollingPolicy>
                <triggeringPolicy class="ch.qos.logback.core.rolling.SizeBasedTriggeringPolicy">
                    <MaxFileSize>5MB</MaxFileSize>
                </triggeringPolicy>
            </appender>
        
            <logger name="com.microfocus" level="${SCHEDULER_LOG_LEVEL}" additivity="false">
                <appender-ref ref="LogFileAppender"/>
            </logger>
        
            <logger name="com.vertica.solutions.kafka" level="${SCHEDULER_LOG_LEVEL}" additivity="false">
                <appender-ref ref="LogFileAppender"/>
            </logger>
        
            <root level="WARN">
                <appender-ref ref="LogFileAppender"/>
            </root>
        </configuration>
---
# Source: nomultimate/charts/itom-ingress-controller/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: itom-ingress-controller-conf
  namespace: default
data:
  body-size: 4096m
  enable-vts-status: "true"
  map-hash-bucket-size: "32"
  proxy-body-size: 20m
  proxy-buffer-size: 256k
  proxy-read-timeout: "720"
  proxy-send-timeout: "720"
  server-name-hash-bucket-size: "128"
  server-name-hash-max-size: "512"
  server-tokens: "false"
  ssl-ciphers: ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES128-SHA256:ECDHE-ECDSA-AES128-SHA256:ECDHE-RSA-AES256-SHA384:ECDHE-ECDSA-AES256-SHA384:DHE-RSA-AES128-SHA256:DHE-RSA-AES256-SHA256:AES128-SHA256:AES256-SHA256
  ssl-protocols: TLSv1.2
  worker-processes: "4"
---
# Source: nomultimate/charts/itomdiadministration/templates/di-administration-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: itom-di-administration-cm
data:
  admin.jvm.args: -Xmx1024m -XX:+HeapDumpOnOutOfMemoryError
  admin.client.auth.enabled: "true"
  admin.use.external.ca.signed.certs: "true"
  admin.service.base.url:  itom-di-administration-svc:8443
  admin.system.level.throttle: "true"
  admin.permissible.request.limit: "6000"
  admin.request.throttle.time: "1"
  admin.config.store.type: db
  # Following db details required if config store type is db
  admin.db.ssl.enable: "false"
  admin.config.store.hostnames: "pxl0nnmi0022.disposistivos.bb.com.br"
  admin.config.store.username: "vertica_rwuser"
  admin.config.store.token.key: "ITOMDI_DBA_PASSWORD_KEY"
  # Config store type can be filesystem | db
  admin.config.store.db: "verticadb"
  admin.config.store.port: "5443"

  admin.logconfig.from.configmap:  "true"
  admin.logconfig.log.to.file: "true"

  vertica.resource.pool.name: itom_di_stream_respool

  #tenant and deployment for schema name
  di.tenant: provider  
  di.deployment: default

  # If partition count for topic is less than 1 then,
  # partition count for topic will be set based on the number of brokers configured at that instant
  admin.topic.partition.count: "3"

  # Message bus
  admin.message.bus: "pulsar"

  # pulsar parameters
  pulsar.service.name: "itomdipulsar-broker"
  pulsar.tls.enable: "true"
  pulsar.tenant: "public"
  pulsar.namespace: "default"
  pulsar.web.service.port: "8080"
  pulsar.web.service.port.tls: "8443"
  pulsar.broker.service.port: "6650"
  pulsar.broker.service.port.tls: "6651"
  pulsar.auth.enable: "true"
  pulsar.auth.class: "org.apache.pulsar.client.impl.auth.AuthenticationTls"
  pulsar.tls.hostname.verification: "false"

  hikaricp.connection.timeout: "60000"
  hikaricp.connection.maximum.lifetime: "600000"
  hikaricp.minimum.idle.connections: "1"
  hikaricp.maximum.pool.size: "5"
  hikaricp.connection.pool.name: store_once_config_pool
  dataset.status.polling.timeout.in.seconds: "130"
  admin.express.load.enable: "true"
  admin.minio.host: itom-di-minio-svc
  admin.minio.port: "9000"
  admin.minio.nodePort: "30006"
  admin.minio.ssl.enable: "true"
  admin.minio.externalAccessHost: itom-di-administration.homologacao.nuvem.hm.bb.com.br

  #
  #admin.minio.accessKey: accessKey
  #

  #
  #admin.minio.secretKey: secretKey
  #


  admin.expressLoad.workerCount: "25"
  admin.expressLoad.pollingIntervalInSec: "30"
  admin.expressLoad.csvObjectLimitForDirectInMB: "100"
  admin.expressLoad.gzipObjectLimitForDirectInMB: "5"
  admin.expressLoad.vertica.connectionPoolSize: "25"
  admin.expressLoad.vertica.connectionLoadbalancing: "true"
  admin.expressLoad.objectSizeLimitInMB: "1024"
  admin.expressLoad.parameters.enforceLength: "true"
  admin.expressLoad.parameters.abortOnError: "true"
  admin.expressLoad.parameters.rejectOnEmptyKey: "true"
  admin.expressLoad.parameters.rejectOnDataMismatch: "true"
  admin.expressLoad.parameters.maxRejections: "0"
  admin.expressLoad.parameters.header: "true"
  admin.expressLoad.verticaResourcePool: itom_di_express_load_respool_provider_default
  admin.auditLogs.enabled: "true"
---
# Source: nomultimate/charts/itomdiadministration/templates/di-administration-logback-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: administration-logback-cm
data:
  logback.xml: |-
        <?xml version="1.0" encoding="UTF-8"?>
        
        <configuration scan="true" scanPeriod="30 seconds">
        
          <property name="FILE_LOG_PATTERN"
                    value="%d{yyyy-MM-dd HH:mm:ss.SSS} %5p ${PID:- } [%t] --- %-40.40logger{39} %X{Context} [%X{DataSetID} %X{Event} %X{ConfigurationType} %X{AggregateName} %X{AggregateType}]: %m%n"/>
        
          <appender name="LogFileAppender" class="ch.qos.logback.core.rolling.RollingFileAppender">
            <append>true</append>
            <file>${ADMINISTRATION_LOG_PATH}/administration.log</file>
            <encoder>
              <pattern>${FILE_LOG_PATTERN}</pattern>
            </encoder>
            <rollingPolicy class="ch.qos.logback.core.rolling.FixedWindowRollingPolicy">
              <minIndex>1</minIndex>
              <maxIndex>10</maxIndex>
              <fileNamePattern>${ADMINISTRATION_LOG_PATH}/administration.log.%i</fileNamePattern>
              </rollingPolicy>
              <triggeringPolicy class="ch.qos.logback.core.rolling.SizeBasedTriggeringPolicy">
                <maxFileSize>10MB</maxFileSize>
              </triggeringPolicy>
          </appender>
          <appender name="MigrationLogFileAppender" class="ch.qos.logback.core.rolling.RollingFileAppender">
            <Append>true</Append>
            <file>${ADMINISTRATION_LOG_PATH}/fileToDbMigration.log</file>
            <encoder>
              <pattern>${FILE_LOG_PATTERN}</pattern>
            </encoder>
            <rollingPolicy class="ch.qos.logback.core.rolling.FixedWindowRollingPolicy">
              <minIndex>1</minIndex>
              <maxIndex>10</maxIndex>
              <fileNamePattern>${ADMINISTRATION_LOG_PATH}/fileToDbMigration.log.%i</fileNamePattern>
             </rollingPolicy>
             <triggeringPolicy class="ch.qos.logback.core.rolling.SizeBasedTriggeringPolicy">
               <maxFileSize>10MB</maxFileSize>
             </triggeringPolicy>
            </appender>
            <appender name="ConsoleAppender" class="ch.qos.logback.core.ConsoleAppender">
                <encoder>
                    <pattern>${FILE_LOG_PATTERN}</pattern>
                </encoder>
            </appender>
        
            <appender name="AuditLogFileAppender" class="ch.qos.logback.core.rolling.RollingFileAppender">
                <append>true</append>
                <file>${ADMINISTRATION_AUDIT_LOG_PATH}/administration-audit.log</file>
                <encoder>
                    <pattern>${FILE_LOG_PATTERN}</pattern>
                </encoder>
                <rollingPolicy class="ch.qos.logback.core.rolling.FixedWindowRollingPolicy">
                    <minIndex>1</minIndex>
                    <maxIndex>10</maxIndex>
                    <fileNamePattern>${ADMINISTRATION_LOG_PATH}/administration.log.%i</fileNamePattern>
                </rollingPolicy>
                <triggeringPolicy class="ch.qos.logback.core.rolling.SizeBasedTriggeringPolicy">
                    <maxFileSize>10MB</maxFileSize>
                </triggeringPolicy>
            </appender>
        
            <logger name="com.swgrp.itomdi.administration.service.fileSystemToDbMigration" level="INFO" additivity="false">
                <appender-ref ref="MigrationLogFileAppender"/>
            </logger>
        
        
            <logger name="com.swgrp.itomdi" level="INFO" additivity="false">
                <appender-ref ref="LogFileAppender"/>
            </logger>
        
        
            <logger name="org.hibernate.internal.util.EntityPrinter" level="ERROR" additivity="false">
                <appender-ref ref="LogFileAppender"/>
            </logger>
        
        
            <logger name="org.hibernate" level="ERROR" additivity="false">
                <appender-ref ref="LogFileAppender"/>
            </logger>
        
        
            <logger name="org.springframework.transaction.support.AbstractPlatformTransactionManager" level="ERROR" additivity="false">
                <appender-ref ref="LogFileAppender"/>
            </logger>
        
        
            <logger name="org.springframework.orm.hibernate5.HibernateTransactionManager" level="ERROR" additivity="false">
                <appender-ref ref="LogFileAppender"/>
            </logger>
        
        
            <logger name="org.springframework.orm.jpa.JpaTransactionManager" level="ERROR" additivity="false">
                <appender-ref ref="LogFileAppender"/>
            </logger>
        
        
            <logger name="org.springframework.jdbc.datasource.DataSourceTransactionManager" level="ERROR" additivity="false">
                <appender-ref ref="LogFileAppender"/>
            </logger>
        
            <logger name="com.swgrp.itomdi.administration.interceptors.LoggerInterceptor" level="INFO" additivity="false">
                <appender-ref ref="AuditLogFileAppender"/>
            </logger>
            <logger name="com.swgrp.itomdi.administration.interceptors.ThrottleInterceptor" level="INFO" additivity="false">
                <appender-ref ref="AuditLogFileAppender"/>
                <appender-ref ref="LogFileAppender"/>
            </logger>
        
            <logger name="com.swgrp.itomdi.administration.service.eventListeners" level="INFO" additivity="false">
                <appender-ref ref="AuditLogFileAppender"/>
            </logger>
        
            <logger name="com.swgrp.itomdi.administration.events.ValidationExceptionEventListener" level="INFO" additivity="false">
                <appender-ref ref="AuditLogFileAppender"/>
            </logger>
            <logger name="org.apache.tomcat.util" level="OFF"/>
            <logger name="org.apache.tomcat.websocket" level="OFF"/>
            <!--<logger name="org.apache.tomcat.util.net.NioEndpoint" level="DEBUG" additivity="false">
                <appender-ref ref="AuditLogFileAppender"/>
            </logger>-->
            <logger name="org.apache.tomcat.util.net.SecureNioChannel" level="DEBUG" additivity="false">
                <appender-ref ref="AuditLogFileAppender"/>
            </logger>
        
            <root level="WARN">
                <appender-ref ref="LogFileAppender"/>
            </root>
        
        </configuration>
---
# Source: nomultimate/charts/itomdidataaccess/templates/data-access-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: itom-di-data-access-cm
data:
  dataaccess.jvm.args: -Xms512m -Xmx1024m -XX:+HeapDumpOnOutOfMemoryError
  vertica.ingestion.service.name: itom-di-metadata-server-svc
  vertica.ingestion.service.port: "8443"
  vertica.datasource.driver-class-name: com.vertica.jdbc.Driver
  vertica.datasource.username: "vertica_rouser"
  vertica.datasource.connection-timeout: "60000"
  dataaccess.resource-pool: "GENERAL"
  vertica.datasource.hostname: "pxl0nnmi0022.disposistivos.bb.com.br"
  vertica.datasource.port: "5443"
  vertica.datasource.databasename: "verticadb"
  vertica.connection.max-reconnect-attempt-on-failure-during-startup: "200"
  vertica.connection.time-delay-between-retries-during-startup: "3000"
  dataaccess.client.auth.enable: "true"
  dataaccess.use.external.ca.signed.certs: "true"
  vertica.datasource.password.key: "ITOMDI_ROUSER_PASSWORD_KEY"
  dataaccess.vertica.ingestion.tls.enable: "true"
  dataaccess.vertica.ssl.enable: "false"
  hikaricp.connection.maximum.lifetime: "1800000"
  hikaricp.minimum.idle.connections: "2"
  hikaricp.idle.session.timeout: "3600s"
  hikaricp.maximum.pool.size: "10"
  hikaricp.connection.pool.name: store_once_pool
  timezone: ""

  dataaccess.logconfig.from.configmap: "true"
---
# Source: nomultimate/charts/itomdidataaccess/templates/data-access-logback-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: data-access-logback-cm
data:
  logback.xml: |-
        <?xml version="1.0" encoding="UTF-8"?>
        
        <configuration scan="true" scanPeriod="30 seconds">
        
            <property name="FILE_LOG_PATTERN" value="%d{yyyy-MM-dd HH:mm:ss.SSS} %5p ${PID:- } [%t] --- %-40.40logger{39} : %m%n"/>
            <appender name="LogFileAppender" class="ch.qos.logback.core.rolling.RollingFileAppender">
                <!--See also http://logback.qos.ch/manual/appenders.html#RollingFileAppender-->
                <Append>true</Append>
                <File>${DATAACCESS_HOME}/log/${POD_NAMESPACE}/${POD_NAMESPACE}__${POD_NAME}__${CONTAINER_NAME}__${NODE_NAME}/dataaccess.log</File>
                <encoder>
                    <pattern>${FILE_LOG_PATTERN}</pattern>
                </encoder>
                <rollingPolicy class="ch.qos.logback.core.rolling.FixedWindowRollingPolicy">
                    <maxIndex>5</maxIndex>
                    <FileNamePattern>${DATAACCESS_HOME}/log/${POD_NAMESPACE}/${POD_NAMESPACE}__${POD_NAME}__${CONTAINER_NAME}__${NODE_NAME}/archive/dataaccess.%i.log</FileNamePattern>
                </rollingPolicy>
                <triggeringPolicy class="ch.qos.logback.core.rolling.SizeBasedTriggeringPolicy">
                    <MaxFileSize>10MB</MaxFileSize>
                </triggeringPolicy>
            </appender>
            <root level="INFO">
                <appender-ref ref="LogFileAppender"/>
            </root>
        
        </configuration>
---
# Source: nomultimate/charts/itomdimetadataserver/templates/metadata-server-logback-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: metadata-server-logback-cm
data:
  logback.xml: |-
        <?xml version="1.0" encoding="UTF-8"?>
        <!-- Logback Version : 99 -->
        <configuration scan="true" scanPeriod="30 seconds">
        
            <property name="FILE_LOG_PATTERN" value="%d{yyyy-MM-dd HH:mm:ss.SSS} %5p ${PID:- } [%t] - %logger{0} %X{Context}: %m%n"/>
            <appender name="LogFileAppender" class="ch.qos.logback.core.rolling.RollingFileAppender">
                <!--See also http://logback.qos.ch/manual/appenders.html#RollingFileAppender-->
                <Append>true</Append>
                <File>${LOG_PATH}/metadata-server-app.log</File>
                <encoder>
                    <pattern>${FILE_LOG_PATTERN}</pattern>
                </encoder>
                <rollingPolicy class="ch.qos.logback.core.rolling.FixedWindowRollingPolicy">
                    <maxIndex>5</maxIndex>
                    <FileNamePattern>${LOG_PATH}/archive/metadata-server-app.%i.log</FileNamePattern>
                </rollingPolicy>
                <triggeringPolicy class="ch.qos.logback.core.rolling.SizeBasedTriggeringPolicy">
                    <MaxFileSize>5MB</MaxFileSize>
                </triggeringPolicy>
            </appender>
        
            <logger name="com.hpe.opsb.di" level="INFO" additivity="false">
                <appender-ref ref="LogFileAppender"/>
            </logger>
        
            <logger name="com.swgrp.itomdi.configuration.client" level="INFO" additivity="false">
                <appender-ref ref="LogFileAppender"/>
            </logger>
        
            <root level="WARN">
                <appender-ref ref="LogFileAppender"/>
            </root>
        </configuration>
---
# Source: nomultimate/charts/itomdiminio/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: itom-di-minio
  labels:
    app: itomdiminio
    chart: itomdiminio-2.5.0-21
    release: RELEASE-NAME
    heritage: Helm
data:
  initialize: |-
    #!/bin/sh
    set -e ; # Have script exit in the event of a failed command.
    
    # connectToMinio
    # Use a check-sleep-check loop to wait for Minio service to be available
    connectToMinio() {
      SCHEME=$1
      ATTEMPTS=0 ; LIMIT=29 ; # Allow 30 attempts
      set -e ; # fail if we can't read the keys.
      ACCESS=$(cat /config/accesskey) ; SECRET=$(cat /config/secretkey) ;
      set +e ; # The connections to minio are allowed to fail.
      echo "Connecting to Minio server: $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT" ;
      MC_COMMAND="mc config host add myminio $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT $ACCESS $SECRET" ;
      $MC_COMMAND ;
      STATUS=$? ;
      until [ $STATUS = 0 ]
      do
        ATTEMPTS=`expr $ATTEMPTS + 1` ;
        echo \"Failed attempts: $ATTEMPTS\" ;
        if [ $ATTEMPTS -gt $LIMIT ]; then
          exit 1 ;
        fi ;
        sleep 2 ; # 1 second intervals between attempts
        $MC_COMMAND ;
        STATUS=$? ;
      done ;
      set -e ; # reset `e` as active
      return 0
    }
    
    # checkBucketExists ($bucket)
    # Check if the bucket exists, by using the exit code of `mc ls`
    checkBucketExists() {
      BUCKET=$1
      CMD=$(/usr/bin/mc ls myminio/$BUCKET > /dev/null 2>&1)
      return $?
    }
    
    # createBucket ($bucket, $policy, $purge)
    # Ensure bucket exists, purging if asked to
    createBucket() {
      BUCKET=$1
      POLICY=$2
      PURGE=$3
    
      # Purge the bucket, if set & exists
      # Since PURGE is user input, check explicitly for `true`
      if [ $PURGE = true ]; then
        if checkBucketExists $BUCKET ; then
          echo "Purging bucket '$BUCKET'."
          set +e ; # don't exit if this fails
          /usr/bin/mc rm -r --force myminio/$BUCKET
          set -e ; # reset `e` as active
        else
          echo "Bucket '$BUCKET' does not exist, skipping purge."
        fi
      fi
    
      # Create the bucket if it does not exist
      if ! checkBucketExists $BUCKET ; then
        echo "Creating bucket '$BUCKET'"
        /usr/bin/mc mb myminio/$BUCKET
      else
        echo "Bucket '$BUCKET' already exists."
      fi
    
      # At this point, the bucket should exist, skip checking for existence
      # Set policy on the bucket
      echo "Setting policy of bucket '$BUCKET' to '$POLICY'."
      /usr/bin/mc policy set $POLICY myminio/$BUCKET
    }
    
    # Try connecting to Minio instance
    scheme=https
    connectToMinio $scheme
---
# Source: nomultimate/charts/itomdimonitoring/templates/vertica-prom-configmap.yaml
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

apiVersion: v1
kind: ConfigMap
metadata:
  name: "itomdimonitoring-verticapromexporter"
  namespace: default
  labels:
    app: itomdimonitoring
    chart: itomdimonitoring-2.5.0-61
    release: RELEASE-NAME
    heritage: Helm
    component: verticapromexporter
    cluster: itomdimonitoring
data:
  VERTICA_HOST: "pxl0nnmi0022.disposistivos.bb.com.br"
  VERTICA_PORT: "5443"
  VERTICA_DATABASE: "verticadb"
  VERTICA_USER: "vertica_rouser"
  VERTICA_USER_KEY: "ITOMDI_ROUSER_PASSWORD_KEY"
  ITOM_METADATA_SCHEMA: itom_di_metadata_provider_default
  VERTICA_SCHEDULER_SCHEMA: itom_di_scheduler_provider_default
  VERTICA_TLS_MODE: "none"
  EXPORTER_UID: "1999"
  EXPORTER_GID: "1999"
  QUERY_TIMEOUT_SEC: "5"
  SCRAPE_INTERVAL_SEC: "60"
  TABLE_PARALLELISM: "8"
  tlsTrustCertFilePath: /var/run/secrets/boostport.com/issue_ca.crt
  SCHEDULER_METRICS_TABLE: "stream_microbatch_history"
---
# Source: nomultimate/charts/itomdipostload/templates/postload-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: itom-di-postload-cm
data:
  ###Postload Properties###
  postload.task.topic: "di_postload_task_topic"
  postload.status.topic: "di_postload_task_status_topic"
  postload.state.topic: "di_internal_postload_state"
  postload.task.execution.interval.millis: "60000"
  postload.consumers.per.task.executor: "20"
  postload.use.receive.to.consume: "true"
  postload.enable.te.process.monitoring: "true"
  postload.enable.topic.monitoring: "false"
  postload.timezone: ""
  postload.task.controller.jvm.args: -Xms256m -Xmx1024m -XX:+HeapDumpOnOutOfMemoryError
  postload.task.executor.jvm.args: -Xms256m -Xmx1024m -XX:+HeapDumpOnOutOfMemoryError
  postload.enrichmentbatchsize: "2000000000"
  postload.postresourcepool: ""
  postload.scrape.interval.millis: "15000"
  postload.pulsar.namespace: "itomdipostload"
  postload.acceptable.missed.trigger.delay.seconds: "7200"

  ###Config Server Properties###
  config.store.db.enable: "true"
  config.server.hostname: "itom-di-administration-svc"
  config.server.port: "18443"
  config.server.connect.retry.delay.ms: "30000"
  config.server.client.heart.beat: "20000"
  config.server.server.heart.beat: "30000"
  config.server.message.buffer.size.limit.in.mb: "2"

  #csv directload configuration
  csvdirectload.compress.archive.files: "true"
  csvdirectload.compress.failed.files: "true"
  #set the below flag to false if you want to compress the archived files periodically instead of compressing each file
  csvdirectload.compress.each.file: "true"
  csvdirectload.cleanup.retention.period.days: "1"
  csvdirectload.cleanup.retention.size.mb: "-1"
  csvdirectload.cleanup.purge.controltable.after.delete: "false"


  ###Pulsar-Properties###
  postload.pulsar.service.name: "itomdipulsar-broker"
  postload.pulsar.broker.service.port: "6650"
  postload.pulsar.broker.service.port.tls: "6651"
  postload.pulsar.web.service.port: "8080"
  postload.pulsar.web.service.port.tls: "8443"
  postload.pulsar.tls.enable: "true"
  postload.pulsar.auth.enable: "true"
  postload.pulsar.auth.class: "org.apache.pulsar.client.impl.auth.AuthenticationTls"
  postload.pulsar.tls.hostname.verification: "false"
  postload.pulsar.connection.retry.interval.seconds: "30"

  ###Vertica Connection Properties###
  vertica.hostname: "pxl0nnmi0022.disposistivos.bb.com.br"
  vertica.username: "vertica_rwuser"
  vertica.password.key: "ITOMDI_DBA_PASSWORD_KEY"
  vertica.db: "verticadb"
  vertica.port: "5443"
  vertica.tlsEnabled: "false"
  vertica.tlsMode: "disable"
  vertica.connection.retry.interval.seconds: "30"
  di.tenant: "provider"
  di.deployment: "default"
  taskExecutor.logconfig.from.configmap:  "true"
  taskExecutor.logconfig.log.to.file: "true"
  taskGenerator.logconfig.from.configmap:  "true"
  taskGenerator.logconfig.log.to.file: "true"
---
# Source: nomultimate/charts/itomdipostload/templates/postload-taskcontroller-logback-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: taskcontroller-logback-cm
data:
  task-controller-logback.xml: |-
        <?xml version="1.0" encoding="UTF-8"?>
        
        <configuration scan="true" scanPeriod="30 seconds">
        
            <property name="FILE_LOG_PATTERN"
                      value="%d{yyyy-MM-dd HH:mm:ss.SSS} %5p ${PID:- } [%t] --- %-40.40logger{39} %X{Context}[%X{TaskDetails}]: %m%n"/>
        
            <appender name="LogFileAppender" class="ch.qos.logback.core.rolling.RollingFileAppender">
                <!--See also http://logback.qos.ch/manual/appenders.html#RollingFileAppender-->
                <Append>true</Append>
                <File>${TG_NFS_LOG_DIR}/taskcontroller.log</File>
                <encoder>
                    <pattern>${FILE_LOG_PATTERN}</pattern>
                </encoder>
                <rollingPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy">
                    <FileNamePattern>${TG_NFS_LOG_DIR}/taskcontroller-%d{yyyy-MM-dd}.log.%i.gz</FileNamePattern>
                    <!-- each file should be at most 100MB, keep 60 days of history, but at most 2GB -->
                    <maxFileSize>100MB</maxFileSize>
                    <maxHistory>60</maxHistory>
                    <totalSizeCap>2GB</totalSizeCap>
                </rollingPolicy>
            </appender>
        
            <appender name="ConsoleAppender" class="ch.qos.logback.core.ConsoleAppender">
                <encoder>
                    <pattern>${FILE_LOG_PATTERN}</pattern>
                </encoder>
            </appender>
        
            <logger name="com.microfocus" level="INFO" additivity="false">
                <appender-ref ref="LogFileAppender"/>
            </logger>
            <logger name="com.swgrp.itomdi" level="INFO" additivity="false">
                <appender-ref ref="LogFileAppender"/>
            </logger>
        
            <logger name="org.springframework.boot.autoconfigure.flyway" level="DEBUG" additivity="false">
                <appender-ref ref="LogFileAppender"/>
            </logger>
        
            <root level="WARN">
                <appender-ref ref="LogFileAppender"/>
            </root>
        </configuration>
---
# Source: nomultimate/charts/itomdipostload/templates/postload-taskexecutor-logback-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: taskexecutor-logback-cm
data:
  task-executor-logback.xml: |-
        <?xml version="1.0" encoding="UTF-8"?>
        
        <configuration scan="true" scanPeriod="30 seconds">
        
            <property name="FILE_LOG_PATTERN"
                      value="%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %logger{60} [%X{TaskDetails}]- %msg%n"/>
        
            <property name="TASK_SPECIFIC_FILE_LOG_PATTERN"
                      value="%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] [%X{TaskDetails}]- %msg%n"/>
        
        
        
            <appender name="LogFileAppender" class="ch.qos.logback.core.rolling.RollingFileAppender">
                <!--See also http://logback.qos.ch/manual/appenders.html#RollingFileAppender-->
                <Append>true</Append>
                <File>${TE_NFS_LOG_DIR}/taskexecutor.log</File>
                <encoder>
                    <pattern>${FILE_LOG_PATTERN}</pattern>
                </encoder>
                <rollingPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy">
                    <!-- each file should be at most 100MB, keep 60 days of history, but at most 2GB -->
                    <maxFileSize>100MB</maxFileSize>
                    <maxHistory>60</maxHistory>
                    <totalSizeCap>2GB</totalSizeCap>
                    <FileNamePattern>${TE_NFS_LOG_DIR}/taskexecutor-%d{yyyy-MM-dd}.log.%i.gz</FileNamePattern>
                </rollingPolicy>
            </appender>
        
            <appender name="CsvDirectLoadAppender" class="ch.qos.logback.core.rolling.RollingFileAppender">
                <!--See also http://logback.qos.ch/manual/appenders.html#RollingFileAppender-->
                <Append>true</Append>
                <File>${TE_NFS_LOG_DIR}/csv-direct-load.log</File>
                <encoder>
                    <pattern>${TASK_SPECIFIC_FILE_LOG_PATTERN}</pattern>
                </encoder>
                <rollingPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy">
                    <!-- each file should be at most 100MB, keep 60 days of history, but at most 2GB -->
                    <maxFileSize>100MB</maxFileSize>
                    <maxHistory>60</maxHistory>
                    <totalSizeCap>2GB</totalSizeCap>
                    <FileNamePattern>${TE_NFS_LOG_DIR}/csv-direct-load-%d{yyyy-MM-dd}.log.%i.gz</FileNamePattern>
                </rollingPolicy>
            </appender>
        
            <appender name="AggregateAppender" class="ch.qos.logback.core.rolling.RollingFileAppender">
                <!--See also http://logback.qos.ch/manual/appenders.html#RollingFileAppender-->
                <Append>true</Append>
                <File>${TE_NFS_LOG_DIR}/aggregate.log</File>
                <encoder>
                    <pattern>${TASK_SPECIFIC_FILE_LOG_PATTERN}</pattern>
                </encoder>
                <rollingPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy">
                    <!-- each file should be at most 100MB, keep 60 days of history, but at most 2GB -->
                    <maxFileSize>100MB</maxFileSize>
                    <maxHistory>60</maxHistory>
                    <totalSizeCap>2GB</totalSizeCap>
                    <FileNamePattern>${TE_NFS_LOG_DIR}/aggregate-%d{yyyy-MM-dd}.log.%i.gz</FileNamePattern>
                </rollingPolicy>
            </appender>
        
            <appender name="ForecastAppender" class="ch.qos.logback.core.rolling.RollingFileAppender">
                <!--See also http://logback.qos.ch/manual/appenders.html#RollingFileAppender-->
                <Append>true</Append>
                <File>${TE_NFS_LOG_DIR}/forecast.log</File>
                <encoder>
                    <pattern>${TASK_SPECIFIC_FILE_LOG_PATTERN}</pattern>
                </encoder>
                <rollingPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy">
                    <!-- each file should be at most 100MB, keep 60 days of history, but at most 2GB -->
                    <maxFileSize>100MB</maxFileSize>
                    <maxHistory>60</maxHistory>
                    <totalSizeCap>2GB</totalSizeCap>
                    <FileNamePattern>${TE_NFS_LOG_DIR}/forecast-%d{yyyy-MM-dd}.log.%i.gz</FileNamePattern>
                </rollingPolicy>
            </appender>
        
            <appender name="PerlTaskAppender" class="ch.qos.logback.core.rolling.RollingFileAppender">
                <!--See also http://logback.qos.ch/manual/appenders.html#RollingFileAppender-->
                <Append>true</Append>
                <File>${TE_NFS_LOG_DIR}/perl-task.log</File>
                <encoder>
                    <pattern>${FILE_LOG_PATTERN}</pattern>
                </encoder>
                <rollingPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy">
                    <!-- each file should be at most 100MB, keep 60 days of history, but at most 2GB -->
                    <maxFileSize>100MB</maxFileSize>
                    <maxHistory>60</maxHistory>
                    <totalSizeCap>2GB</totalSizeCap>
                    <FileNamePattern>${TE_NFS_LOG_DIR}/perl-task-%d{yyyy-MM-dd}.log.%i.gz</FileNamePattern>
                </rollingPolicy>
            </appender>
        
            <appender name="ConsoleAppender" class="ch.qos.logback.core.ConsoleAppender">
                <encoder>
                    <pattern>${FILE_LOG_PATTERN}</pattern>
                </encoder>
            </appender>
        
            <!-- Task specific loggers  -->
        
            <logger name="com.microfocus.itomdi.postload.taskexecutor.tasks.outofbox.CsvDirectLoadTask" level="INFO" additivity="false">
                <appender-ref ref="CsvDirectLoadAppender"/>
            </logger>
        
            <logger name="com.microfocus.itomdi.postload.taskexecutor.tasks.outofbox.CsvDirectLoadArchiveCleanupTask" level="INFO" additivity="false">
                <appender-ref ref="CsvDirectLoadAppender"/>
            </logger>
        
            <logger name="com.microfocus.itomdi.postload.taskexecutor.tasks.outofbox.CsvDirectLoadArchiveCompressTask" level="INFO" additivity="false">
                <appender-ref ref="CsvDirectLoadAppender"/>
            </logger>
        
            <logger name="com.microfocus.itomdi.postload.taskexecutor.tasks.outofbox.PerlTask" level="INFO" additivity="false">
                <appender-ref ref="PerlTaskAppender"/>
            </logger>
        
            <logger name="com.microfocus.itomdi.dataenrich.taskexecutor.AggregateTask" level="INFO" additivity="false">
                <appender-ref ref="AggregateAppender"/>
            </logger>
        
            <logger name="com.microfocus.itomdi.dataenrich.taskexecutor.ForecastTask" level="INFO" additivity="false">
                <appender-ref ref="ForecastAppender"/>
            </logger>
        
            <!-- Task specific loggers end    -->
        
            <logger name="com.swgrp.itomdi" level="INFO" additivity="false">
                <appender-ref ref="LogFileAppender"/>
            </logger>
            <logger name="com.microfocus" level="INFO" additivity="false">
                <appender-ref ref="LogFileAppender"/>
            </logger>
            <root level="WARN">
                <appender-ref ref="LogFileAppender"/>
            </root>
        </configuration>
---
# Source: nomultimate/charts/itomdipulsar/templates/bastion/bastion-configmap.yaml
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

apiVersion: v1
kind: ConfigMap
metadata:
  name: "itomdipulsar-bastion"
  namespace: default
  labels:
    app: itomdipulsar
    chart: itomdipulsar-2.8.1-31
    release: RELEASE-NAME
    heritage: Helm
    cluster: itomdipulsar
    app.kubernetes.io/name: itomdipulsar-bastion
    app.kubernetes.io/managed-by: RELEASE-NAME
    app.kubernetes.io/version: 2.8.1-31
    itom.microfocus.com/capability: itom-data-ingestion
    tier.itom.microfocus.com/backend: backend
    component: bastion
data:
  BOOKIE_LOG_APPENDER: "RollingFile"
  zkServers: "itomdipulsar-zookeeper:2281"
  zkLedgersRootPath: "/ledgers"
  # enable bookkeeper http server
  httpServerEnabled: "true"
  httpServerPort: "8000"
  # config the stats provider
  statsProviderClass: org.apache.bookkeeper.stats.prometheus.PrometheusMetricsProvider
  # use hostname as the bookie id
  useHostNameAsBookieID: "true"
  # talk to proxy
  webServiceUrl: "https://itomdipulsar-proxy:8443/"
  brokerServiceUrl: "pulsar+ssl://itomdipulsar-proxy:6651/"
  useTls: "true"
  tlsAllowInsecureConnection: "false"
  tlsTrustCertsFilePath: "/var/run/secrets/boostport.com/issue_ca.crt"
  tlsEnableHostnameVerification: "false"
  # Authentication Settings 
  PULSAR_MEM: |
    -Xms128M -Xmx256M -XX:MaxDirectMemorySize=128M
  authParams: tlsCertFile:/var/run/secrets/boostport.com/server.crt,tlsKeyFile:/var/run/secrets/boostport.com/server.key
  authPlugin: org.apache.pulsar.client.impl.auth.AuthenticationTls
  tlsTrustCertsFilePath: /var/run/secrets/boostport.com/issue_ca.crt
  # Include log configuration file, If you want to configure the log level and other configuration
  # items, you can modify the configmap, and eventually it will overwrite the log4j2.yaml file under conf
  log4j2.yaml: "#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or
    more contributor license agreements.  See the NOTICE file\n# distributed with this
    work for additional information\n# regarding copyright ownership.  The ASF licenses
    this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\");
    you may not use this file except in compliance\n# with the License.  You may obtain
    a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n#
    Unless required by applicable law or agreed to in writing,\n# software distributed
    under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR
    CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n#
    specific language governing permissions and limitations\n# under the License.\n#\n\n\nConfiguration:\n
    \ status: INFO\n  monitorInterval: 30\n  name: pulsar\n  packages: io.prometheus.client.log4j2\n\n
    \ Properties:\n    Property:\n      - name: \"pulsar.log.dir\"\n        value: \"logs\"\n
    \     - name: \"pulsar.log.file\"\n        value: \"pulsar.log\"\n      - name:
    \"pulsar.log.appender\"\n        value: \"RoutingAppender\"\n      - name: \"pulsar.log.root.level\"\n
    \       value: \"info\"\n      - name: \"pulsar.log.level\"\n        value: \"info\"\n
    \     - name: \"pulsar.routing.appender.default\"\n        value: \"Console\"\n\n
    \ # Example: logger-filter script\n  Scripts:\n    ScriptFile:\n      name: filter.js\n
    \     language: JavaScript\n      path: ./conf/log4j2-scripts/filter.js\n      charset:
    UTF-8\n\n  Appenders:\n\n    # Console\n    Console:\n      name: Console\n      target:
    SYSTEM_OUT\n      PatternLayout:\n        Pattern: \"%d{ISO8601}{GMT} [%t] %-5level
    %logger{36} - %msg%n\"\n\n    # Rolling file appender configuration\n    RollingFile:\n
    \     name: RollingFile\n      fileName: \"${sys:pulsar.log.dir}/${sys:pulsar.log.file}\"\n
    \     filePattern: \"${sys:pulsar.log.dir}/${sys:pulsar.log.file}-%d{MM-dd-yyyy}-%i.log.gz\"\n
    \     immediateFlush: false\n      PatternLayout:\n        Pattern: \"%d{ISO8601}{GMT}
    [%t] %-5level %logger{36} - %msg%n\"\n      Policies:\n        TimeBasedTriggeringPolicy:\n
    \         interval: 1\n          modulate: true\n        SizeBasedTriggeringPolicy:\n
    \         size: 1 GB\n      # Delete file older than 30days\n      DefaultRolloverStrategy:\n
    \         Delete:\n            basePath: ${sys:pulsar.log.dir}\n            maxDepth:
    2\n            IfFileName:\n              glob: \"*/${sys:pulsar.log.file}*log.gz\"\n
    \           IfLastModified:\n              age: 30d\n\n    Prometheus:\n      name:
    Prometheus\n\n    # Routing\n    Routing:\n      name: RoutingAppender\n      Routes:\n
    \       pattern: \"$${ctx:function}\"\n        Route:\n          -\n            Routing:\n
    \             name: InstanceRoutingAppender\n              Routes:\n                pattern:
    \"$${ctx:instance}\"\n                Route:\n                  -\n                    RollingFile:\n
    \                     name: \"Rolling-${ctx:function}\"\n                      fileName
    : \"${sys:pulsar.log.dir}/functions/${ctx:function}/${ctx:functionname}-${ctx:instance}.log\"\n
    \                     filePattern : \"${sys:pulsar.log.dir}/functions/${sys:pulsar.log.file}-${ctx:instance}-%d{MM-dd-yyyy}-%i.log.gz\"\n
    \                     PatternLayout:\n                        Pattern: \"%d{ISO8601}{GMT}
    %level{length=5} [%thread] [instance: %X{instance}] %logger{1} - %msg%n\"\n                      Policies:\n
    \                       TimeBasedTriggeringPolicy:\n                          interval:
    1\n                          modulate: true\n                        SizeBasedTriggeringPolicy:\n
    \                         size: \"20MB\"\n                        # Trigger every
    day at midnight that also scan\n                        # roll-over strategy that
    deletes older file\n                        CronTriggeringPolicy:\n                          schedule:
    \"0 0 0 * * ?\"\n                      # Delete file older than 30days\n                      DefaultRolloverStrategy:\n
    \                         Delete:\n                            basePath: ${sys:pulsar.log.dir}\n
    \                           maxDepth: 2\n                            IfFileName:\n
    \                             glob: \"*/${sys:pulsar.log.file}*log.gz\"\n                            IfLastModified:\n
    \                             age: 30d\n                  - ref: \"${sys:pulsar.routing.appender.default}\"\n
    \                   key: \"${ctx:function}\"\n          - ref: \"${sys:pulsar.routing.appender.default}\"\n
    \           key: \"${ctx:function}\"\n\n  Loggers:\n\n    # Default root logger
    configuration\n    Root:\n      level: \"${sys:pulsar.log.root.level}\"\n      additivity:
    true\n      AppenderRef:\n        - ref: \"${sys:pulsar.log.appender}\"\n          level:
    \"${sys:pulsar.log.level}\"\n        - ref: Prometheus\n          level: info\n\n
    \   Logger:\n      - name: org.apache.bookkeeper.bookie.BookieShell\n        level:
    info\n        additivity: false\n        AppenderRef:\n          - ref: Console\n\n
    \     - name: verbose\n        level: info\n        additivity: false\n        AppenderRef:\n
    \         - ref: Console\n      ## Turn of cert logging for now. Once the required
    ciphers are added after consultation with security team, this needs to be removed\n
    \     - name: io.netty.handler.ssl.ReferenceCountedOpenSslContext\n        level:
    error\n        additivity: false\n        AppenderRef:\n          - ref: Console\n
    \     - name: org.apache.pulsar.common.util.KeyManagerProxy \n        level: error\n
    \       additivity: false\n        AppenderRef:\n          - ref: Console\n    #
    Logger to inject filter script\n#     - name: org.apache.bookkeeper.mledger.impl.ManagedLedgerImpl\n#
    \      level: debug\n#       additivity: false\n#       AppenderRef:\n#         ref:
    \"${sys:pulsar.log.appender}\"\n#         ScriptFilter:\n#           onMatch: ACCEPT\n#
    \          onMisMatch: DENY\n#           ScriptRef:\n#             ref: filter.js\n"
  kafka.properties: |
  tlsTrustCertsFilePath: /var/run/secrets/boostport.com/issue_ca.crt
  authParams: "tlsCertFile:/var/run/secrets/boostport.com/server.crt,tlsKeyFile:/var/run/secrets/boostport.com/server.key"
---
# Source: nomultimate/charts/itomdipulsar/templates/bookkeeper/bookkeeper-autorecovery-configmap.yaml
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

apiVersion: v1
kind: ConfigMap
metadata:
  name: "itomdipulsar-autorecovery"
  namespace: default
  labels:
    app: itomdipulsar
    chart: itomdipulsar-2.8.1-31
    release: RELEASE-NAME
    heritage: Helm
    cluster: itomdipulsar
    app.kubernetes.io/name: itomdipulsar-autorecovery
    app.kubernetes.io/managed-by: RELEASE-NAME
    app.kubernetes.io/version: 2.8.1-31
    itom.microfocus.com/capability: itom-data-ingestion
    tier.itom.microfocus.com/backend: backend
    component: autorecovery
data:
  # common config
  zkServers: "itomdipulsar-zookeeper:2281"
  zkLedgersRootPath: "/ledgers"
  # enable bookkeeper http server
  httpServerEnabled: "true"
  httpServerPort: "8000"
  # config the stats provider
  statsProviderClass: org.apache.bookkeeper.stats.prometheus.PrometheusMetricsProvider
  # use hostname as the bookie id
  useHostNameAsBookieID: "true"
  BOOKIE_MEM: |
    -Xms64m -Xmx64m
  log4j2.yaml: |
    #
    # Licensed to the Apache Software Foundation (ASF) under one
    # or more contributor license agreements.  See the NOTICE file
    # distributed with this work for additional information
    # regarding copyright ownership.  The ASF licenses this file
    # to you under the Apache License, Version 2.0 (the
    # "License"); you may not use this file except in compliance
    # with the License.  You may obtain a copy of the License at
    #
    #   http://www.apache.org/licenses/LICENSE-2.0
    #
    # Unless required by applicable law or agreed to in writing,
    # software distributed under the License is distributed on an
    # "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
    # KIND, either express or implied.  See the License for the
    # specific language governing permissions and limitations
    # under the License.
    #
  
  
    Configuration:
      status: INFO
      monitorInterval: 30
      name: pulsar
      packages: io.prometheus.client.log4j2
  
      Properties:
        Property:
          - name: "pulsar.log.dir"
            value: "logs"
          - name: "pulsar.log.file"
            value: "pulsar.log"
          - name: "pulsar.log.appender"
            value: "RoutingAppender"
          - name: "pulsar.log.root.level"
            value: "info"
          - name: "pulsar.log.level"
            value: "info"
          - name: "pulsar.routing.appender.default"
            value: "Console"
  
      # Example: logger-filter script
      Scripts:
        ScriptFile:
          name: filter.js
          language: JavaScript
          path: ./conf/log4j2-scripts/filter.js
          charset: UTF-8
  
      Appenders:
  
        # Console
        Console:
          name: Console
          target: SYSTEM_OUT
          PatternLayout:
            Pattern: "%d{ISO8601}{GMT} [%t] %-5level %logger{36} - %msg%n"
  
        # Rolling file appender configuration
        RollingFile:
          name: RollingFile
          fileName: "${sys:pulsar.log.dir}/${sys:pulsar.log.file}"
          filePattern: "${sys:pulsar.log.dir}/${sys:pulsar.log.file}-%d{MM-dd-yyyy}-%i.log.gz"
          immediateFlush: false
          PatternLayout:
            Pattern: "%d{ISO8601}{GMT} [%t] %-5level %logger{36} - %msg%n"
          Policies:
            TimeBasedTriggeringPolicy:
              interval: 1
              modulate: true
            SizeBasedTriggeringPolicy:
              size: 1 GB
          # Delete file older than 30days
          DefaultRolloverStrategy:
              Delete:
                basePath: ${sys:pulsar.log.dir}
                maxDepth: 2
                IfFileName:
                  glob: "*/${sys:pulsar.log.file}*log.gz"
                IfLastModified:
                  age: 30d
  
        Prometheus:
          name: Prometheus
  
        # Routing
        Routing:
          name: RoutingAppender
          Routes:
            pattern: "$${ctx:function}"
            Route:
              -
                Routing:
                  name: InstanceRoutingAppender
                  Routes:
                    pattern: "$${ctx:instance}"
                    Route:
                      -
                        RollingFile:
                          name: "Rolling-${ctx:function}"
                          fileName : "${sys:pulsar.log.dir}/functions/${ctx:function}/${ctx:functionname}-${ctx:instance}.log"
                          filePattern : "${sys:pulsar.log.dir}/functions/${sys:pulsar.log.file}-${ctx:instance}-%d{MM-dd-yyyy}-%i.log.gz"
                          PatternLayout:
                            Pattern: "%d{ISO8601}{GMT} %level{length=5} [%thread] [instance: %X{instance}] %logger{1} - %msg%n"
                          Policies:
                            TimeBasedTriggeringPolicy:
                              interval: 1
                              modulate: true
                            SizeBasedTriggeringPolicy:
                              size: "20MB"
                            # Trigger every day at midnight that also scan
                            # roll-over strategy that deletes older file
                            CronTriggeringPolicy:
                              schedule: "0 0 0 * * ?"
                          # Delete file older than 30days
                          DefaultRolloverStrategy:
                              Delete:
                                basePath: ${sys:pulsar.log.dir}
                                maxDepth: 2
                                IfFileName:
                                  glob: "*/${sys:pulsar.log.file}*log.gz"
                                IfLastModified:
                                  age: 30d
                      - ref: "${sys:pulsar.routing.appender.default}"
                        key: "${ctx:function}"
              - ref: "${sys:pulsar.routing.appender.default}"
                key: "${ctx:function}"
  
      Loggers:
  
        # Default root logger configuration
        Root:
          level: "${sys:pulsar.log.root.level}"
          additivity: true
          AppenderRef:
            - ref: "${sys:pulsar.log.appender}"
              level: "${sys:pulsar.log.level}"
            - ref: Prometheus
              level: info
  
        Logger:
          - name: org.apache.bookkeeper.bookie.BookieShell
            level: info
            additivity: false
            AppenderRef:
              - ref: Console
  
          - name: verbose
            level: info
            additivity: false
            AppenderRef:
              - ref: Console
  
        # Logger to inject filter script
    #     - name: org.apache.bookkeeper.mledger.impl.ManagedLedgerImpl
    #       level: debug
    #       additivity: false
    #       AppenderRef:
    #         ref: "${sys:pulsar.log.appender}"
    #         ScriptFilter:
    #           onMatch: ACCEPT
    #           onMisMatch: DENY
    #           ScriptRef:
    #             ref: filter.js
---
# Source: nomultimate/charts/itomdipulsar/templates/bookkeeper/bookkeeper-configmap.yaml
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

apiVersion: v1
kind: ConfigMap
metadata:
  name: "itomdipulsar-bookkeeper"
  namespace: default
  labels:
    app: itomdipulsar
    chart: itomdipulsar-2.8.1-31
    release: RELEASE-NAME
    heritage: Helm
    cluster: itomdipulsar
    app.kubernetes.io/name: itomdipulsar-bookkeeper
    app.kubernetes.io/managed-by: RELEASE-NAME
    app.kubernetes.io/version: 2.8.1-31
    itom.microfocus.com/capability: itom-data-ingestion
    tier.itom.microfocus.com/backend: backend
    component: bookkeeper
data:
  # common config
  zkServers: "itomdipulsar-zookeeper:2281"
  zkLedgersRootPath: "/ledgers"
  # enable bookkeeper http server
  httpServerEnabled: "true"
  httpServerPort: "8000"
  # config the stats provider
  statsProviderClass: org.apache.bookkeeper.stats.prometheus.PrometheusMetricsProvider
  # use hostname as the bookie id
  useHostNameAsBookieID: "true"
  # disable auto recovery on bookies since we will start AutoRecovery in separated pods
  autoRecoveryDaemonEnabled: "false"
  # Do not retain journal files as it increase the disk utilization
  journalMaxBackups: "0"
  journalDirectories: "data/bookkeeper/journal"
  PULSAR_PREFIX_journalDirectories: "data/bookkeeper/journal"
  ledgerDirectories: "data/bookkeeper/ledgers"
  # TLS config
  
  PULSAR_PREFIX_tlsProvider: "OpenSSL"
  PULSAR_PREFIX_tlsKeyStoreType: "PKCS12"
  PULSAR_PREFIX_tlsKeyStore: "/pulsar/ssl/store/keystore.p12"
  PULSAR_PREFIX_tlsTrustStore: "/pulsar/ssl/store/truststore.p12"
  PULSAR_PREFIX_tlsKeyStorePasswordPath: /pulsar/ssl/store/bookie.keystore.passwd
  PULSAR_PREFIX_tlsTrustStorePasswordPath: /pulsar/ssl/store/bookie.truststore.passwd
  PULSAR_PREFIX_clientTrustStorePasswordPath: /pulsar/ssl/store/client.truststore.passwd
  PULSAR_PREFIX_tlsClientAuthentication: "true"
  PULSAR_PREFIX_tlsProviderFactoryClass: "org.apache.bookkeeper.tls.TLSContextFactory"
  BOOKIE_GC: |
    -XX:+UseG1GC -XX:MaxGCPauseMillis=10 -XX:+ParallelRefProcEnabled -XX:+UnlockExperimentalVMOptions -XX:+AggressiveOpts -XX:+DoEscapeAnalysis -XX:ParallelGCThreads=4 -XX:ConcGCThreads=4 -XX:G1NewSizePercent=50 -XX:+DisableExplicitGC -XX:-ResizePLAB -XX:+ExitOnOutOfMemoryError -XX:+PerfDisableSharedMem -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCApplicationStoppedTime -XX:+PrintHeapAtGC -verbosegc -Xloggc:/var/log/bookie-gc.log -XX:G1LogLevel=finest
  BOOKIE_MEM: |
    -Xms2g -Xmx2g -XX:MaxDirectMemorySize=1g
  PULSAR_MEM: |
    -Xms2g -Xmx2g -XX:MaxDirectMemorySize=1g
  allowLoopback: "true"
  autoRecoveryDaemonEnabled: "false"
  enforceMinNumRacksPerWriteQuorum: "false"
  ensemblePlacementPolicy: org.apache.bookkeeper.client.RackawareEnsemblePlacementPolicy
  forceAllowCompaction: "true"
  isForceGCAllowWhenNoSpace: "true"
  journalMaxBackups: "0"
  journalMaxGroupWaitMSec: "3"
  journalMaxSizeMB: "2048"
  journalSyncData: "false"
  majorCompactionMaxTimeMillis: "1800000"
  managedLedgerDigestType: DUMMY
  minNumRacksPerWriteQuorum: "2"
  minorCompactionMaxTimeMillis: "900000"
  openFileLimit: "100000"
  pageSize: "8192"
  reppDnsResolverClass: org.apache.pulsar.zookeeper.ZkBookieRackAffinityMapping
  statsProviderClass: org.apache.bookkeeper.stats.prometheus.PrometheusMetricsProvider
  useHostNameAsBookieID: "true"
  # Include log configuration file, If you want to configure the log level and other configuration
  # items, you can modify the configmap, and eventually it will overwrite the log4j2.yaml file under conf
  log4j2.yaml: |
    #
    # Licensed to the Apache Software Foundation (ASF) under one
    # or more contributor license agreements.  See the NOTICE file
    # distributed with this work for additional information
    # regarding copyright ownership.  The ASF licenses this file
    # to you under the Apache License, Version 2.0 (the
    # "License"); you may not use this file except in compliance
    # with the License.  You may obtain a copy of the License at
    #
    #   http://www.apache.org/licenses/LICENSE-2.0
    #
    # Unless required by applicable law or agreed to in writing,
    # software distributed under the License is distributed on an
    # "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
    # KIND, either express or implied.  See the License for the
    # specific language governing permissions and limitations
    # under the License.
    #
  
  
    Configuration:
      status: INFO
      monitorInterval: 30
      name: pulsar
      packages: io.prometheus.client.log4j2
  
      Properties:
        Property:
          - name: "pulsar.log.dir"
            value: "logs"
          - name: "pulsar.log.file"
            value: "pulsar.log"
          - name: "pulsar.log.appender"
            value: "RoutingAppender"
          - name: "pulsar.log.root.level"
            value: "info"
          - name: "pulsar.log.level"
            value: "info"
          - name: "pulsar.routing.appender.default"
            value: "Console"
  
      # Example: logger-filter script
      Scripts:
        ScriptFile:
          name: filter.js
          language: JavaScript
          path: ./conf/log4j2-scripts/filter.js
          charset: UTF-8
  
      Appenders:
  
        # Console
        Console:
          name: Console
          target: SYSTEM_OUT
          PatternLayout:
            Pattern: "%d{ISO8601}{GMT} [%t] %-5level %logger{36} - %msg%n"
  
        # Rolling file appender configuration
        RollingFile:
          name: RollingFile
          fileName: "${sys:pulsar.log.dir}/${sys:pulsar.log.file}"
          filePattern: "${sys:pulsar.log.dir}/${sys:pulsar.log.file}-%d{MM-dd-yyyy}-%i.log.gz"
          immediateFlush: false
          PatternLayout:
            Pattern: "%d{ISO8601}{GMT} [%t] %-5level %logger{36} - %msg%n"
          Policies:
            TimeBasedTriggeringPolicy:
              interval: 1
              modulate: true
            SizeBasedTriggeringPolicy:
              size: 1 GB
          # Delete file older than 30days
          DefaultRolloverStrategy:
              Delete:
                basePath: ${sys:pulsar.log.dir}
                maxDepth: 2
                IfFileName:
                  glob: "*/${sys:pulsar.log.file}*log.gz"
                IfLastModified:
                  age: 30d
  
        Prometheus:
          name: Prometheus
  
        # Routing
        Routing:
          name: RoutingAppender
          Routes:
            pattern: "$${ctx:function}"
            Route:
              -
                Routing:
                  name: InstanceRoutingAppender
                  Routes:
                    pattern: "$${ctx:instance}"
                    Route:
                      -
                        RollingFile:
                          name: "Rolling-${ctx:function}"
                          fileName : "${sys:pulsar.log.dir}/functions/${ctx:function}/${ctx:functionname}-${ctx:instance}.log"
                          filePattern : "${sys:pulsar.log.dir}/functions/${sys:pulsar.log.file}-${ctx:instance}-%d{MM-dd-yyyy}-%i.log.gz"
                          PatternLayout:
                            Pattern: "%d{ISO8601}{GMT} %level{length=5} [%thread] [instance: %X{instance}] %logger{1} - %msg%n"
                          Policies:
                            TimeBasedTriggeringPolicy:
                              interval: 1
                              modulate: true
                            SizeBasedTriggeringPolicy:
                              size: "20MB"
                            # Trigger every day at midnight that also scan
                            # roll-over strategy that deletes older file
                            CronTriggeringPolicy:
                              schedule: "0 0 0 * * ?"
                          # Delete file older than 30days
                          DefaultRolloverStrategy:
                              Delete:
                                basePath: ${sys:pulsar.log.dir}
                                maxDepth: 2
                                IfFileName:
                                  glob: "*/${sys:pulsar.log.file}*log.gz"
                                IfLastModified:
                                  age: 30d
                      - ref: "${sys:pulsar.routing.appender.default}"
                        key: "${ctx:function}"
              - ref: "${sys:pulsar.routing.appender.default}"
                key: "${ctx:function}"
  
      Loggers:
  
        # Default root logger configuration
        Root:
          level: "${sys:pulsar.log.root.level}"
          additivity: true
          AppenderRef:
            - ref: "${sys:pulsar.log.appender}"
              level: "${sys:pulsar.log.level}"
            - ref: Prometheus
              level: info
  
        Logger:
          - name: org.apache.bookkeeper.bookie.BookieShell
            level: info
            additivity: false
            AppenderRef:
              - ref: Console
  
          - name: verbose
            level: info
            additivity: false
            AppenderRef:
              - ref: Console
  
        # Logger to inject filter script
    #     - name: org.apache.bookkeeper.mledger.impl.ManagedLedgerImpl
    #       level: debug
    #       additivity: false
    #       AppenderRef:
    #         ref: "${sys:pulsar.log.appender}"
    #         ScriptFilter:
    #           onMatch: ACCEPT
    #           onMisMatch: DENY
    #           ScriptRef:
    #             ref: filter.js
---
# Source: nomultimate/charts/itomdipulsar/templates/broker/broker-configmap.yaml
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

apiVersion: v1
kind: ConfigMap
metadata:
  name: "itomdipulsar-broker"
  namespace: default
  labels:
    app: itomdipulsar
    chart: itomdipulsar-2.8.1-31
    release: RELEASE-NAME
    heritage: Helm
    cluster: itomdipulsar
    app.kubernetes.io/name: itomdipulsar-broker
    app.kubernetes.io/managed-by: RELEASE-NAME
    app.kubernetes.io/version: 2.8.1-31
    itom.microfocus.com/capability: itom-data-ingestion
    tier.itom.microfocus.com/backend: backend
    component: broker
data:
  # Metadata settings
  zookeeperServers: "itomdipulsar-zookeeper:2281"
  configurationStoreServers: "itomdipulsar-zookeeper:2281"

  # Broker settings
  clusterName: "itomdipulsar"
  exposeTopicLevelMetricsInPrometheus: "true"
  numHttpServerThreads: "8"
  zooKeeperSessionTimeoutMillis: "30000"
  statusFilePath: "/pulsar/tmp/status"

  ## Offloading settings

  # Function Worker Settings
  # function worker configuration
  functionsWorkerEnabled: "true"
  PULSAR_EXTRA_CLASSPATH: "/pulsar/extraLibs"

  # prometheus needs to access /metrics endpoint
  webServicePort: "8080"
  tlsEnabled: "true"
  brokerServicePortTls: "6651"
  webServicePortTls: "8443"
  # TLS Settings
  tlsCertificateFilePath: "/var/run/secrets/boostport.com/server.crt"
  tlsKeyFilePath: "/var/run/secrets/boostport.com/server.key"

  # Authentication Settings
  authenticationEnabled: "true"
  authenticationProviders: "org.apache.pulsar.broker.authentication.AuthenticationProviderTls"
  brokerClientAuthenticationPlugin: "org.apache.pulsar.client.impl.auth.AuthenticationTls"
  brokerClientAuthenticationParameters: "tlsCertFile:/var/run/secrets/boostport.com/server.crt,tlsKeyFile:/var/run/secrets/boostport.com/server.key"
  authenticateOriginalAuthData: "true"
  brokerClientTlsEnabled: "true"
  brokerClientTrustCertsFilePath: "/var/run/secrets/boostport.com/issue_ca.crt"
  tlsTrustCertsFilePath: "/pulsar/ssl/broker_client_ca.crt"

  # Client Configuration purposefully configured for client to talk to the local broker not the proxy.
  webServiceUrl: https://localhost:8443/
  brokerServiceUrl: pulsar+ssl://localhost:6651/
  # bookkeeper tls settings
  authPlugin: "org.apache.pulsar.client.impl.auth.AuthenticationTls"
  authParams: "tlsCertFile:/var/run/secrets/boostport.com/server.crt,tlsKeyFile:/var/run/secrets/boostport.com/server.key"
  bookkeeperTLSKeyFilePath: /var/run/secrets/boostport.com/server.key
  bookkeeperTLSCertificateFilePath: "/var/run/secrets/boostport.com/server.crt"

  # https://github.com/apache/bookkeeper/pull/2300
  bookkeeperUseV2WireProtocol: "false"
  AWS_ACCESS_KEY_ID: '[YOUR AWS ACCESS KEY ID]'
  AWS_SECRET_ACCESS_KEY: '[YOUR SECRET]'
  PULSAR_GC: |
    -XX:+UseG1GC -XX:MaxGCPauseMillis=10 -XX:+UseContainerSupport -Dio.netty.leakDetectionLevel=disabled -Dio.netty.recycler.linkCapacity=1024 -XX:+ParallelRefProcEnabled -XX:+UnlockExperimentalVMOptions -XX:+AggressiveOpts -XX:+DoEscapeAnalysis -XX:ParallelGCThreads=4 -XX:ConcGCThreads=4 -XX:G1NewSizePercent=50 -XX:+DisableExplicitGC -XX:-ResizePLAB -XX:+ExitOnOutOfMemoryError -XX:+PerfDisableSharedMem
  PULSAR_MEM: |
    -Xms2g -Xmx2g -XX:MaxDirectMemorySize=1g
  allowAutoTopicCreation: "false"
  authorizationEnabled: "false"
  bookkeeperClientEnforceMinNumRacksPerWriteQuorum: "false"
  bookkeeperClientMinNumRacksPerWriteQuorum: "2"
  brokerDeduplicationEnabled: "false"
  brokerDeleteInactiveTopicsEnabled: "false"
  brokerServicePortTls: "6651"
  deduplicationEnabled: "false"
  defaultRetentionSizeInMB: "1000"
  defaultRetentionTimeInMinutes: "60"
  exposePreciseBacklogInPrometheus: "true"
  functionsWorkerEnabled: "true"
  managedLedgerCursorRolloverTimeInSeconds: "120"
  managedLedgerDefaultAckQuorum: "2"
  managedLedgerDefaultEnsembleSize: "2"
  managedLedgerDefaultWriteQuorum: "2"
  managedLedgerMaxLedgerRolloverTimeMinutes: "2"
  managedLedgerMinLedgerRolloverTimeMinutes: "1"
  proxyRoles: itomdipulsar-proxy
  supportedNamespaceBundleSplitAlgorithms: range_equally_divide
  tlsCiphers: TLS_RSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA,TLS_RSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA,TLS_RSA_WITH_AES_256_CBC_SHA
  tlsProtocols: TLSv1.2
  ttlDurationDefaultInSeconds: "3600"
  webServicePortTls: "8443"
  # Include log configuration file, If you want to configure the log level and other configuration
  # items, you can modify the configmap, and eventually it will overwrite the log4j2.yaml file under conf
  log4j2.yaml: "#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or
    more contributor license agreements.  See the NOTICE file\n# distributed with this
    work for additional information\n# regarding copyright ownership.  The ASF licenses
    this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\");
    you may not use this file except in compliance\n# with the License.  You may obtain
    a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n#
    Unless required by applicable law or agreed to in writing,\n# software distributed
    under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR
    CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n#
    specific language governing permissions and limitations\n# under the License.\n#\n\n\nConfiguration:\n
    \ status: INFO\n  monitorInterval: 30\n  name: pulsar\n  packages: io.prometheus.client.log4j2\n\n
    \ Properties:\n    Property:\n      - name: \"pulsar.log.dir\"\n        value: \"logs\"\n
    \     - name: \"pulsar.log.file\"\n        value: \"pulsar.log\"\n      - name:
    \"pulsar.log.appender\"\n        value: \"RoutingAppender\"\n      - name: \"pulsar.log.root.level\"\n
    \       value: \"info\"\n      - name: \"pulsar.log.level\"\n        value: \"info\"\n
    \     - name: \"pulsar.routing.appender.default\"\n        value: \"Console\"\n\n
    \ # Example: logger-filter script\n  Scripts:\n    ScriptFile:\n      name: filter.js\n
    \     language: JavaScript\n      path: ./conf/log4j2-scripts/filter.js\n      charset:
    UTF-8\n\n  Appenders:\n\n    # Console\n    Console:\n      name: Console\n      target:
    SYSTEM_OUT\n      PatternLayout:\n        Pattern: \"%d{ISO8601}{GMT} [%t] %-5level
    %logger{36} - %msg%n\"\n\n    # Rolling file appender configuration\n    RollingFile:\n
    \     name: RollingFile\n      fileName: \"${sys:pulsar.log.dir}/${sys:pulsar.log.file}\"\n
    \     filePattern: \"${sys:pulsar.log.dir}/${sys:pulsar.log.file}-%d{MM-dd-yyyy}-%i.log.gz\"\n
    \     immediateFlush: false\n      PatternLayout:\n        Pattern: \"%d{ISO8601}{GMT}
    [%t] %-5level %logger{36} - %msg%n\"\n      Policies:\n        TimeBasedTriggeringPolicy:\n
    \         interval: 1\n          modulate: true\n        SizeBasedTriggeringPolicy:\n
    \         size: 1 GB\n      # Delete file older than 30days\n      DefaultRolloverStrategy:\n
    \         Delete:\n            basePath: ${sys:pulsar.log.dir}\n            maxDepth:
    2\n            IfFileName:\n              glob: \"*/${sys:pulsar.log.file}*log.gz\"\n
    \           IfLastModified:\n              age: 30d\n\n    Prometheus:\n      name:
    Prometheus\n\n    # Routing\n    Routing:\n      name: RoutingAppender\n      Routes:\n
    \       pattern: \"$${ctx:function}\"\n        Route:\n          -\n            Routing:\n
    \             name: InstanceRoutingAppender\n              Routes:\n                pattern:
    \"$${ctx:instance}\"\n                Route:\n                  -\n                    RollingFile:\n
    \                     name: \"Rolling-${ctx:function}\"\n                      fileName
    : \"${sys:pulsar.log.dir}/functions/${ctx:function}/${ctx:functionname}-${ctx:instance}.log\"\n
    \                     filePattern : \"${sys:pulsar.log.dir}/functions/${sys:pulsar.log.file}-${ctx:instance}-%d{MM-dd-yyyy}-%i.log.gz\"\n
    \                     PatternLayout:\n                        Pattern: \"%d{ISO8601}{GMT}
    %level{length=5} [%thread] [instance: %X{instance}] %logger{1} - %msg%n\"\n                      Policies:\n
    \                       TimeBasedTriggeringPolicy:\n                          interval:
    1\n                          modulate: true\n                        SizeBasedTriggeringPolicy:\n
    \                         size: \"20MB\"\n                        # Trigger every
    day at midnight that also scan\n                        # roll-over strategy that
    deletes older file\n                        CronTriggeringPolicy:\n                          schedule:
    \"0 0 0 * * ?\"\n                      # Delete file older than 30days\n                      DefaultRolloverStrategy:\n
    \                         Delete:\n                            basePath: ${sys:pulsar.log.dir}\n
    \                           maxDepth: 2\n                            IfFileName:\n
    \                             glob: \"*/${sys:pulsar.log.file}*log.gz\"\n                            IfLastModified:\n
    \                             age: 30d\n                  - ref: \"${sys:pulsar.routing.appender.default}\"\n
    \                   key: \"${ctx:function}\"\n          - ref: \"${sys:pulsar.routing.appender.default}\"\n
    \           key: \"${ctx:function}\"\n\n  Loggers:\n\n    # Default root logger
    configuration\n    Root:\n      level: \"${sys:pulsar.log.root.level}\"\n      additivity:
    true\n      AppenderRef:\n        - ref: \"${sys:pulsar.log.appender}\"\n          level:
    \"${sys:pulsar.log.level}\"\n        - ref: Prometheus\n          level: info\n\n
    \   Logger:\n      - name: org.apache.bookkeeper.bookie.BookieShell\n        level:
    info\n        additivity: false\n        AppenderRef:\n          - ref: Console\n\n
    \     - name: org.apache.bookkeeper.client.BookieWatcherImpl\n        level: error\n
    \       additivity: false\n        AppenderRef:\n          - ref: Console\n\n      -
    name: verbose\n        level: info\n        additivity: false\n        AppenderRef:\n
    \         - ref: Console\n\n      - name: org.eclipse.jetty.server.RequestLog\n
    \       level: warn\n        additivity: false\n        AppenderRef:\n          -
    ref: Console\n          \n    # Logger to inject filter script\n#     - name: org.apache.bookkeeper.mledger.impl.ManagedLedgerImpl\n#
    \      level: debug\n#       additivity: false\n#       AppenderRef:\n#         ref:
    \"${sys:pulsar.log.appender}\"\n#         ScriptFilter:\n#           onMatch: ACCEPT\n#
    \          onMisMatch: DENY\n#           ScriptRef:\n#             ref: filter.js\n"
---
# Source: nomultimate/charts/itomdipulsar/templates/broker/function-worker-configfile-configmap.yaml
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

## function config map
apiVersion: v1
kind: ConfigMap
metadata:
  name: "itomdipulsar-functions-worker-configfile"
  namespace: default
  labels:
    app: itomdipulsar
    chart: itomdipulsar-2.8.1-31
    release: RELEASE-NAME
    heritage: Helm
    cluster: itomdipulsar
    app.kubernetes.io/name: itomdipulsar-functions-worker
    app.kubernetes.io/managed-by: RELEASE-NAME
    app.kubernetes.io/version: 2.8.1-31
    itom.microfocus.com/capability: itom-data-ingestion
    tier.itom.microfocus.com/backend: backend
    component: functions-worker
data:
  functions_worker.yml: |
    numFunctionPackageReplicas: 2
    pulsarFunctionsCluster: itomdipulsar
    pulsarServiceUrl: pulsar://localhost:6650
    pulsarWebServiceUrl: http://localhost:8080
    useTls: true
    functionRuntimeFactoryConfigs:
      changeConfigMap: "itomdipulsar-functions-worker-config"
      changeConfigMapNamespace: default
      installUserCodeDependencies: true
      jobNamespace: default
      pulsarDockerImageName: atf.intranet.bb.com.br:5001/hpeswitom/itom-pulsar-core:2.8.1-26
      pulsarRootDir: null
      pulsarAdminUrl: "https://itomdipulsar-broker:8443/"
      pulsarServiceUrl: "pulsar+ssl://itomdipulsar-broker:6651/"
      submittingInsidePod: true
      expectedMetricsCollectionInterval: "30"
      imagePullPolicy: Always
      logDirectory: logs/
      percentMemoryPadding: 10
    kubernetesContainerFactory:
      changeConfigMap: "itomdipulsar-functions-worker-config"
      changeConfigMapNamespace: default
      installUserCodeDependencies: true
      jobNamespace: default
      pulsarDockerImageName: atf.intranet.bb.com.br:5001/hpeswitom/itom-pulsar-core:2.8.1-26
      pulsarRootDir: /pulsar
      pulsarAdminUrl: "https://itomdipulsar-broker:8443/"
      pulsarServiceUrl: "pulsar+ssl://itomdipulsar-broker:6651/"
      submittingInsidePod: true
      expectedMetricsCollectionInterval: "30"
    superUserRoles: null
    runtimeCustomizerClassName: com.swgrp.itomdi.pulsar.functions.runtime.kubernetes.COSOKubernetesManifestCustomizer
    runtimeCustomizerConfig:
      globalTlsTrustStore: "default-ca-certificates"
      metaIoVersion: 2.8.1-26
      runAsGroup: "1999"
      runAsUser: "1999"
      runFsGroup: "1999"
      serviceAccount: itomdipulsar-broker-sa
      vaultAppRole: default-default
      vaultInitImage: atf.intranet.bb.com.br:5001/hpeswitom/kubernetes-vault-init:0.15.0-0038
      vaultRenewImage: atf.intranet.bb.com.br:5001/hpeswitom/kubernetes-vault-renew:0.15.0-0038
    assignmentWriteMaxRetries: 60
    authenticationEnabled: true
    authenticationProviders:
    - org.apache.pulsar.broker.authentication.AuthenticationProviderTls
    authorizationEnabled: false
    authorizationProvider: org.apache.pulsar.broker.authorization.PulsarAuthorizationProvider
    brokerClientTrustCertsFilePath: /var/run/secrets/boostport.com/issue_ca.crt
    clientAuthenticationParameters: tlsCertFile:/var/run/secrets/boostport.com/server.crt,tlsKeyFile:/var/run/secrets/boostport.com/server.key
    clientAuthenticationPlugin: org.apache.pulsar.client.impl.auth.AuthenticationTls
    clusterCoordinationTopicName: coordinate
    configurationStoreServers: localhost:2181
    connectorsDirectory: ./connectors
    downloadDirectory: download/pulsar_functions
    failureCheckFreqMs: 30000
    functionAssignmentTopicName: assignments
    functionAuthProviderClassName: com.swgrp.itomdi.pulsar.functions.auth.COSOKubernetesTlsFunctionAuthProvider
    functionMetadataTopicName: metadata
    functionRuntimeFactoryClassName: org.apache.pulsar.functions.runtime.kubernetes.KubernetesRuntimeFactory
    functionsDirectory: ./functions
    initialBrokerReconnectMaxRetries: 60
    instanceLivenessCheckFreqMs: 30000
    numHttpServerThreads: 8
    pulsarFunctionsNamespace: public/functions
    rebalanceCheckFreqSec: -1
    rescheduleTimeoutMs: 60000
    schedulerClassName: org.apache.pulsar.functions.worker.scheduler.RoundRobinScheduler
    tlsAllowInsecureConnection: false
    tlsCertRefreshCheckDurationSec: 300
    tlsCertificateFilePath: /var/run/secrets/boostport.com/server.crt
    tlsEnableHostnameVerification: false
    tlsEnabled: true
    tlsKeyFilePath: /var/run/secrets/boostport.com/server.key
    tlsTrustCertsFilePath: /var/run/secrets/boostport.com/issue_ca.crt
    topicCompactionFrequencySec: 1800
    useCompactedMetadataTopic: false
    validateConnectorConfig: false
    workerHostname: localhost
    workerId: standalone
    workerPort: 6750
    workerPortTls: 6751
    zooKeeperOperationTimeoutSeconds: 30
    zooKeeperSessionTimeoutMillis: 30000
---
# Source: nomultimate/charts/itomdipulsar/templates/broker/function-worker-configmap.yaml
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

## function config map
apiVersion: v1
kind: ConfigMap
metadata:
  name: "itomdipulsar-functions-worker-config"
  namespace: default
  labels:
    app: itomdipulsar
    chart: itomdipulsar-2.8.1-31
    release: RELEASE-NAME
    heritage: Helm
    cluster: itomdipulsar
    app.kubernetes.io/name: itomdipulsar-functions-worker
    app.kubernetes.io/managed-by: RELEASE-NAME
    app.kubernetes.io/version: 2.8.1-31
    itom.microfocus.com/capability: itom-data-ingestion
    tier.itom.microfocus.com/backend: backend
    component: functions-worker
data:
  pulsarDockerImageName: "atf.intranet.bb.com.br:5001/hpeswitom/itom-pulsar-core:2.8.1-26"
---
# Source: nomultimate/charts/itomdipulsar/templates/broker/post-upgrade-configmap.yaml
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

apiVersion: v1
kind: ConfigMap
metadata:
  name: "itomdipulsar-broker-post-upgrade"
  namespace: default
  labels:
    app: itomdipulsar
    chart: itomdipulsar-2.8.1-31
    release: RELEASE-NAME
    heritage: Helm
    cluster: itomdipulsar
    app.kubernetes.io/name: itomdipulsar-broker
    app.kubernetes.io/managed-by: RELEASE-NAME
    app.kubernetes.io/version: 2.8.1-31
    itom.microfocus.com/capability: itom-data-ingestion
    tier.itom.microfocus.com/backend: backend
    component: broker
data:
  BOOKIE_LOG_APPENDER: "RollingFile"
  zkServers: "itomdipulsar-zookeeper:2281"
  zkLedgersRootPath: "/ledgers"
  # enable bookkeeper http server
  httpServerEnabled: "true"
  httpServerPort: "8000"
  # config the stats provider
  statsProviderClass: org.apache.bookkeeper.stats.prometheus.PrometheusMetricsProvider
  # use hostname as the bookie id
  useHostNameAsBookieID: "true"
  # talk to broker
  webServiceUrl: "https://itomdipulsar-broker:8443/"
  brokerServiceUrl: "pulsar+ssl://itomdipulsar-broker:6651/"
  useTls: "true"
  tlsAllowInsecureConnection: "false"
  tlsTrustCertsFilePath: "/var/run/secrets/boostport.com/issue_ca.crt"
  tlsEnableHostnameVerification: "false"

  # Authentication Settings
  PULSAR_MEM: |
    -Xms128M -Xmx256M -XX:MaxDirectMemorySize=128M
  authParams: tlsCertFile:/var/run/secrets/boostport.com/server.crt,tlsKeyFile:/var/run/secrets/boostport.com/server.key
  authPlugin: org.apache.pulsar.client.impl.auth.AuthenticationTls
  tlsTrustCertsFilePath: /var/run/secrets/boostport.com/issue_ca.crt
  # Include log configuration file, If you want to configure the log level and other configuration
  # items, you can modify the configmap, and eventually it will overwrite the log4j2.yaml file under conf
  log4j2.yaml: "#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or
    more contributor license agreements.  See the NOTICE file\n# distributed with this
    work for additional information\n# regarding copyright ownership.  The ASF licenses
    this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\");
    you may not use this file except in compliance\n# with the License.  You may obtain
    a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n#
    Unless required by applicable law or agreed to in writing,\n# software distributed
    under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR
    CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n#
    specific language governing permissions and limitations\n# under the License.\n#\n\n\nConfiguration:\n
    \ status: INFO\n  monitorInterval: 30\n  name: pulsar\n  packages: io.prometheus.client.log4j2\n\n
    \ Properties:\n    Property:\n      - name: \"pulsar.log.dir\"\n        value: \"logs\"\n
    \     - name: \"pulsar.log.file\"\n        value: \"pulsar.log\"\n      - name:
    \"pulsar.log.appender\"\n        value: \"RoutingAppender\"\n      - name: \"pulsar.log.root.level\"\n
    \       value: \"info\"\n      - name: \"pulsar.log.level\"\n        value: \"info\"\n
    \     - name: \"pulsar.routing.appender.default\"\n        value: \"Console\"\n\n
    \ # Example: logger-filter script\n  Scripts:\n    ScriptFile:\n      name: filter.js\n
    \     language: JavaScript\n      path: ./conf/log4j2-scripts/filter.js\n      charset:
    UTF-8\n\n  Appenders:\n\n    # Console\n    Console:\n      name: Console\n      target:
    SYSTEM_OUT\n      PatternLayout:\n        Pattern: \"%d{ISO8601}{GMT} [%t] %-5level
    %logger{36} - %msg%n\"\n\n    # Rolling file appender configuration\n    RollingFile:\n
    \     name: RollingFile\n      fileName: \"${sys:pulsar.log.dir}/${sys:pulsar.log.file}\"\n
    \     filePattern: \"${sys:pulsar.log.dir}/${sys:pulsar.log.file}-%d{MM-dd-yyyy}-%i.log.gz\"\n
    \     immediateFlush: false\n      PatternLayout:\n        Pattern: \"%d{ISO8601}{GMT}
    [%t] %-5level %logger{36} - %msg%n\"\n      Policies:\n        TimeBasedTriggeringPolicy:\n
    \         interval: 1\n          modulate: true\n        SizeBasedTriggeringPolicy:\n
    \         size: 1 GB\n      # Delete file older than 30days\n      DefaultRolloverStrategy:\n
    \         Delete:\n            basePath: ${sys:pulsar.log.dir}\n            maxDepth:
    2\n            IfFileName:\n              glob: \"*/${sys:pulsar.log.file}*log.gz\"\n
    \           IfLastModified:\n              age: 30d\n\n    Prometheus:\n      name:
    Prometheus\n\n    # Routing\n    Routing:\n      name: RoutingAppender\n      Routes:\n
    \       pattern: \"$${ctx:function}\"\n        Route:\n          -\n            Routing:\n
    \             name: InstanceRoutingAppender\n              Routes:\n                pattern:
    \"$${ctx:instance}\"\n                Route:\n                  -\n                    RollingFile:\n
    \                     name: \"Rolling-${ctx:function}\"\n                      fileName
    : \"${sys:pulsar.log.dir}/functions/${ctx:function}/${ctx:functionname}-${ctx:instance}.log\"\n
    \                     filePattern : \"${sys:pulsar.log.dir}/functions/${sys:pulsar.log.file}-${ctx:instance}-%d{MM-dd-yyyy}-%i.log.gz\"\n
    \                     PatternLayout:\n                        Pattern: \"%d{ISO8601}{GMT}
    %level{length=5} [%thread] [instance: %X{instance}] %logger{1} - %msg%n\"\n                      Policies:\n
    \                       TimeBasedTriggeringPolicy:\n                          interval:
    1\n                          modulate: true\n                        SizeBasedTriggeringPolicy:\n
    \                         size: \"20MB\"\n                        # Trigger every
    day at midnight that also scan\n                        # roll-over strategy that
    deletes older file\n                        CronTriggeringPolicy:\n                          schedule:
    \"0 0 0 * * ?\"\n                      # Delete file older than 30days\n                      DefaultRolloverStrategy:\n
    \                         Delete:\n                            basePath: ${sys:pulsar.log.dir}\n
    \                           maxDepth: 2\n                            IfFileName:\n
    \                             glob: \"*/${sys:pulsar.log.file}*log.gz\"\n                            IfLastModified:\n
    \                             age: 30d\n                  - ref: \"${sys:pulsar.routing.appender.default}\"\n
    \                   key: \"${ctx:function}\"\n          - ref: \"${sys:pulsar.routing.appender.default}\"\n
    \           key: \"${ctx:function}\"\n\n  Loggers:\n\n    # Default root logger
    configuration\n    Root:\n      level: \"${sys:pulsar.log.root.level}\"\n      additivity:
    true\n      AppenderRef:\n        - ref: \"${sys:pulsar.log.appender}\"\n          level:
    \"${sys:pulsar.log.level}\"\n        - ref: Prometheus\n          level: info\n\n
    \   Logger:\n      - name: org.apache.bookkeeper.bookie.BookieShell\n        level:
    info\n        additivity: false\n        AppenderRef:\n          - ref: Console\n\n
    \     - name: verbose\n        level: info\n        additivity: false\n        AppenderRef:\n
    \         - ref: Console\n      ## Turn of cert logging for now. Once the required
    ciphers are added after consultation with security team, this needs to be removed\n
    \     - name: io.netty.handler.ssl.ReferenceCountedOpenSslContext\n        level:
    error\n        additivity: false\n        AppenderRef:\n          - ref: Console\n
    \     - name: org.apache.pulsar.common.util.KeyManagerProxy \n        level: error\n
    \       additivity: false\n        AppenderRef:\n          - ref: Console\n    #
    Logger to inject filter script\n#     - name: org.apache.bookkeeper.mledger.impl.ManagedLedgerImpl\n#
    \      level: debug\n#       additivity: false\n#       AppenderRef:\n#         ref:
    \"${sys:pulsar.log.appender}\"\n#         ScriptFilter:\n#           onMatch: ACCEPT\n#
    \          onMisMatch: DENY\n#           ScriptRef:\n#             ref: filter.js\n"
  kafka.properties: |
  tlsTrustCertsFilePath: /var/run/secrets/boostport.com/issue_ca.crt
  authParams: "tlsCertFile:/var/run/secrets/boostport.com/server.crt,tlsKeyFile:/var/run/secrets/boostport.com/server.key"
---
# Source: nomultimate/charts/itomdipulsar/templates/coso/coso-init.yaml
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

apiVersion: v1
kind: ConfigMap
metadata:
  name: "itomdipulsar-cosoinit-configmap"
  namespace: default
  labels:
    app: itomdipulsar
    chart: itomdipulsar-2.8.1-31
    release: RELEASE-NAME
    heritage: Helm
    cluster: itomdipulsar
    app.kubernetes.io/name: itomdipulsar-cosoinit
    app.kubernetes.io/managed-by: RELEASE-NAME
    app.kubernetes.io/version: 2.8.1-31
    itom.microfocus.com/capability: itom-data-ingestion
    tier.itom.microfocus.com/backend: backend
    component: cosoinit
data:
  coso-init.sh: |
    #!/bin/bash

    isClient=${2:-true}

    function logger(){
        x=$?; res=${*};
        if [[ "$x" -eq 0 ]]; then
            if [[ ! -z "$res" ]]; then
                echo -e "[`date`] - #INFO: $res" 2>&1 ;
            fi
        else
            echo -e "[`date`] - ERROR:$res" 2>&1 ;
            exit 1;
        fi;
    }

    function logger_warn(){
        x=$?; res=${*};
        if [[ "$x" -eq 0 ]]; then
            if [[ ! -z "$res" ]]; then
                echo -e "[`date`] - #INFO: $res" 2>&1 ;
            fi
        else
            echo -e "[`date`] - #WARNING: $res" 2>&1 ;
        fi;
    }

    if [ -z "${HOME}" -a -z "${RANDFILE}" ]
    then
        export RANDFILE=/pulsar/data/.rnd
    fi

    export PULSAR_HOME=/pulsar
    export PULSAR_SSLHOME=$PULSAR_HOME/ssl
    export VERTX_CACHE_DIR=/pulsar/tmp/vertx-cache
    export OPTS=-Dvertx.cacheDirBase=${VERTX_CACHE_DIR}
    export PULSAR_SSLCUSTOM_HOME=$PULSAR_SSLHOME/custom
    export LOG_DIR=/pulsar/logs/${POD_NAMESPACE}/${POD_NAMESPACE}__${POD_NAME}__${CONTAINER_NAME}__${NODE_NAME}
    mkdir -p ${VERTX_CACHE_DIR}
    mkdir -p $LOG_DIR
    chmod 700 $LOG_DIR

    # Clean ssl directory in case of restart
    rm -rf /pulsar/ssl/vault/* /pulsar/ssl/store/*

    # Move staged configs into the mutable mounted conf directory
    cd /pulsar/stage-conf
    if [ -f /pulsar/conf/log4j2.yaml ]; then
        cp -R `ls /pulsar/stage-conf | grep -v log4j2.yaml` /pulsar/conf/.
    else
        cp -R /pulsar/stage-conf/. /pulsar/conf/.
    fi
    cd /pulsar
    echo 'pulsar application logs set to ' $LOG_DIR
    id


    logger "$(mkdir -p /pulsar/ssl/vault/ 2>&1)"

    logger "$(cp -R /var/run/secrets/boostport.com/* /pulsar/ssl/vault 2>&1)"

    randompass="pulsar$RANDOM"
    export kopSslKeystorePassword=$randompass
    export kopSslKeyPassword=$randompass
    export kopSslTruststorePassword=$randompass
    export PULSAR_PREFIX_clientTrustStorePassword=$randompass
    export PULSAR_PREFIX_tlsKeystorePassword=$randompass
    export PULSAR_PREFIX_tlsTrustStorePassword=$randompass
    export PULSAR_PREFIX_zookeeper_ssl_keyStore_password=$randompass
    export PULSAR_PREFIX_zookeeper_ssl_trustStore_password=$randompass



    export PULSAR_PREFIX_tlsKeyStorePasswordPath=/pulsar/ssl/store/bookie.keystore.passwd
    export PULSAR_PREFIX_tlsTrustStorePasswordPath=/pulsar/ssl/store/bookie.truststore.passwd
    export PULSAR_PREFIX_clientTrustStorePasswordPath=/pulsar/ssl/store/client.truststore.passwd




    # SSL Configuration
    logger "$(echo "SSL configuration" 2>&1)"

    export PULSAR_SSL_CLIENT_AUTH=$PULSAR_SSL_CLIENT_AUTH
    export KEYSTORE_TYPE=pkcs12
    export TRUSTSTORE_TYPE=$TRUSTSTORE_TYPE

    logger "$(mkdir -p $PULSAR_SSLHOME/store 2>&1)"

    logger "$(chmod 700 $PULSAR_SSLHOME/store/ 2>&1)"
    logger "$(chmod 700 $PULSAR_SSLHOME/vault/ 2>&1)"

    echo $randompass >> /pulsar/ssl/store/bookie.keystore.passwd
    echo $randompass >> /pulsar/ssl/store/bookie.truststore.passwd
    echo $randompass >> /pulsar/ssl/store/client.truststore.passwd

    logger "$(echo "SSL client authentication is $PULSAR_SSL_CLIENT_AUTH" 2>&1)"

    logger "$(echo "Creating KeyStore" 2>&1)"

    export KEYSTORE_CERTS_PATH=$PULSAR_SSLHOME/vault


    if [ -f $PULSAR_SSLCUSTOM_HOME/server.crt ] && [ -f $PULSAR_SSLCUSTOM_HOME/server.key ]; then
        $KEYSTORE_CERTS_PATH=$PULSAR_SSLCUSTOM_HOME
    elif [ -f $PULSAR_SSLHOME/vault/RE/server.crt ] && [ -f $PULSAR_SSLHOME/vault/RE/server.key ]; then
        export KEYSTORE_CERTS_PATH=$PULSAR_SSLHOME/vault/RE
    fi

    logger "$(openssl pkcs12 -export -in $KEYSTORE_CERTS_PATH/server.crt -inkey $KEYSTORE_CERTS_PATH/server.key -certfile $KEYSTORE_CERTS_PATH/server.crt -out $PULSAR_SSLHOME/vault/sourcekeystore.p12 -password pass:$randompass 2>&1)"
    keytool -importkeystore -srckeystore $PULSAR_SSLHOME/vault/sourcekeystore.p12 -srcstoretype pkcs12 -destkeystore $PULSAR_SSLHOME/store/keystore.p12 -deststoretype pkcs12 -srcstorepass $randompass -deststorepass $randompass -noprompt
    if [ $? -ne 0 ]
    then
        echo "[`date`] - ERROR:Error importing Keystore"
        exit 1
    fi




    # combine provided CA cert with Vault CA cert for Proxy
    if [ -d $PULSAR_SSLCUSTOM_HOME/ca ]; then
        logger "$(echo "Combining CA certs for proxy ..." 2>&1)"
        # Concatenate all the certs onto one
        awk '{print}' $PULSAR_SSLHOME/vault/trustedCAs/RE_ca.crt $PULSAR_SSLHOME/vault/trustedCAs/RID_ca.crt $PULSAR_SSLCUSTOM_HOME/ca/* > $PULSAR_SSLHOME/combined_ca.crt
        if [ $? -ne 0 ]
        then
            echo "[`date`] - #WARNING: Error combining proxy certs."
        fi
    fi

    # Create client cert to trust for Broker
    logger "$(echo "Combining CA certs for broker..." 2>&1)"
    # Concatenate RE and RID certs onto one
    awk '{print}' $PULSAR_SSLHOME/vault/trustedCAs/RE_ca.crt $PULSAR_SSLHOME/vault/trustedCAs/RID_ca.crt > $PULSAR_SSLHOME/broker_client_ca.crt
    if [ $? -ne 0 ]
    then
        echo "[`date`] - #WARNING: Error combining broker certs."
    fi

    keytool -import -file $PULSAR_SSLHOME/vault/trustedCAs/RID_ca.crt -alias vaultca -keystore $PULSAR_SSLHOME/store/truststore.p12 -storepass $randompass -noprompt -storetype pkcs12
    if [ $? -ne 0 ]
    then
        echo "[`date`] - ERROR:Error generating Keystore"
        exit 1
    fi

    # octane issue 1180984 - Do not Split and add certs in issue_ca.crt
    # Instead, Add specific certs in trustedCAs, which are the original certs merged into issue_ca.crt
    # The issue here is that csplit on "BEGIN CERTIFICATE" might cause any leading metadata at the
    # start of the certificate, such as Bag Attributes, to get split out into its own (invalid) cert
    # and an attempt to add that would cause keytool import to fail

    keytool -importcert -alias RID_ca -trustcacerts -file $PULSAR_SSLHOME/vault/trustedCAs/RID_ca.crt -keystore ${PULSAR_SSLHOME}/store/truststore.p12 -storepass $randompass -noprompt -storetype pkcs12
    keytool -importcert -alias RE_ca -trustcacerts -file $PULSAR_SSLHOME/vault/trustedCAs/RE_ca.crt -keystore ${PULSAR_SSLHOME}/store/truststore.p12 -storepass $randompass -noprompt -storetype pkcs12
    if [ -f "$PULSAR_SSLHOME/vault/trustedCAs/CUS_ca.crt" ]; then
      keytool -importcert -alias CUS_ca -trustcacerts -file $PULSAR_SSLHOME/vault/trustedCAs/CUS_ca.crt -keystore ${PULSAR_SSLHOME}/store/truststore.p12 -storepass $randompass -noprompt -storetype pkcs12
    fi
    echo $'\n' >> conf/pulsar_env.sh
    echo "PULSAR_EXTRA_OPTS=\"\${PULSAR_EXTRA_OPTS} -Dzookeeper.ssl.quorum.keyStore.location=${PULSAR_SSLHOME}/store/keystore.p12 -Dzookeeper.ssl.quorum.keyStore.password=${randompass} -Dzookeeper.ssl.quorum.trustStore.location=${PULSAR_SSLHOME}/store/truststore.p12 -Dzookeeper.ssl.quorum.trustStore.password=${randompass} -Dzookeeper.ssl.quorum.hostnameVerification=false \"" >> conf/pulsar_env.sh
    echo $'\n' >> conf/pulsar_env.sh

    echo $'\n' >> conf/pulsar_env.sh
    echo "BOOKIE_EXTRA_OPTS=\"\${BOOKIE_EXTRA_OPTS} -Dzookeeper.ssl.quorum.keyStore.location=${PULSAR_SSLHOME}/store/keystore.p12 -Dzookeeper.ssl.quorum.keyStore.password=${randompass} -Dzookeeper.ssl.quorum.trustStore.location=${PULSAR_SSLHOME}/store/truststore.p12 -Dzookeeper.ssl.quorum.trustStore.password=${randompass} -Dzookeeper.ssl.quorum.hostnameVerification=false \"" >> conf/pulsar_env.sh
    echo $'\n' >> conf/pulsar_env.sh

    if [[ "x${isClient}" == "xtrue" ]]; then
        echo "update tls client settings ..."
        echo $'\n' >> conf/pulsar_env.sh
        echo "PULSAR_EXTRA_OPTS=\"\${PULSAR_EXTRA_OPTS} -Dzookeeper.clientCnxnSocket=org.apache.zookeeper.ClientCnxnSocketNetty -Dzookeeper.client.secure=true -Dzookeeper.ssl.keyStore.location=${PULSAR_SSLHOME}/store/keystore.p12 -Dzookeeper.ssl.keyStore.password=${randompass} -Dzookeeper.ssl.trustStore.location=${PULSAR_SSLHOME}/store/truststore.p12 -Dzookeeper.ssl.trustStore.password=${randompass}\"" >> conf/pulsar_env.sh
        echo $'\n' >> conf/pulsar_env.sh
        echo $'\n' >> conf/bkenv.sh
        echo "BOOKIE_EXTRA_OPTS=\"\${BOOKIE_EXTRA_OPTS} -Dzookeeper.clientCnxnSocket=org.apache.zookeeper.ClientCnxnSocketNetty -Dzookeeper.client.secure=true -Dzookeeper.ssl.keyStore.location=${PULSAR_SSLHOME}/store/keystore.p12 -Dzookeeper.ssl.keyStore.password=${randompass} -Dzookeeper.ssl.trustStore.location=${PULSAR_SSLHOME}/store/truststore.p12 -Dzookeeper.ssl.trustStore.password=${randompass}\"" >> conf/bkenv.sh
        echo $'\n' >> conf/bkenv.sh
    else
        echo "update tls client settings ..."
        echo $'\n' >> conf/pulsar_env.sh
        echo "PULSAR_EXTRA_OPTS=\"\${PULSAR_EXTRA_OPTS} -Dzookeeper.ssl.keyStore.location=${PULSAR_SSLHOME}/store/keystore.p12 -Dzookeeper.ssl.keyStore.password=${randompass} -Dzookeeper.ssl.trustStore.location=${PULSAR_SSLHOME}/store/truststore.p12 -Dzookeeper.ssl.trustStore.password=${randompass}\"" >> conf/pulsar_env.sh
        echo $'\n' >> conf/pulsar_env.sh
    fi
    echo ${randompass} > conf/password
---
# Source: nomultimate/charts/itomdipulsar/templates/proxy/proxy-configmap.yaml
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

apiVersion: v1
kind: ConfigMap
metadata:
  name: "itomdipulsar-proxy"
  namespace: default
  labels:
    app: itomdipulsar
    chart: itomdipulsar-2.8.1-31
    release: RELEASE-NAME
    heritage: Helm
    cluster: itomdipulsar
    app.kubernetes.io/name: itomdipulsar-proxy
    app.kubernetes.io/managed-by: RELEASE-NAME
    app.kubernetes.io/version: 2.8.1-31
    itom.microfocus.com/capability: itom-data-ingestion
    tier.itom.microfocus.com/backend: backend
    component: proxy
data:
  clusterName: itomdipulsar
  httpNumThreads: "8"
  statusFilePath: "/pulsar/tmp/status"
  # prometheus needs to access /metrics endpoint
  webServicePort: "8080"
  tlsEnabledInProxy: "true"
  servicePortTls: "6651"
  webServicePortTls: "8443"
  tlsCertificateFilePath: "/var/run/secrets/boostport.com/RE/server1.crt"
  tlsKeyFilePath: "/var/run/secrets/boostport.com/RE/server1.key"
  clusterCaCert: "/var/run/secrets/boostport.com/issue_ca.crt"
  tlsTrustCertsFilePath: "/pulsar/ssl/combined_ca.crt"
  # if broker enables TLS, configure proxy to talk to broker using TLS
  brokerServiceURLTLS: pulsar+ssl://itomdipulsar-broker:6651
  brokerWebServiceURLTLS: https://itomdipulsar-broker:8443
  tlsEnabledWithBroker: "true"
  tlsCertRefreshCheckDurationSec: "300"
  clusterCaCert: "/var/run/secrets/boostport.com/issue_ca.crt"
  brokerClientTrustCertsFilePath: "/var/run/secrets/boostport.com/issue_ca.crt"
  # Client configuration is purposefully configured to localhost of the proxy.
  # When using the client here we want to talk to this specific proxy not any proxy.
  webServiceUrl: https://localhost:8443/
  brokerServiceUrl: pulsar+ssl://localhost:6651/
  authPlugin: org.apache.pulsar.client.impl.auth.AuthenticationTls
  authParams: "tlsCertFile:/var/run/secrets/boostport.com/server.crt,tlsKeyFile:/var/run/secrets/boostport.com/server.key"# Authentication Settings
  authenticationEnabled: "true"
  forwardAuthorizationCredentials: "true"
  authenticationProviders: "org.apache.pulsar.broker.authentication.AuthenticationProviderTls"
  brokerClientAuthenticationPlugin: "org.apache.pulsar.client.impl.auth.AuthenticationTls"
  brokerClientAuthenticationParameters: "tlsCertFile:/var/run/secrets/boostport.com/server.crt,tlsKeyFile:/var/run/secrets/boostport.com/server.key"
  PULSAR_GC: |
    -XX:+UseG1GC -XX:MaxGCPauseMillis=10 -Dio.netty.leakDetectionLevel=disabled -Dio.netty.recycler.linkCapacity=1024 -XX:+ParallelRefProcEnabled -XX:+UnlockExperimentalVMOptions -XX:+AggressiveOpts -XX:+DoEscapeAnalysis -XX:ParallelGCThreads=4 -XX:ConcGCThreads=4 -XX:G1NewSizePercent=50 -XX:+DisableExplicitGC -XX:-ResizePLAB -XX:+ExitOnOutOfMemoryError -XX:+PerfDisableSharedMem
  PULSAR_MEM: |
    -Xms2g -Xmx2g -XX:MaxDirectMemorySize=1g
  allowAutoTopicCreation: "false"
  tlsCiphers: TLS_RSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA,TLS_RSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA,TLS_RSA_WITH_AES_256_CBC_SHA
  tlsProtocols: TLSv1.2
  # Include log configuration file, If you want to configure the log level and other configuration
  # items, you can modify the configmap, and eventually it will overwrite the log4j2.yaml file under conf
  log4j2.yaml: |
    #
    # Licensed to the Apache Software Foundation (ASF) under one
    # or more contributor license agreements.  See the NOTICE file
    # distributed with this work for additional information
    # regarding copyright ownership.  The ASF licenses this file
    # to you under the Apache License, Version 2.0 (the
    # "License"); you may not use this file except in compliance
    # with the License.  You may obtain a copy of the License at
    #
    #   http://www.apache.org/licenses/LICENSE-2.0
    #
    # Unless required by applicable law or agreed to in writing,
    # software distributed under the License is distributed on an
    # "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
    # KIND, either express or implied.  See the License for the
    # specific language governing permissions and limitations
    # under the License.
    #
  
  
    Configuration:
      status: INFO
      monitorInterval: 30
      name: pulsar
      packages: io.prometheus.client.log4j2
  
      Properties:
        Property:
          - name: "pulsar.log.dir"
            value: "logs"
          - name: "pulsar.log.file"
            value: "pulsar.log"
          - name: "pulsar.log.appender"
            value: "RoutingAppender"
          - name: "pulsar.log.root.level"
            value: "info"
          - name: "pulsar.log.level"
            value: "info"
          - name: "pulsar.routing.appender.default"
            value: "Console"
  
      # Example: logger-filter script
      Scripts:
        ScriptFile:
          name: filter.js
          language: JavaScript
          path: ./conf/log4j2-scripts/filter.js
          charset: UTF-8
  
      Appenders:
  
        # Console
        Console:
          name: Console
          target: SYSTEM_OUT
          PatternLayout:
            Pattern: "%d{ISO8601}{GMT} [%t] %-5level %logger{36} - %msg%n"
  
        # Rolling file appender configuration
        RollingFile:
          name: RollingFile
          fileName: "${sys:pulsar.log.dir}/${sys:pulsar.log.file}"
          filePattern: "${sys:pulsar.log.dir}/${sys:pulsar.log.file}-%d{MM-dd-yyyy}-%i.log.gz"
          immediateFlush: false
          PatternLayout:
            Pattern: "%d{ISO8601}{GMT} [%t] %-5level %logger{36} - %msg%n"
          Policies:
            TimeBasedTriggeringPolicy:
              interval: 1
              modulate: true
            SizeBasedTriggeringPolicy:
              size: 1 GB
          # Delete file older than 30days
          DefaultRolloverStrategy:
              Delete:
                basePath: ${sys:pulsar.log.dir}
                maxDepth: 2
                IfFileName:
                  glob: "*/${sys:pulsar.log.file}*log.gz"
                IfLastModified:
                  age: 30d
  
        Prometheus:
          name: Prometheus
  
        # Routing
        Routing:
          name: RoutingAppender
          Routes:
            pattern: "$${ctx:function}"
            Route:
              -
                Routing:
                  name: InstanceRoutingAppender
                  Routes:
                    pattern: "$${ctx:instance}"
                    Route:
                      -
                        RollingFile:
                          name: "Rolling-${ctx:function}"
                          fileName : "${sys:pulsar.log.dir}/functions/${ctx:function}/${ctx:functionname}-${ctx:instance}.log"
                          filePattern : "${sys:pulsar.log.dir}/functions/${sys:pulsar.log.file}-${ctx:instance}-%d{MM-dd-yyyy}-%i.log.gz"
                          PatternLayout:
                            Pattern: "%d{ISO8601}{GMT} %level{length=5} [%thread] [instance: %X{instance}] %logger{1} - %msg%n"
                          Policies:
                            TimeBasedTriggeringPolicy:
                              interval: 1
                              modulate: true
                            SizeBasedTriggeringPolicy:
                              size: "20MB"
                            # Trigger every day at midnight that also scan
                            # roll-over strategy that deletes older file
                            CronTriggeringPolicy:
                              schedule: "0 0 0 * * ?"
                          # Delete file older than 30days
                          DefaultRolloverStrategy:
                              Delete:
                                basePath: ${sys:pulsar.log.dir}
                                maxDepth: 2
                                IfFileName:
                                  glob: "*/${sys:pulsar.log.file}*log.gz"
                                IfLastModified:
                                  age: 30d
                      - ref: "${sys:pulsar.routing.appender.default}"
                        key: "${ctx:function}"
              - ref: "${sys:pulsar.routing.appender.default}"
                key: "${ctx:function}"
  
      Loggers:
  
        # Default root logger configuration
        Root:
          level: "${sys:pulsar.log.root.level}"
          additivity: true
          AppenderRef:
            - ref: "${sys:pulsar.log.appender}"
              level: "${sys:pulsar.log.level}"
            - ref: Prometheus
              level: info
  
        Logger:
          - name: org.apache.bookkeeper.bookie.BookieShell
            level: info
            additivity: false
            AppenderRef:
              - ref: Console
  
          - name: verbose
            level: info
            additivity: false
            AppenderRef:
              - ref: Console
  
        # Logger to inject filter script
    #     - name: org.apache.bookkeeper.mledger.impl.ManagedLedgerImpl
    #       level: debug
    #       additivity: false
    #       AppenderRef:
    #         ref: "${sys:pulsar.log.appender}"
    #         ScriptFilter:
    #           onMatch: ACCEPT
    #           onMisMatch: DENY
    #           ScriptRef:
    #             ref: filter.js
---
# Source: nomultimate/charts/itomdipulsar/templates/tls/keytool.yaml
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

# script to process key/cert to keystore and truststore
apiVersion: v1
kind: ConfigMap
metadata:
  name: "itomdipulsar-keytool-configmap"
  namespace: default
  labels:
    app: itomdipulsar
    chart: itomdipulsar-2.8.1-31
    release: RELEASE-NAME
    heritage: Helm
    cluster: itomdipulsar
    app.kubernetes.io/name: itomdipulsar-keytool
    app.kubernetes.io/managed-by: RELEASE-NAME
    app.kubernetes.io/version: 2.8.1-31
    itom.microfocus.com/capability: itom-data-ingestion
    tier.itom.microfocus.com/backend: backend
    component: keytool
data:
  keytool.sh: |
    #!/bin/bash
    component=$1
    name=$2
    isClient=$3
    crtFile=/var/run/secrets/boostport.com/server.crt
    keyFile=/var/run/secrets/boostport.com/server.key
    caFile=/var/run/secrets/boostport.com/issue_ca.crt
    p12File=/pulsar/${component}.p12
    keyStoreFile=/pulsar/${component}.keystore.jks
    trustStoreFile=/pulsar/${component}.truststore.jks
    
    function checkFile() {
        local file=$1
        local len=$(wc -c ${file} | awk '{print $1}')
        echo "processing ${file} : len = ${len}"
        if [ ! -f ${file} ]; then
            echo "${file} is not found"
            return -1
        fi
        if [ $len -le 0 ]; then
            echo "${file} is empty"
            return -1
        fi
    }

    function ensureFileNotEmpty() {
        local file=$1
        until checkFile ${file}; do
            echo "file isn't initialized yet ... check in 3 seconds ..." && sleep 3;
        done;
    }
    
    ensureFileNotEmpty ${crtFile}
    ensureFileNotEmpty ${keyFile}
    ensureFileNotEmpty ${caFile}
    
    export PASSWORD=$(head /dev/urandom | base64 | head -c 24)
    
    openssl pkcs12 \
        -export \
        -in ${crtFile} \
        -inkey ${keyFile} \
        -out ${p12File} \
        -name ${name} \
        -passout "pass:${PASSWORD}"
    
    keytool -importkeystore \
        -srckeystore ${p12File} \
        -srcstoretype PKCS12 -srcstorepass "${PASSWORD}" \
        -alias ${name} \
        -destkeystore ${keyStoreFile} \
        -deststorepass "${PASSWORD}"
    
    keytool -import \
        -file ${caFile} \
        -storetype JKS \
        -alias ${name} \
        -keystore ${trustStoreFile} \
        -storepass "${PASSWORD}" \
        -trustcacerts -noprompt
    
    ensureFileNotEmpty ${keyStoreFile}
    ensureFileNotEmpty ${trustStoreFile}
    
    if [[ "x${isClient}" == "xtrue" ]]; then
        echo "update tls client settings ..."
        echo $'\n' >> conf/pulsar_env.sh
        echo "PULSAR_EXTRA_OPTS=\"${PULSAR_EXTRA_OPTS} -Dzookeeper.clientCnxnSocket=org.apache.zookeeper.ClientCnxnSocketNetty -Dzookeeper.client.secure=true -Dzookeeper.ssl.keyStore.location=${keyStoreFile} -Dzookeeper.ssl.keyStore.password=${PASSWORD} -Dzookeeper.ssl.trustStore.location=${trustStoreFile} -Dzookeeper.ssl.trustStore.password=${PASSWORD}\"" >> conf/pulsar_env.sh
        echo $'\n' >> conf/bkenv.sh
        echo "BOOKIE_EXTRA_OPTS=\"${BOOKIE_EXTRA_OPTS} -Dzookeeper.clientCnxnSocket=org.apache.zookeeper.ClientCnxnSocketNetty -Dzookeeper.client.secure=true -Dzookeeper.ssl.keyStore.location=${keyStoreFile} -Dzookeeper.ssl.keyStore.password=${PASSWORD} -Dzookeeper.ssl.trustStore.location=${trustStoreFile} -Dzookeeper.ssl.trustStore.password=${PASSWORD}\"" >> conf/bkenv.sh
        echo $'\n' >> conf/bkenv.sh
    else
        echo "update tls client settings ..."
        echo $'\n' >> conf/pulsar_env.sh
        echo "PULSAR_EXTRA_OPTS=\"${PULSAR_EXTRA_OPTS} -Dzookeeper.ssl.keyStore.location=${keyStoreFile} -Dzookeeper.ssl.keyStore.password=${PASSWORD} -Dzookeeper.ssl.trustStore.location=${trustStoreFile} -Dzookeeper.ssl.trustStore.password=${PASSWORD}\"" >> conf/pulsar_env.sh
    fi
    echo ${PASSWORD} > conf/password
---
# Source: nomultimate/charts/itomdipulsar/templates/zookeeper/gen-zk-conf.yaml
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

apiVersion: v1
kind: ConfigMap
metadata:
  name: "itomdipulsar-genzkconf-configmap"
  namespace: 
  labels:
    app: itomdipulsar
    chart: itomdipulsar-2.8.1-31
    release: RELEASE-NAME
    heritage: Helm
    cluster: itomdipulsar
    app.kubernetes.io/name: itomdipulsar-zookeeper
    app.kubernetes.io/managed-by: RELEASE-NAME
    app.kubernetes.io/version: 2.8.1-31
    itom.microfocus.com/capability: itom-data-ingestion
    tier.itom.microfocus.com/backend: backend
    component: zookeeper
data:
  gen-zk-conf.sh: |
    #!/bin/bash
    
    # Apply env variables to config file and start the regular command
    
    CONF_FILE=$1
    IDX=$2
    PEER_TYPE=$3
    
    if [ $? != 0 ]; then
        echo "Error: Failed to apply changes to config file"
        exit 1
    fi
    
    # DOMAIN=`hostname -d`
    DOMAIN=$(grep $HOSTNAME /etc/hosts | awk '{print $2}' | awk -F $HOSTNAME.  '{ print $2 }')
    
    # Generate list of servers and detect the current server ID,
    # based on the hostname
    ((IDX++))
    for SERVER in $(echo $ZOOKEEPER_SERVERS | tr "," "\n")
    doecho "server.$IDX=$SERVER.$DOMAIN:2888:3888:${PEER_TYPE};2281" >> $CONF_FILEif [ "$HOSTNAME" == "$SERVER" ]; then
            MY_ID=$IDX
            echo "Current server id $MY_ID"
        fi
    
    	((IDX++))
    done
    
    # For ZooKeeper container we need to initialize the ZK id
    if [ ! -z "$MY_ID" ]; then
        # Get ZK data dir
        DATA_DIR=`grep '^dataDir=' $CONF_FILE | awk -F= '{print $2}'`
        if [ ! -e $DATA_DIR/myid ]; then
            echo "Creating $DATA_DIR/myid with id = $MY_ID"
            mkdir -p $DATA_DIR
            echo $MY_ID > $DATA_DIR/myid
        fi
    fi
---
# Source: nomultimate/charts/itomdipulsar/templates/zookeeper/zookeeper-configmap.yaml
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

# deploy zookeeper only when `components.zookeeper` is true
apiVersion: v1
kind: ConfigMap
metadata:
  name: "itomdipulsar-zookeeper"
  namespace: default
  labels:
    app: itomdipulsar
    chart: itomdipulsar-2.8.1-31
    release: RELEASE-NAME
    heritage: Helm
    cluster: itomdipulsar
    app.kubernetes.io/name: itomdipulsar-zookeeper
    app.kubernetes.io/managed-by: RELEASE-NAME
    app.kubernetes.io/version: 2.8.1-31
    itom.microfocus.com/capability: itom-data-ingestion
    tier.itom.microfocus.com/backend: backend
    component: zookeeper
data:
  dataDir: /pulsar/data/zookeeper
  PULSAR_PREFIX_serverCnxnFactory: org.apache.zookeeper.server.NettyServerCnxnFactory
  serverCnxnFactory: org.apache.zookeeper.server.NettyServerCnxnFactory
  # enable zookeeper tls
  secureClientPort: "2281"
  PULSAR_PREFIX_secureClientPort: "2281"
  PULSAR_PREFIX_peerType: participant
  PULSAR_GC: |
    -XX:+UseG1GC -XX:MaxGCPauseMillis=10 -Dcom.sun.management.jmxremote -Djute.maxbuffer=10485760 -XX:+ParallelRefProcEnabled -XX:+UnlockExperimentalVMOptions -XX:+AggressiveOpts -XX:+DoEscapeAnalysis -XX:+DisableExplicitGC -XX:+PerfDisableSharedMem -Dzookeeper.forceSync=no
  PULSAR_MEM: |
    -Xms64m -Xmx128m
  sslQuorum: "true"
  # Include log configuration file, If you want to configure the log level and other configuration
  # items, you can modify the configmap, and eventually it will overwrite the log4j2.yaml file under conf
  log4j2.yaml: |
    #
    # Licensed to the Apache Software Foundation (ASF) under one
    # or more contributor license agreements.  See the NOTICE file
    # distributed with this work for additional information
    # regarding copyright ownership.  The ASF licenses this file
    # to you under the Apache License, Version 2.0 (the
    # "License"); you may not use this file except in compliance
    # with the License.  You may obtain a copy of the License at
    #
    #   http://www.apache.org/licenses/LICENSE-2.0
    #
    # Unless required by applicable law or agreed to in writing,
    # software distributed under the License is distributed on an
    # "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
    # KIND, either express or implied.  See the License for the
    # specific language governing permissions and limitations
    # under the License.
    #
  
  
    Configuration:
      status: INFO
      monitorInterval: 30
      name: pulsar
      packages: io.prometheus.client.log4j2
  
      Properties:
        Property:
          - name: "pulsar.log.dir"
            value: "logs"
          - name: "pulsar.log.file"
            value: "pulsar.log"
          - name: "pulsar.log.appender"
            value: "RoutingAppender"
          - name: "pulsar.log.root.level"
            value: "info"
          - name: "pulsar.log.level"
            value: "info"
          - name: "pulsar.routing.appender.default"
            value: "Console"
  
      # Example: logger-filter script
      Scripts:
        ScriptFile:
          name: filter.js
          language: JavaScript
          path: ./conf/log4j2-scripts/filter.js
          charset: UTF-8
  
      Appenders:
  
        # Console
        Console:
          name: Console
          target: SYSTEM_OUT
          PatternLayout:
            Pattern: "%d{ISO8601}{GMT} [%t] %-5level %logger{36} - %msg%n"
  
        # Rolling file appender configuration
        RollingFile:
          name: RollingFile
          fileName: "${sys:pulsar.log.dir}/${sys:pulsar.log.file}"
          filePattern: "${sys:pulsar.log.dir}/${sys:pulsar.log.file}-%d{MM-dd-yyyy}-%i.log.gz"
          immediateFlush: false
          PatternLayout:
            Pattern: "%d{ISO8601}{GMT} [%t] %-5level %logger{36} - %msg%n"
          Policies:
            TimeBasedTriggeringPolicy:
              interval: 1
              modulate: true
            SizeBasedTriggeringPolicy:
              size: 1 GB
          # Delete file older than 30days
          DefaultRolloverStrategy:
              Delete:
                basePath: ${sys:pulsar.log.dir}
                maxDepth: 2
                IfFileName:
                  glob: "*/${sys:pulsar.log.file}*log.gz"
                IfLastModified:
                  age: 30d
  
        Prometheus:
          name: Prometheus
  
        # Routing
        Routing:
          name: RoutingAppender
          Routes:
            pattern: "$${ctx:function}"
            Route:
              -
                Routing:
                  name: InstanceRoutingAppender
                  Routes:
                    pattern: "$${ctx:instance}"
                    Route:
                      -
                        RollingFile:
                          name: "Rolling-${ctx:function}"
                          fileName : "${sys:pulsar.log.dir}/functions/${ctx:function}/${ctx:functionname}-${ctx:instance}.log"
                          filePattern : "${sys:pulsar.log.dir}/functions/${sys:pulsar.log.file}-${ctx:instance}-%d{MM-dd-yyyy}-%i.log.gz"
                          PatternLayout:
                            Pattern: "%d{ISO8601}{GMT} %level{length=5} [%thread] [instance: %X{instance}] %logger{1} - %msg%n"
                          Policies:
                            TimeBasedTriggeringPolicy:
                              interval: 1
                              modulate: true
                            SizeBasedTriggeringPolicy:
                              size: "20MB"
                            # Trigger every day at midnight that also scan
                            # roll-over strategy that deletes older file
                            CronTriggeringPolicy:
                              schedule: "0 0 0 * * ?"
                          # Delete file older than 30days
                          DefaultRolloverStrategy:
                              Delete:
                                basePath: ${sys:pulsar.log.dir}
                                maxDepth: 2
                                IfFileName:
                                  glob: "*/${sys:pulsar.log.file}*log.gz"
                                IfLastModified:
                                  age: 30d
                      - ref: "${sys:pulsar.routing.appender.default}"
                        key: "${ctx:function}"
              - ref: "${sys:pulsar.routing.appender.default}"
                key: "${ctx:function}"
  
      Loggers:
  
        # Default root logger configuration
        Root:
          level: "${sys:pulsar.log.root.level}"
          additivity: true
          AppenderRef:
            - ref: "${sys:pulsar.log.appender}"
              level: "${sys:pulsar.log.level}"
            - ref: Prometheus
              level: info
  
        Logger:
          - name: org.apache.bookkeeper.bookie.BookieShell
            level: info
            additivity: false
            AppenderRef:
              - ref: Console
  
          - name: verbose
            level: info
            additivity: false
            AppenderRef:
              - ref: Console
  
        # Logger to inject filter script
    #     - name: org.apache.bookkeeper.mledger.impl.ManagedLedgerImpl
    #       level: debug
    #       additivity: false
    #       AppenderRef:
    #         ref: "${sys:pulsar.log.appender}"
    #         ScriptFilter:
    #           onMatch: ACCEPT
    #           onMisMatch: DENY
    #           ScriptRef:
    #             ref: filter.js
---
# Source: nomultimate/charts/nomapiserver/templates/itom-nom-api-server-cm.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: itom-nom-api-server-config
  labels:
    name: itom-nom-api-server
data:
  ZK_HOSTNAME: "nomzk-client-svc"
  ZK_DEFAULT_PORT: "2181"
  ZK_DEFAULT_ZOOKEEPER_USERNAME: "nomadmin"
  ZK_DEFAULT_ADMIN_PASSWORD_KEY: "NOM_ZK_ADMIN_PASSWORD_VAULT_KEY"
  ZK_DEFAULT_NAMESPACE: "nom"
  ZK_DEFAULT_SESSION_TIMEOUT: "3000"
  ZK_DEFAULT_CONNECTION_TIMEOUT: "3000"
  EXTERNAL_ACCESS_HOST: "nom-api-server.homologacao.nuvem.hm.bb.com.br"
  EXTERNAL_ACCESS_PORT: "443"
  NOM_IDM_ADMIN_USER: "nomadmin"
  IDM_ADMIN_KEY: "idm_nom_admin_password"
  SUITE_INSTALLER_PORT: "5443"
---
# Source: nomultimate/charts/nomcore/templates/register-backup.yaml
# we need to deploy SOMETHING in this file, to prevent errors during "helm delete" of empty file
apiVersion: v1
kind: ConfigMap
metadata:
  name: register-backup-cm
data:
  opsbridgeIntegration: "false"
  internalDatabase: "false"
  containerizedMode: "false"
  edition: ultimate
---
# Source: nomultimate/charts/nomzk/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: nomzk-cm
data:
  jvm.min.heap: 256M
  jvm.max.heap: 256M
  tick: "2000"
  init: "10"
  sync: "5"
  client.cnxns: "60"
  snap.retain: "3"
  purge.interval: "0"
---
# Source: nomultimate/templates/default-database-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  annotations:
  name: default-database-configmap
data:
  CREATE_DATABASE: "true"
  DB_SSL_ENABLED: "false"
  DEFAULT_DB_CA: ""
  DEFAULT_DB_CDFIDM_PASSWORD_KEY: defaultdb_cdfidm_user_password
  DEFAULT_DB_CDFIDM_USERNAME: cdfidm
  DEFAULT_DB_CONNECTION_URL: ""
  DEFAULT_DB_CONNECTIONSTRING: ""
  DEFAULT_DB_HA: "false"
  DEFAULT_DB_HOST: default-postgresql-svc.core
  DEFAULT_DB_NAME: defaultdb
  DEFAULT_DB_ORCWAY: ""
  DEFAULT_DB_PASSWORD_KEY: defaultdb_user_password
  DEFAULT_DB_PORT: "5432"
  DEFAULT_DB_SCHEMA: ""
  DEFAULT_DB_TYPE: postgresql
  DEFAULT_DB_USERNAME: postgres
  EMBEDDED_DB: "true"
---
# Source: nomultimate/templates/idm-conf-file.yaml
#
# This is the configuration for IDM.  The original source came from CDF "core" namespace, but
# now delivered with each NOM composition chart.
#

apiVersion: v1
kind: ConfigMap
metadata:
  annotations:
  name: idm-conf-file
data:
  com.microfocus.cdf__2018.11__Add_Update_User.json: |
        [
          {
            "operation":"ADD_OR_UPDATE",
            "type":"organization",
            "attributes": {
              "name":"Provider",
              "displayName":"Provider",
              "type":"PROVIDER"
            }
          },
          {
            "operation":"ADD_OR_UPDATE",
            "type":"organization",
            "attributes": {
              "name":"IdMIntegration",
              "displayName":"IdMIntegration",
              "type":"PROVIDER"
            }
          },
          {
            "operation":"ADD_OR_UPDATE",
            "type":"passwordPolicy",
            "names":{
              "organizationName": "IdMIntegration"
            },
            "attributes":{
              "name":"default_policy",
              "upperAndLowerCase": true,
              "numerical": true,
              "specialChar": true,
              "infoSensitive": false,
              "historyCheck": false,
              "lengthCheck": true,
              "expirationCheck": true,
              "minLength": 8,
              "maxLength": 20,
              "duration": 90
            }
          },
          {
            "operation":"ADD_OR_UPDATE",
            "type":"passwordPolicy",
            "names":{
              "organizationName": "Provider"
            },
            "attributes":{
              "name":"default_policy",
              "upperAndLowerCase": true,
              "numerical": true,
              "specialChar": true,
              "infoSensitive": false,
              "historyCheck": false,
              "lengthCheck": true,
              "expirationCheck": true,
              "minLength": 8,
              "maxLength": 20,
              "duration": 90
            }
          },
          {
            "operation":"ADD",
            "type":"permission",
            "attributes":{
              "name":"MNG_ADMIN",
              "displayName":"management portal admin",
              "description":"admin for management portal",
              "integration":"true",
              "type":"CONSUMER"
            }
          },
          {
            "operation":"ADD",
            "type":"permission",
            "attributes":{
              "name":"Autopass_ADMIN",
              "displayName":"Autopass portal admin",
              "description":"admin for Autopass",
              "integration":"true",
              "type":"CONSUMER"
            }
          },
          {
            "operation":"ADD",
            "type":"permission",
            "attributes":{
                "name":"Autopass_View",
                "displayName":"Autopass ViewOnly User",
                "description":" Autopass ViewOnly User ",
                "integration":"true",
                "type":"CONSUMER"
            }
          },
          {
            "operation":"ADD",
            "type":"role",
            "names":{
              "organizationName":"Provider"
            },
            "attributes":{
              "name":"idmAdminRole",
                "displayName":"Super IDM Admin",
                "description":"Super IDM Admin role"
            },
            "associations":[
              {
                "name":"SUPER_IDM_ADMIN",
                "type":"permission"
              }
            ]
          },
          {
            "operation":"ADD_OR_UPDATE",
            "type":"role",
            "names":{
              "organizationName":"Provider"
            },
            "attributes":{
              "name":"mngAdminRole",
              "displayName":"Management Portal Admin",
              "description":"Management Portal Admin role"
            },
            "associations":[
              {
                "name":"MNG_ADMIN",
                "type":"permission"
              }
            ]
          },
          {
            "operation":"ADD",
            "type":"role",
            "names":{
              "organizationName":"Provider"
            },
            "attributes":{
              "name":"superAdmin",
              "displayName":"Super IDM Admin",
              "description":"Super IDM Admin role"
            },
            "associations":[
              {
                "name":"SUPER_IDM_ADMIN",
                "type":"permission"
              }
            ]
          },
          {
            "operation":"ADD",
            "type":"role",
            "names":{
              "organizationName":"Provider"
            },
            "attributes":{
              "name":"bvd_admin",
              "displayName":"BVD Admin",
              "description":"BVD Admin Role"
            }
          },
          {
            "operation":"ADD_OR_UPDATE",
            "type":"role",
            "names":{
              "organizationName":"Provider"
            },
            "attributes":{
              "name":"nom_admin",
              "displayName":"NOM Admin",
              "description":"NOM Admin Role"
            },
            "associations":[
              {
                "name":"Autopass_ADMIN",
                "type":"permission"
              }
            ]
          },
          {
            "operation":"ADD",
            "type":"role",
            "names":{
              "organizationName":"Provider"
            },
            "attributes":{
              "name":"nom_level2",
              "displayName":"NOM Level 2",
              "description":"NOM Level 2 Role"
            }
          },
          {
            "operation":"ADD",
            "type":"role",
            "names":{
              "organizationName":"Provider"
            },
            "attributes":{
              "name":"nom_level1",
              "displayName":"NOM Level 1",
              "description":"NOM Level 1 Role"
            }
          },
          {
            "operation":"ADD",
            "type":"role",
            "names":{
              "organizationName":"Provider"
            },
            "attributes":{
              "name":"nom_guest",
              "displayName":"NOM Guest",
              "description":"NOM Guest Role"
            }
          },
          {
            "operation":"ADD",
            "type":"AbstractGroup",
            "names":{
              "organizationName":"Provider"
            },
            "attributes":{
              "name":"Administrators",
              "displayName":"Administrators",
              "groupInfo":"Administrators group",
              "description":"Administrators group"
            },
            "associations":[
              {
                "name":"idmAdminRole",
                "type":"role"
              },
              {
                "name":"mngAdminRole",
                "type":"role"
              }
            ]
          },
          {
            "operation":"ADD_OR_UPDATE",
            "type":"groupRepresentation",
            "names":{
              "organizationName":"Provider",
              "abstractGroupName": "Administrators"
            },
            "attributes":{
              "name": "Administrators",
              "displayName": "Administrators",
              "representationType": "DATABASE_GROUP_REPRESENTATION"
            }
          },
          {
            "operation":"ADD",
            "type":"AbstractGroup",
            "names":{
              "organizationName":"Provider"
            },
            "attributes":{
              "name":"SuiteAdministrators",
              "displayName":"Suite Administrators",
              "groupInfo":"Suite Administrators group",
              "description":"Suite Administrators group"
            },
            "associations":[
              {
                "name":"idmAdminRole",
                "type":"role"
              },
              {
                "name":"bvd_admin",
                "type":"role"
              }
            ]
          },
          {
            "operation":"ADD_OR_UPDATE",
            "type":"groupRepresentation",
            "names":{
              "organizationName":"Provider",
              "abstractGroupName": "SuiteAdministrators"
            },
            "attributes":{
              "name": "SuiteAdministrators",
              "displayName": "Suite Administrators",
              "representationType": "DATABASE_GROUP_REPRESENTATION"
            }
          },
          {
            "operation":"ADD",
            "type":"AbstractGroup",
            "names":{
              "organizationName":"Provider"
            },
            "attributes":{
              "name":"admin",
              "displayName":"NOM admin",
              "groupInfo":"NOM admin group",
              "description":"NOM admin group"
            },
            "associations":[
              {
                "name":"idmAdminRole",
                "type":"role"
              },
              {
                "name":"nom_admin",
                "type":"role"
              },
              {
                "name":"bvd_admin",
                "type":"role"
              }
            ]
          },
          {
            "operation":"ADD",
            "type":"AbstractGroup",
            "names":{
              "organizationName":"Provider"
            },
            "attributes":{
              "name":"level1",
              "displayName":"NOM operator level 1",
              "groupInfo":"NOM operator 1 group",
              "description":"NOM operator 1 group"
            },
            "associations":[
              {
                "name":"nom_level1",
                "type":"role"
              }
            ]
          },
          {
            "operation":"ADD",
            "type":"AbstractGroup",
            "names":{
              "organizationName":"Provider"
            },
            "attributes":{
              "name":"level2",
              "displayName":"NOM operator level 2",
              "groupInfo":"NOM operator 2 group",
              "description":"NOM operator 2 group"
            },
              "associations":[
              {
                "name":"nom_level2",
                "type":"role"
              }
            ]
          },
          {
            "operation":"ADD",
            "type":"AbstractGroup",
            "names":{
              "organizationName":"Provider"
            },
            "attributes":{
              "name":"guest",
              "displayName":"NOM guest",
              "groupInfo":"NOM guest group",
              "description":"NOM guest group"
            },
            "associations":[
              {
                "name":"nom_guest",
                "type":"role"
              }
            ]
          },
          {
            "operation":"ADD",
            "type":"AbstractGroup",
            "names":{
              "organizationName":"Provider"
            },
            "attributes":{
              "name":"globalops",
              "displayName":"NOM global ops",
              "groupInfo":"NOM global ops group",
              "description":"NOM global ops group"
            },
            "associations":[
            ]
          },
          {
            "operation":"ADD_OR_UPDATE",
            "type":"groupRepresentation",
            "names":{
              "organizationName":"Provider",
              "abstractGroupName": "admin"
            },
            "attributes":{
              "name": "admin",
              "displayName": "NOM Admin",
              "representationType": "DATABASE_GROUP_REPRESENTATION"
            }
          },
          {
            "operation":"ADD_OR_UPDATE",
            "type":"groupRepresentation",
            "names":{
              "organizationName":"Provider",
              "abstractGroupName": "admin"
            },
            "attributes":{
              "name": "calculatedAdmin",
              "displayName": "Calculated Admin",
              "representationType": "CALCULATED_GROUP_REPRESENTATION",
              "combinationStrategy": "AND"
            },
            "additional": [{
              "fieldName": "admin",
              "fieldValue": "admin",
              "modifier": "NONE"
            }]
          },
          {
            "operation":"ADD_OR_UPDATE",
            "type":"groupRepresentation",
            "names":{
              "organizationName":"Provider",
              "abstractGroupName": "level1"
            },
            "attributes":{
              "name": "level1",
              "displayName": "NOM operator, level 1",
              "representationType": "DATABASE_GROUP_REPRESENTATION"
            }
          },
          {
            "operation":"ADD_OR_UPDATE",
            "type":"groupRepresentation",
            "names":{
              "organizationName":"Provider",
              "abstractGroupName": "level1"
            },
            "attributes": {
              "name": "calculatedLevel1",
              "displayName": "Calculated NOM operator, level 1",
              "representationType": "CALCULATED_GROUP_REPRESENTATION",
              "combinationStrategy": "AND"
            },
            "additional":[{
                "fieldName": "level1",
                "fieldValue": "level1",
                "modifier": "NONE"
            }]
          },
          {
            "operation":"ADD_OR_UPDATE",
            "type":"groupRepresentation",
            "names":{
              "organizationName":"Provider",
              "abstractGroupName": "level2"
            },
            "attributes":{
              "name": "level2",
              "displayName": "NOM operator, level 2",
              "representationType": "DATABASE_GROUP_REPRESENTATION"
            }
          },
          {
            "operation":"ADD_OR_UPDATE",
            "type":"groupRepresentation",
            "names":{
              "organizationName":"Provider",
              "abstractGroupName": "level2"
            },
            "attributes": {
              "name": "calculatedLevel2",
              "displayName": "Calculated NOM operator, level 2",
              "representationType": "CALCULATED_GROUP_REPRESENTATION",
              "combinationStrategy": "AND"
            },
            "additional": [{
                "fieldName": "level2",
                "fieldValue": "level2",
                "modifier": "NONE"
            }]
          },
          {
            "operation":"ADD_OR_UPDATE",
            "type":"groupRepresentation",
            "names":{
              "organizationName":"Provider",
              "abstractGroupName": "guest"
            },
            "attributes":{
              "name": "guest",
              "displayName": "NOM guest",
              "representationType": "DATABASE_GROUP_REPRESENTATION"
            }
          },
          {
            "operation":"ADD_OR_UPDATE",
            "type":"groupRepresentation",
            "names":{
              "organizationName":"Provider",
              "abstractGroupName": "guest"
            },
            "attributes": {
              "name": "calculatedGuest",
              "displayName": "Calculated NOM guest",
              "representationType": "CALCULATED_GROUP_REPRESENTATION",
              "combinationStrategy": "AND"
            },
            "additional": [{
                "fieldName": "guest",
                "fieldValue": "guest",
                "modifier": "NONE"
            }]
          },
          {
            "operation":"ADD_OR_UPDATE",
            "type":"groupRepresentation",
            "names":{
              "organizationName":"Provider",
              "abstractGroupName": "globalops"
            },
            "attributes":{
              "name": "globalops",
              "displayName": "NOM global ops",
              "representationType": "DATABASE_GROUP_REPRESENTATION"
            }
          },
          {
            "operation":"ADD_OR_UPDATE",
            "type":"groupRepresentation",
            "names":{
              "organizationName":"Provider",
              "abstractGroupName": "globalops"
            },
            "attributes": {
              "name": "calculatedGlobalops",
              "displayName": "Calculated NOM global ops",
              "representationType": "CALCULATED_GROUP_REPRESENTATION",
              "combinationStrategy": "AND"
            },
            "additional": [{
                "fieldName": "globalops",
                "fieldValue": "globalops",
                "modifier": "NONE"
            }]
          },
          {
            "operation":"ADD",
            "type":"AbstractGroup",
            "names":{
              "organizationName":"Provider"
            },
            "attributes":{
              "name":"superIDMAdmins",
              "displayName":"Super IDM Admins",
              "groupInfo":"Super IDM Admins group",
              "description":"ASuper IDM Admins group"
            },
            "associations":[
              {
                "name":"superAdmin",
                "type":"role"
              }
            ]
          },
          {
            "operation":"ADD_OR_UPDATE",
            "type":"groupRepresentation",
            "names":{
              "organizationName":"Provider",
              "abstractGroupName": "superIDMAdmins"
            },
            "attributes":{
              "name": "superIDMAdmins",
              "displayName": "Super IDM Admins",
              "representationType": "DATABASE_GROUP_REPRESENTATION"
            }
          },
          {
            "operation":"ADD",
            "type":"databaseUser",
            "names":{
              "organizationName":"Provider"
            },
            "attributes":{
              "name":"admin",
              "password":"{idm_admin_admin_password}",
              "displayName": "Admin",
              "email": "admin@email.com",
              "common_name": "admin_common",
              "userEmail":"admin@userEmail.com"
            },
            "associations":[
              {
                "type": "group",
                "name": "Administrators"
              }
            ]
          },
          {
            "operation":"ADD",
            "type":"databaseUser",
            "names":{
              "organizationName":"Provider"
            },
            "attributes":{
              "name":"nomadmin",
              "password":"{idm_nom_admin_password}",
              "displayName": "NOM Admin",
              "email": "nomadmin@email.com",
              "common_name": "nomadmin_common",
              "userEmail":"nomadmin@userEmail.com"
            },
            "associations":[
              {
                "type": "group",
                "name": "SuiteAdministrators"
              },
              {
                "type": "group",
                "name": "admin"
              },
              {
                "type": "group",
                "name": "superIDMAdmins"
              }
            ]
          },
          {
            "operation":"ADD",
            "type":"databaseUser",
            "names":{
              "organizationName":"Provider"
            },
            "attributes":{
              "name":"opsbridge_admin",
              "password":"{idm_opsbridge_admin_password}",
              "userEmail":"opsbridge_admin@test.com",
              "type": "INTERNAL_SEEDED_USER"
            },
            "associations":[
              {
                "type": "group",
                "name": "superIDMAdmins"
              }
            ]
          },
          {
            "operation":"ADD",
            "type":"databaseUser",
            "names":{
              "organizationName":"Provider"
            },
            "attributes":{
              "name":"nom_admin",
              "password":"{idm_nom_admin_password}",
              "userEmail":"nom_admin@test.com",
              "type": "INTERNAL_SEEDED_USER"
            },
            "associations":[
              {
                "type": "group",
                "name": "superIDMAdmins"
              }
            ]
          },
          {
            "operation":"ADD",
            "type":"databaseUser",
            "names":{
              "organizationName":"Provider"
            },
            "attributes":{
              "name":"system",
              "password":"{NNM_SYS_PASSWD_KEY}",
              "displayName": "System",
              "email": "system@email.com",
              "common_name": "system_common",
              "userEmail":"system@userEmail.com"
            },
            "associations":[
              {
                "type": "group",
                "name": "superIDMAdmins"
              },
              {
                "type": "group",
                "name": "admin"
              }
            ]
          },
          {
            "operation": "ADD",
            "type": "databaseUser",
            "names": {
              "organizationName": "IdMIntegration"
            },
            "attributes": {
              "name": "transport_admin",
              "password": "{idm_transport_admin_password}",
              "type": "INTEGRATION_USER"
            }
          },
          {
            "operation": "ADD",
            "type": "databaseUser",
            "names": {
              "organizationName": "Provider"
            },
            "attributes": {
              "name": "transport_admin",
              "password": "{idm_transport_admin_password}",
              "type": "INTEGRATION_USER"
            }
          },
          {
            "operation": "ADD",
            "type": "databaseUser",
            "names": {
              "organizationName": "Provider"
            },
            "attributes": {
              "name": "integration_admin",
              "password": "{idm_integration_admin_password}",
              "type": "INTERNAL_SEEDED_USER"
            },
            "associations":[
              {
                "type": "group",
                "name": "superIDMAdmins"
              }
            ]
          },
          {
            "operation": "ADD_OR_UPDATE",
            "type": "organization",
            "attributes": {
              "name": "Provider",
              "type": "PROVIDER",
              "displayName": "NOM",
              "description": "Network Operations Management",
              "portalTitle": "Network Operations Management",
              "portalWelcomeMsg": "This is welcome message",
              "portalTermsOfUseUrl": "https://www.microfocus.com/en-us/legal/software-licensing",
              "iconUrl": "/static/images/nom-logo.png",
              "backgroundImageUrl": "/static/images/nom-login.jpg",
              "loginTheme": "stylishRight"
            }
          },
          {
            "operation":"ADD_OR_UPDATE",
            "type":"role",
            "names":{
              "organizationName":"Provider"
            },
            "attributes":{
              "name":"deploymentAdminRole",
              "displayName":"Deployment Admin",
              "description":"Deployment Admin role"
            },
            "associations":[
              {
                "name":"Administrators",
                "type":"group"
              },
              {
                "name":"SuiteAdministrators",
                "type":"group"
              },
              {
                "name":"superIDMAdmins",
                "type":"group"
              }
            ]
          },
          {
            "operation":"ADD_OR_UPDATE",
            "type":"role",
            "names":{
              "organizationName":"Provider"
            },
            "attributes":{
              "name":"dbMaintenanceRole",
              "displayName":"DB Maintenance",
              "description":"DB Maintenance role"
            },
            "associations":[
              {
                "name":"Administrators",
                "type":"group"
              },
              {
                "name":"SuiteAdministrators",
                "type":"group"
              },
              {
                "name":"superIDMAdmins",
                "type":"group"
              },
              {
                "name":"ROLE_REST",
                "type":"permission"
              }
            ]
          }
        ]
    

  com.microfocus.cdf__2019.02__Add_Update_User.json: |
        [
          {
            "operation": "ADD_OR_UPDATE",
            "type": "role",
            "names": {
              "organizationName": "Provider"
            },
            "attributes": {
              "name": "CLUSTER_ADMIN",
              "displayName": "CLUSTER ADMIN",
              "description": "CLUSTER ADMIN ROLE"
            },
            "associations": [
              {
                "name": "Administrators",
                "type": "group"
              },
              {
                "name": "SUPER_IDM_ADMIN",
                "type": "permission"
              },
              {
                "name": "MNG_ADMIN",
                "type": "permission"
              }
            ]
          },
          {
            "operation": "ADD_OR_UPDATE",
            "type": "role",
            "names": {
              "organizationName": "Provider"
            },
            "attributes": {
              "name": "DEPLOYMENT_ADMIN",
              "displayName": "DEPLOYMENT ADMIN",
              "description": "DEPLOYMENT ADMIN ROLE"
            },
            "associations": [
              {
                "name": "Administrators",
                "type": "group"
              },
              {
                "name": "SUPER_IDM_ADMIN",
                "type": "permission"
              },
              {
                "name": "MNG_ADMIN",
                "type": "permission"
              }
            ]
          },
          {
            "operation":"ADD_OR_UPDATE",
            "type":"databaseUser",
            "names":{
              "organizationName":"Provider",
              "databaseUserName":"integration_admin"
            },
            "attributes":{
              "name":"integration_admin"
            },
            "associations":[
              {
                "type": "group",
                "name": "Administrators"
              }
            ]
          },
          {
            "operation":"ADD_OR_UPDATE",
            "type":"databaseUser",
            "names":{
              "organizationName":"Provider",
              "databaseUserName":"opsbridge_admin"
            },
            "attributes":{
              "name":"opsbridge_admin"
            },
            "associations":[
              {
                "type": "group",
                "name": "Administrators"
              }
            ]
          },
          {
            "operation":"ADD_OR_UPDATE",
            "type":"databaseUser",
            "names":{
              "organizationName":"Provider",
              "databaseUserName":"nomadmin"
            },
            "attributes":{
              "name":"nomadmin"
            },
            "associations":[
              {
                "type": "group",
                "name": "Administrators"
              }
            ]
          },
          {
            "operation":"ADD_OR_UPDATE",
            "type":"databaseUser",
            "names":{
              "organizationName":"Provider",
              "databaseUserName":"nom_admin"
            },
            "attributes":{
              "name":"nom_admin"
            },
            "associations":[
              {
                "type": "group",
                "name": "Administrators"
              }
            ]
          },
          {
            "operation":"ADD_OR_UPDATE",
            "type":"databaseUser",
            "names":{
              "organizationName":"Provider",
              "databaseUserName":"system"
            },
            "attributes":{
              "name":"system"
            },
            "associations":[
              {
                "type": "group",
                "name": "Administrators"
              }
            ]
          }
        ]
    
  system_resource_config__1.29.1.0__update.json: |
    [{
      "operation": "UPDATE",
      "type": "systemResourceConfig",
      "names": {
        "systemResourceConfigName": "lwssoConfig.domain.mode"
      },
      "attributes": {
        "value": "hpsso"
      }
    }]
---
# Source: nomultimate/templates/nom-cert-cm.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: default-ca-certificates
data:
---
# Source: nomultimate/templates/nom-client-cert.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: api-client-ca-certificates
data:
---
# Source: nomultimate/templates/nom-cm.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: nom-ultimate-cm
data:
  version: "1.5.0+20211100.162"
  appVersion: "2021.02"
  acceptedEULA: "true"

  # NOM Ultimate is always mixed mode
  nom.mixedMode: "true"
  nnmi.host: "pxl0nnmi0011.dispositivos.bb.com.br"
  nnmi.port: "443"
  nnmi.url: "pxl0nnmi0011.dispositivos.bb.com.br:443"
  nnmi.user: "system"
  nnmi.passwordKey: "EXT_NNM_USER_PASSWD_KEY"

  # NNMi failover host is completely optional
  nnmi.failoverHost: "pxl0nnmi0009.dispositivos.bb.com.br"
  nnmi.failoverUrl: "pxl0nnmi0009.dispositivos.bb.com.br:443"
  na.host: "na.servicos.bb.com.br"
  na.port: "443"
  na.url: "na.servicos.bb.com.br:443"



  # if COSO, then Vertica is required (either embeddeded (POC only) or external)
  # copy Vertica connection details for NOM pods to use     #  external Vertica

  vertica.host: "pxl0nnmi0022.disposistivos.bb.com.br"
  vertica.port: "5443"
  vertica.db: "verticadb"
  vertica.username: "vertica_rwuser"
  vertica.passwordKey: "ITOMDI_DBA_PASSWORD_KEY"  # this should not be changed.
  vertica.username_ro: "vertica_rouser"
  vertica.passwordKey_ro: "ITOMDI_ROUSER_PASSWORD_KEY"  # this should not be changed.
  vertica.schema: "mf_shared_provider_default"
  vertica.useSsl: "false"

  # For MinIO
  minio.accessKey: "ITOMDI_MINIO_CLIENT_ACCESS_KEY"
  minio.secretKey: "ITOMDI_MINIO_CLIENT_SECRET_KEY"
  # For AWS

  # Determine the datasource to be used based on values provided by user in values.yaml

  # During upgrade if the user uses nomreportingcontent.sybase.*, inform user to change to new property

  # perfT flag selected but coso flag or sybase details not provided.

  # Coso datasource
  perfTroubleshooting.datasource: "coso"

  # NPS datasource

  # Both Coso and NPS datasources

  # determine if COSO, Traffic, and MinIO flags are all consistent
  

  # make sure deploySize is one of allowed values, or fail
  
  deploySize: large
---
# Source: nomultimate/templates/nom-databases-cm.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  annotations:
  name: nom-database-init
data:
  dbinit-script.sql: |
    create extension dblink; DO $do$ BEGIN
    IF EXISTS (SELECT FROM pg_database WHERE datname = 'APLS') THEN
    RAISE NOTICE 'Database already exists';
    ELSE
    PERFORM dblink_exec('dbname=' || current_database() , 'CREATE ROLE APLS PASSWORD NULL LOGIN');
    PERFORM dblink_exec('dbname=' || current_database() , 'CREATE DATABASE APLS OWNER APLS');
    END IF;
    IF EXISTS (SELECT FROM pg_database WHERE datname = 'BVD') THEN
    RAISE NOTICE 'Database already exists';
    ELSE
    PERFORM dblink_exec('dbname=' || current_database() , 'CREATE ROLE BVD PASSWORD NULL LOGIN');
    PERFORM dblink_exec('dbname=' || current_database() , 'CREATE DATABASE BVD OWNER BVD');
    END IF;
    IF EXISTS (SELECT FROM pg_database WHERE datname = 'IDM') THEN
    RAISE NOTICE 'Database already exists';
    ELSE
    PERFORM dblink_exec('dbname=' || current_database() , 'CREATE ROLE IDM PASSWORD NULL LOGIN');
    PERFORM dblink_exec('dbname=' || current_database() , 'CREATE DATABASE IDM OWNER IDM');
    END IF;
    IF EXISTS (SELECT FROM pg_database WHERE datname = 'BTCD') THEN
    RAISE NOTICE 'Database already exists';
    ELSE
    PERFORM dblink_exec('dbname=' || current_database() , 'CREATE ROLE BTCD PASSWORD NULL LOGIN');
    PERFORM dblink_exec('dbname=' || current_database() , 'CREATE DATABASE BTCD OWNER BTCD');
    END IF;
    END $do$;
---
# Source: nomultimate/templates/nom-pvc.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: RELEASE-NAME-nom-pvc-cm
data:
  version: 1.5.0.20211100.162
  persistenceEnabled: "true"
  dataVolume: "data-idm"
  configVolume: "config-idm"
  dbVolume: "db-idm"
  logVolume: "log-idm"
---
# Source: nomultimate/templates/secret-storage.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: secret-storage
  labels:
    name: secret-storage
data:
  TYPE: "k8s"
  SECRET_STORAGE_NAME: "nom-secret"
---
# Source: nomultimate/charts/autopass/templates/apls-rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: itom-autopass-lms
  namespace: default
rules:
- apiGroups:
  - ""
  resources:
  - pods
  - serviceaccounts
  - configmaps
  - secrets
  verbs:
  - get
  - list
  - watch
  - create
  - patch
---
# Source: nomultimate/charts/bvd/templates/bvd-rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: bvd
  namespace: default
rules:
- apiGroups:
  - ""
  resources:
  - pods
  - serviceaccounts
  - configmaps
  - secrets
  verbs:
  - get
  - list
  - watch
  - create
  - patch
---
# Source: nomultimate/charts/idm/templates/idm-rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: microfocus:cdf:itom-idm
  namespace: default
rules:
- apiGroups:
  - ""
  resources:
  - pods
  - serviceaccounts
  - configmaps
  - secrets
  verbs:
  - get
  - list
  - watch
  - create
  - update
  - patch
  - delete
  - deletecollection
---
# Source: nomultimate/charts/itom-di-udx-scheduler/templates/scheduler-rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: "itomdi:scheduler"
rules:
- apiGroups: [""]
  resources:  
   - configmaps
   - serviceaccounts
   - pods  
   - services
  verbs: ["get", "list", "watch"]  
- apiGroups: [""]
  resources:  
   - secrets
  verbs: ["get", "list", "watch", "patch"]  
- apiGroups: ["apps"]
  resources:
   - deployments    
   - replicasets
  verbs: ["get", "list", "watch"]
---
# Source: nomultimate/charts/itom-ingress-controller/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: microfocus:cdf:itom-ingress-controller
  namespace: default
rules:
  - apiGroups:
      - ""
    resources:
      - endpoints
      - pods
      - serviceaccounts
    verbs:
      - list
      - watch
      - get
  - apiGroups:
      - ""
    resources:
      - configmaps
      - secrets
    verbs:
      - list
      - watch
      - get
      - update
      - create
  - apiGroups:
      - ""
    resources:
      - namespaces
    verbs:
      - get
  - apiGroups:
      - ""
    resources:
      - services
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - "extensions"
      - "networking.k8s.io"
    resources:
      - ingresses
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
  - apiGroups:
      - "extensions"
      - "networking.k8s.io"
    resources:
      - ingresses/status
    verbs:
      - update
---
# Source: nomultimate/charts/itom-reloader/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: itom:reloader
  namespace: default
rules:
  - apiGroups:
      - ""
    resources:
      - secrets
      - configmaps
    verbs:
      - list
      - get
      - watch
  - apiGroups:
      - "apps"
    resources:
      - deployments
      - daemonsets
      - statefulsets
    verbs:
      - list
      - get
      - update
      - patch
  - apiGroups:
      - "extensions"
    resources:
      - deployments
      - daemonsets
    verbs:
      - list
      - get
      - update
      - patch
---
# Source: nomultimate/charts/itom-vault-client/templates/vault-client-rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name:  microfocus:cdf:itom-vault-client
  namespace: default
rules:
- apiGroups:
  - ""
  resources:
  - serviceaccounts
  verbs:
  - get
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - get
  - list
- apiGroups:
  - ""
  resources:
  - configmaps
  verbs:
  - get
  - list
- apiGroups:
  - ""
  resources:
  - secrets
  verbs:
  - get
  - list
  - patch
---
# Source: nomultimate/charts/itom-vault/templates/vault-rbac.yaml
# create cdf itom-vault role. Type: clusterrole, Name: microfocus:cdf:itom-vault
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: microfocus:cdf:itom-vault
  namespace: default
rules:
- apiGroups:
  - ""
  resources:
  - configmaps
  - secrets
  verbs:
  - get
  - list
  - watch
  - create
  - update
  - patch
  - delete
  - deletecollection
---
# Source: nomultimate/charts/itomdiadministration/templates/di-administration-rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: "itomdi:administration"
rules:
- apiGroups: [""]
  resources:  
   - configmaps
   - serviceaccounts
   - pods
   - services
  verbs: ["get", "list", "watch"]
- apiGroups: [""]
  resources:
   - secrets
  verbs: ["list", "watch", "get", "patch"]
- apiGroups: ["apps"]
  resources:
   - deployments
   - replicasets
  verbs: ["list", "watch", "get"]
---
# Source: nomultimate/charts/itomdidataaccess/templates/data-access-rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: "itomdi:data-access"
rules:
- apiGroups: [""]
  resources:  
   - configmaps
   - serviceaccounts
   - pods
   - endpoints
   - services
  verbs: ["get", "list", "watch"]
- apiGroups: [""]
  resources:
   - secrets
  verbs: ["list", "watch", "get", "patch"]
- apiGroups: ["apps"]
  resources:
   - deployments
   - replicasets
  verbs: ["list", "watch", "get"]
---
# Source: nomultimate/charts/itomdimetadataserver/templates/metadata-server-rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: "itomdi:metadata-server"
rules:
  - apiGroups: [""]
    resources:
      - configmaps
      - serviceaccounts
      - pods
      - endpoints
      - services
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources:
      - secrets
    verbs: ["list", "watch", "get", "patch"]
  - apiGroups: ["apps"]
    resources:
      - deployments
      - replicasets
    verbs: ["list", "watch", "get"]
---
# Source: nomultimate/charts/itomdiminio/templates/minio-rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: "itomdi:minio-role"
rules:
- apiGroups: [""]
  resources:
  - services
  - endpoints
  - pods
  - serviceaccounts
  - secrets
  - configmaps
  verbs: ["get", "list", "watch"]
- apiGroups: ["apps"]
  resources:
  - daemonsets
  - deployments
  - replicasets
  verbs: ["list", "watch", "update"]
---
# Source: nomultimate/charts/itomdimonitoring/templates/gen-certs-rbac.yaml
# Role with required permissions for creating/updating Prometheus scrape certs
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: "itomdimonitoring-gen-certs-role"
rules:
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - create
      - update
      - patch
---
# Source: nomultimate/charts/itomdimonitoring/templates/gen-certs-rbac.yaml
# Role with required permissions for vault-init, vault-renew containers
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: "itomdimonitoring-gen-certs-vault-role"
rules:
  - apiGroups:
      - ""
    resources:
      - pods
    verbs:
      - list
  - apiGroups:
      - ""
    resources:
      - serviceaccounts
    verbs:
      - get      
  - apiGroups:
      - ""
    resources:
      - configmaps
    verbs:
      - get
      - list
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - get
      - list
---
# Source: nomultimate/charts/itomdimonitoring/templates/monitoring-rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: "itomdimonitoring"
rules:
  - apiGroups:
      - ""
    resources:
      - pods
    verbs:
      - list
  - apiGroups:
      - ""
    resources:
      - serviceaccounts
    verbs:
      - get      
  - apiGroups:
      - ""
    resources:
      - configmaps
    verbs:
      - get
      - list
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - get
      - list
---
# Source: nomultimate/charts/itomdipostload/templates/postload-rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: "itomdi:postload"
rules:
- apiGroups: [""]
  resources:  
   - configmaps
   - serviceaccounts
   - pods  
   - services
  verbs: ["get", "list", "watch"]
- apiGroups: [""]
  resources:   
   - secrets
  verbs: ["list", "watch", "get", "patch"]   
- apiGroups: ["apps"]
  resources:
   - deployments    
   - replicasets
  verbs: ["get", "list", "watch"]
---
# Source: nomultimate/charts/itomdipulsar/templates/broker/broker-role-binding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: "itomdipulsar-broker"
  labels:
    app: itomdipulsar
    chart: itomdipulsar-2.8.1-31
    release: RELEASE-NAME
    heritage: Helm
    cluster: itomdipulsar
    app.kubernetes.io/name: itomdipulsar-broker
    app.kubernetes.io/managed-by: RELEASE-NAME
    app.kubernetes.io/version: 2.8.1-31
    itom.microfocus.com/capability: itom-data-ingestion
    tier.itom.microfocus.com/backend: backend
    component: broker

# Pulsar broker fires off a new stateful set to enable our functions module
# that enables the bulk load feature.
# (see example: https://github.houston.softwaregrp.net/itom-data-ingestion/pulsar/blob/pulsar-2.7.2/pulsar-broker/src/main/java/org/apache/pulsar/PulsarClusterMetadataSetup.java)
# In order to be able to manage the lifecycle of our bulk load in k8s, the broker needs broad latitude.
rules:
- apiGroups: [""]
  resources:  
   - configmaps
   - serviceaccounts
  verbs: ["get", "list", "watch"]
- apiGroups: [""]
  resources:
   - pods  
   - secrets
  verbs: ["list", "get"]
- apiGroups: [""]
  resources:  
   - services
  verbs: ["list", "watch", "get", "create", "delete"]
- apiGroups: ["apps"]
  resources:
   - deployments    
  verbs: ["list", "watch", "get"]
- apiGroups: ["apps"]
  resources:
   - statefulsets
  verbs: ["list", "watch", "get", "create", "delete","update","patch"]
---
# Source: nomultimate/charts/itomdipulsar/templates/coso/pulsar-common-rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: "itomdipulsar"
# These rules are designed primarily to allow vault to provide information
rules:
  - apiGroups:
      - ""
    resources:
      - pods
    verbs:
      - list
  - apiGroups:
      - ""
    resources:
      - serviceaccounts
    verbs:
      - get
  - apiGroups:
      - ""
    resources:
      - configmaps
    verbs:
      - get
      - list
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - get
      - list
---
# Source: nomultimate/templates/nom-role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: microfocus:itom:nom:update-k8s-secret
  namespace: default
rules:
- apiGroups:
  - ""
  resources:
  - secrets
  verbs:
  - update
---
# Source: nomultimate/charts/autopass/templates/apls-rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: itom-autopass-lms:itom-autopass-lms
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: itom-autopass-lms
subjects:
- kind: ServiceAccount
  name: itom-autopass-lms
  namespace: default
---
# Source: nomultimate/charts/bvd/templates/bvd-rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: bvd:bvd
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: bvd
subjects:
- kind: ServiceAccount
  name: bvd
  namespace: default
---
# Source: nomultimate/charts/bvd/templates/bvd-rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: bvd:vault-client
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: "microfocus:cdf:itom-vault-client"
subjects:
- kind: ServiceAccount
  name: bvd
  namespace: default
---
# Source: nomultimate/charts/idm/templates/idm-rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: microfocus:cdf:itom-idm
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: microfocus:cdf:itom-idm
subjects:
- kind: ServiceAccount
  name: itom-idm
  namespace: default
---
# Source: nomultimate/charts/itom-di-udx-scheduler/templates/scheduler-rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: "itomdi:scheduler"
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: "itomdi:scheduler"
subjects:
- kind: ServiceAccount
  name: itom-di-scheduler-sa  
  namespace: default
---
# Source: nomultimate/charts/itom-ingress-controller/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: microfocus:cdf:itom-ingress-controller
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: microfocus:cdf:itom-ingress-controller
subjects:
  - kind: ServiceAccount
    name: itom-ingress-controller
    namespace: default
---
# Source: nomultimate/charts/itom-reloader/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: itom-reloader-role-binding
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: itom:reloader
subjects:
  - kind: ServiceAccount
    name: itom-cdf-reloader
    namespace: default
---
# Source: nomultimate/charts/itom-vault/templates/vault-rbac.yaml
#create rolebinding for itom-vault service account in default namespace
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: microfocus:cdf:itom-vault
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: microfocus:cdf:itom-vault
subjects:
- kind: ServiceAccount
  name: itom-vault
  namespace: default
---
# Source: nomultimate/charts/itomdiadministration/templates/di-administration-rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: "itomdi:administration"
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: "itomdi:administration"
subjects:
- kind: ServiceAccount
  name: itom-di-administration-sa
  namespace: default
---
# Source: nomultimate/charts/itomdidataaccess/templates/data-access-rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: "itomdi:data-access"
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: "itomdi:data-access"
subjects:
- kind: ServiceAccount  
  name: itom-di-data-access-sa  
  namespace: default
---
# Source: nomultimate/charts/itomdimetadataserver/templates/metadata-server-rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: "itomdi:metadata-server"
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: "itomdi:metadata-server"
subjects:
- kind: ServiceAccount
  name: itom-di-metadata-server-sa
  namespace: default
---
# Source: nomultimate/charts/itomdiminio/templates/minio-rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: "itomdi:minio-rolebinding"
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: "itomdi:minio-role"
subjects:
- kind: ServiceAccount  
  name: itom-di-minio-sa  
  namespace: default
---
# Source: nomultimate/charts/itomdimonitoring/templates/gen-certs-rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: "itomdimonitoring-gen-certs-rb"
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: "itomdimonitoring-gen-certs-role"
subjects:
- kind: ServiceAccount
  name: itomdimonitoring-gen-certs-sa
  namespace: default
---
# Source: nomultimate/charts/itomdimonitoring/templates/gen-certs-rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: "itomdimonitoring-gen-certs-vault-rb"
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: "itomdimonitoring-gen-certs-vault-role"
subjects:
- kind: ServiceAccount
  name: itomdimonitoring-gen-certs-sa
  namespace: default
---
# Source: nomultimate/charts/itomdimonitoring/templates/monitoring-rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: "itomdimonitoring"
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: "itomdimonitoring"
subjects:
- kind: ServiceAccount
  name: itom-di-monitoring-sa
  namespace: default
---
# Source: nomultimate/charts/itomdipostload/templates/postload-rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: "itomdi:postload"
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: "itomdi:postload"
subjects:
- kind: ServiceAccount  
  name: itom-di-postload-sa  
  namespace: default
---
# Source: nomultimate/charts/itomdipulsar/templates/broker/broker-role-binding.yaml
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

## TODO create our own cluster role with less privledges than admin
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: "itomdipulsar-broker"
  labels:
    app: itomdipulsar
    chart: itomdipulsar-2.8.1-31
    release: RELEASE-NAME
    heritage: Helm
    cluster: itomdipulsar
    app.kubernetes.io/name: itomdipulsar-broker
    app.kubernetes.io/managed-by: RELEASE-NAME
    app.kubernetes.io/version: 2.8.1-31
    itom.microfocus.com/capability: itom-data-ingestion
    tier.itom.microfocus.com/backend: backend
    component: broker
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: "itomdipulsar-broker"
subjects:
- kind: ServiceAccount
  name: "itomdipulsar-broker-sa"
  namespace: default
---
# Source: nomultimate/charts/itomdipulsar/templates/coso/pulsar-common-rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: "itomdipulsar"
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: "itomdipulsar"
subjects:
- kind: ServiceAccount
  name: itomdipulsar-sa
  namespace: default
---
# Source: nomultimate/charts/itomnomcosodataaccess/templates/serviceaccount.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: itom-nom-coso-dl:itom-vault-client
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: microfocus:cdf:itom-vault-client
subjects:
  - kind: ServiceAccount
    name: itom-nom-coso-dl
    namespace: default
---
# Source: nomultimate/charts/nomapiserver/templates/serviceaccount.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: itom-nom-api-server:itom-vault-client
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: microfocus:cdf:itom-vault-client
subjects:
  - kind: ServiceAccount
    name: itom-nom-api-server
    namespace: default
---
# Source: nomultimate/charts/nomcore/templates/serviceaccount.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: nom-core:itom-vault-client
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: microfocus:cdf:itom-vault-client
subjects:
  - kind: ServiceAccount
    name: nom-core
    namespace: default
---
# Source: nomultimate/charts/nomhttp/templates/serviceaccount.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: itom-nom-http-backend:itom-vault-client
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: microfocus:cdf:itom-vault-client
subjects:
  - kind: ServiceAccount
    name: itom-nom-http-backend
    namespace: default
---
# Source: nomultimate/charts/nommetricstransform/templates/serviceaccount.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: itom-nom-metrics:itom-vault-client
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: microfocus:cdf:itom-vault-client
subjects:
  - kind: ServiceAccount
    name: itom-nom-metrics
    namespace: default
---
# Source: nomultimate/charts/nomreportingcontent/templates/serviceaccount.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: itom-nom-reporting:itom-vault-client
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: microfocus:cdf:itom-vault-client
subjects:
  - kind: ServiceAccount
    name: itom-nom-reporting
    namespace: default
---
# Source: nomultimate/charts/nomxui/templates/serviceaccount.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: nom-ui:itom-vault-client
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: microfocus:cdf:itom-vault-client
subjects:
  - kind: ServiceAccount
    name: nom-ui
    namespace: default
---
# Source: nomultimate/charts/nomzk/templates/serviceaccount.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: itom-nom-zookeeper:itom-vault-client
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: microfocus:cdf:itom-vault-client
subjects:
  - kind: ServiceAccount
    name: itom-nom-zookeeper
    namespace: default
---
# Source: nomultimate/charts/autopass/templates/apls-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: itom-autopass-lms
spec:
  sessionAffinity: ClientIP
  ports:
  - port: 5814
    protocol: TCP
    name: apls-port
#label keys and values that must match in order to receive traffic for this service
  selector:
    name: itom-autopass-lms
---
# Source: nomultimate/charts/bvd/templates/ap-bridge-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: bvd-ap-bridge
  namespace: default
  labels:
    service: bvd-ap-bridge
    app.kubernetes.io/name: bvd-ap-bridge
    app.kubernetes.io/managed-by: bvd-config
    app.kubernetes.io/version: 11.8.11
    itom.microfocus.com/capability: bvd  
    tier.itom.microfocus.com/autopassService: autopassService 
spec:
  ports:
  - name: bvd-ap-bridge
    port: 8080
    protocol: TCP
    targetPort: 8080
  selector:
    service: bvd-ap-bridge
---
# Source: nomultimate/charts/bvd/templates/controller-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: bvd-controller
  namespace: default
  labels:
    service: bvd-controller
    app.kubernetes.io/name: bvd-controller
    app.kubernetes.io/managed-by: bvd-config
    app.kubernetes.io/version: 11.8.11
    itom.microfocus.com/capability: bvd  
    tier.itom.microfocus.com/controllerService: controllerService
spec:
  ports:
  - name: bvd-controller
    port: 4000
    protocol: TCP
    targetPort: 4000
  selector:
    service: bvd-controller
---
# Source: nomultimate/charts/bvd/templates/explore-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: bvd-explore
  namespace: default
  labels:
    service: bvd-explore
    app.kubernetes.io/name: bvd-explore
    app.kubernetes.io/managed-by: bvd-config
    app.kubernetes.io/version: 11.8.11
    itom.microfocus.com/capability: bvd
    tier.itom.microfocus.com/exploreService: exploreService
spec:
  ports:
  - name: bvd-explore
    port: 4000
    protocol: TCP
    targetPort: 4000
  selector:
    service: bvd-explore
---
# Source: nomultimate/charts/bvd/templates/quexserv-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: bvd-quexserv
  namespace: default
  labels:
    service: bvd-quexserv
    app.kubernetes.io/name: bvd-quexserv
    app.kubernetes.io/managed-by: bvd-config
    app.kubernetes.io/version: 11.8.11
    itom.microfocus.com/capability: bvd  
    tier.itom.microfocus.com/queryServerService: queryServerService 
spec:
  ports:
  - name: quexserv
    port: 4000
    protocol: TCP
    targetPort: 4000
  selector:
    service: bvd-quexserv
---
# Source: nomultimate/charts/bvd/templates/receiver-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: bvd-receiver
  namespace: default
  labels:
    service: bvd-receiver
    app.kubernetes.io/name: bvd-receiver
    app.kubernetes.io/managed-by: bvd-config
    app.kubernetes.io/version: 11.8.11
    itom.microfocus.com/capability: bvd  
    tier.itom.microfocus.com/receiverService: receiverService
spec:
  ports:
  - name: data-receiver
    port: 4000
    protocol: TCP
    targetPort: 4000
  selector:
    service: bvd-receiver
---
# Source: nomultimate/charts/bvd/templates/redis-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: bvd-redis
  namespace: default
  labels:
    service: bvd-redis
    app.kubernetes.io/name: bvd-redis
    app.kubernetes.io/managed-by: bvd-config
    app.kubernetes.io/version: 11.8.11
    itom.microfocus.com/capability: bvd  
    tier.itom.microfocus.com/redisService: redisService
spec:
  ports:
  - name: bvd-redis
    port: 6380
    protocol: TCP
    targetPort: 6380
  - name: redis-exporter
    port: 4000
    protocol: TCP
    targetPort: redis-exporter    
  selector:
    service: bvd-redis
---
# Source: nomultimate/charts/bvd/templates/webtopdf-service.yaml
apiVersion: v1
kind: Service
metadata:
  annotations:
    description: itom print service
  labels:
    service: webtopdf
  name: webtopdf
spec:
  ports:
  - name: p1
    port: 3000
    protocol: TCP
    targetPort: 3000
  selector:
    service: webtopdf
---
# Source: nomultimate/charts/bvd/templates/www-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: bvd-www
  namespace: default
  labels:
    service: bvd-www
    app.kubernetes.io/name: bvd-www
    app.kubernetes.io/managed-by: bvd-config
    app.kubernetes.io/version: 11.8.11
    itom.microfocus.com/capability: bvd  
    tier.itom.microfocus.com/frontendService: frontendService 
spec:
  ports:
  - name: www-server
    port: 4000
    protocol: TCP
    targetPort: 4000
  selector:
    service: bvd-www
---
# Source: nomultimate/charts/idm/templates/idm-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: itom-idm-admin
  namespace: default
spec:
  ports:
  - name: ssl
    port: 18443
    protocol: TCP
    targetPort: 8443
  selector:
    app: itom-idm-app
---
# Source: nomultimate/charts/idm/templates/idm-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: itom-idm-svc
  namespace: default
spec:
  sessionAffinity: ClientIP
  ports:
    - port: 18443
      targetPort: 8443
      protocol: TCP
      name: ssl
    - port: 18444
      targetPort: 8444
      protocol: TCP
      name: ssl2
  selector:
    app: itom-idm-app
---
# Source: nomultimate/charts/idm/templates/idm-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: itom-idm
  namespace: default
spec:
  sessionAffinity: ClientIP
  ports:
    - port: 443
      targetPort: 8443
      protocol: TCP
      name: ssl
    - port: 444
      targetPort: 8444
      protocol: TCP
      name: ssl2
  selector:
    app: itom-idm-app
---
# Source: nomultimate/charts/itom-di-udx-scheduler/templates/scheduler-service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app: itomdischeduler
    app.kubernetes.io/name: itom-di-scheduler-svc
    app.kubernetes.io/managed-by: RELEASE-NAME
    app.kubernetes.io/version: 2.5.0-33
    itom.microfocus.com/capability: itom-data-ingestion
    tier.itom.microfocus.com/backend: backend
  name: itom-di-scheduler-svc
spec:
  ports:
    - name: scheduler-port
      port: 8443
      protocol: TCP
      targetPort: 8443
    - name: metrics
      port: 5555
      protocol: TCP
      targetPort: 8443
  selector:
    app: itom-di-scheduler-udx
---
# Source: nomultimate/charts/itom-ingress-controller/templates/service.yaml
# Compatible with deprecated k8sProvider key

# Default to cdf if not set


apiVersion: v1
kind: Service
metadata:
  name: itom-ingress-controller-svc
  namespace: default
  annotations:
spec:
  type: NodePort
  ports:
    - name: https
      protocol: TCP
      targetPort: 8443
      nodePort: 80
      port: 80
  selector:
    app.kubernetes.io/name: itom-ingress-controller
    app.kubernetes.io/instance: RELEASE-NAME
---
# Source: nomultimate/charts/itom-vault/templates/vault-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: itom-vault
  namespace: default
spec:
#  {VAULT_NODEPORT_TYPE}
  ports:
  - port: 8200
    targetPort: 8200
    name: https
#    {VAULT_NODEPORT_DEF}
  - port: 8201
    targetPort: 8201
    name: client
  # label keys and values that must match in order to receive traffic for this service
  selector:
    app: itom-vault-app
---
# Source: nomultimate/charts/itomdiadministration/templates/administration-nodeport.yaml
kind: Service
apiVersion: v1
metadata:
  name: itom-di-administration-nodeport
spec:
  type: NodePort
  ports:
    - port: 18443
      protocol: TCP
      name: itom-di-administration-nodeport
      targetPort: 8443
      nodePort: 30004
  selector:
    app: itom-di-administration
---
# Source: nomultimate/charts/itomdiadministration/templates/di-administration-service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/name: itom-di-administration-svc
    app.kubernetes.io/managed-by: RELEASE-NAME
    app.kubernetes.io/version: 2.5.0-46
    itom.microfocus.com/capability: itom-data-ingestion
    tier.itom.microfocus.com/backend: backend
  name: itom-di-administration-svc
  annotations:
    
spec:
  type: ClusterIP
  
  
  ports:
  - name: administration-port-old
    port: 8443
    protocol: TCP
    targetPort: 8443
  - name: administration-port
    port: 18443
    protocol: TCP
    targetPort: 8443
  selector:
    app: itom-di-administration
---
# Source: nomultimate/charts/itomdiadministration/templates/di-configuration-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: itom-di-configuration-svc
spec:
  ports:
  - name: configuration-port
    port: 8443
    protocol: TCP
    targetPort: 8443
  selector:
    app: itom-di-administration
---
# Source: nomultimate/charts/itomdidataaccess/templates/data-access-nodeport.yaml
kind: Service
apiVersion: v1
metadata:
  name: data-access-nodeport
spec:
  type: NodePort
  ports:
    - port: 28443
      targetPort: 8443
      nodePort: 30003
  selector:
    app: itom-di-data-access
---
# Source: nomultimate/charts/itomdidataaccess/templates/data-access-svc.yaml
kind: Service
apiVersion: v1
metadata:
  labels:
    app.kubernetes.io/name: itom-di-data-access-svc
    app.kubernetes.io/managed-by: RELEASE-NAME
    app.kubernetes.io/version: 2.5.0-34
    itom.microfocus.com/capability: itom-data-ingestion
    tier.itom.microfocus.com/backend: backend
  name: itom-di-data-access-svc
  annotations:
    
spec:
  type: ClusterIP
  
  
  ports:
  - name: data-access-port-old
    port: 8443
    protocol: TCP
    targetPort: 8443
  - name: data-access-port
    port: 28443
    targetPort: 8443
  selector:
    app: itom-di-data-access
---
# Source: nomultimate/charts/itomdimetadataserver/templates/metadata-server-deployment.yaml
kind: Service
apiVersion: v1
metadata:
  labels:
    app.kubernetes.io/name: itom-di-metadata-server-svc
    app.kubernetes.io/managed-by: RELEASE-NAME
    app.kubernetes.io/version: 2.5.0-25
    itom.microfocus.com/capability: itom-data-ingestion
    tier.itom.microfocus.com/backend: backend
  name: itom-di-metadata-server-svc
spec:
  selector:
    app: itom-di-metadata-server
  ports:
  - protocol: TCP
    name: metadata-server-port
    port: 8443
    targetPort: 8443
---
# Source: nomultimate/charts/itomdiminio/templates/service-distributed.yaml
apiVersion: v1
kind: Service
metadata:
  name: itom-di-minio-nodeport
  labels:
    app: itomdiminio
    chart: itomdiminio-2.5.0-21
    release: RELEASE-NAME
    heritage: Helm
    cluster: itom-di-minio
    app.kubernetes.io/name:  itom-di-minio
    app.kubernetes.io/managed-by: RELEASE-NAME
    app.kubernetes.io/version: 2.5.0-21
    itom.microfocus.com/capability: minio
    tier.itom.microfocus.com/backend: backend
  annotations:
    deployment.microfocus.com/default-replica-count: "4"
    deployment.microfocus.com/runlevel: UP
spec:
  type: NodePort
  ports:
    - name: service
      port: 9000
      protocol: TCP
      nodePort: 30006
  selector:
    app: itomdiminio
---
# Source: nomultimate/charts/itomdiminio/templates/statefulset.yaml
apiVersion: v1
kind: Service
metadata:
  name: itom-di-minio-svc
  labels:
    app: itomdiminio
    chart: itomdiminio-2.5.0-21
    release: RELEASE-NAME
    heritage: Helm
    cluster: itom-di-minio
    app.kubernetes.io/name:  itom-di-minio
    app.kubernetes.io/managed-by: RELEASE-NAME
    app.kubernetes.io/version: 2.5.0-21
    itom.microfocus.com/capability: minio
    tier.itom.microfocus.com/backend: backend
  annotations:
    deployment.microfocus.com/default-replica-count: "4"
    deployment.microfocus.com/runlevel: UP
    
spec:
  clusterIP: None
  ports:
    - name: service
      port: 9000
      protocol: TCP
    - name: metrics
      port: 5555
      protocol: TCP
      targetPort: 9000
  selector:
    app: itomdiminio
---
# Source: nomultimate/charts/itomdimonitoring/templates/vertica-prom-svc.yaml
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

apiVersion: v1
kind: Service
metadata:
  name: "itomdimonitoring-verticapromexporter-svc"
  namespace: default
  labels:
    app.kubernetes.io/name: "itomdimonitoring-verticapromexporter"
    app: itomdimonitoring
    app.kubernetes.io/managed-by: RELEASE-NAME
    app.kubernetes.io/version: 2.5.0-61
    itom.microfocus.com/capability: itom-data-ingestion
    tier.itom.microfocus.com/backend: backend
    chart: itomdimonitoring-2.5.0-61
    release: RELEASE-NAME
    heritage: Helm
    component: verticapromexporter
    cluster: itomdimonitoring
  annotations:
    deployment.microfocus.com/default-replica-count: "1"
    deployment.microfocus.com/runlevel: UP 
spec:
  type: ClusterIP
  ports:
  - name: metrics
    port: 8443
  - name: health
    port: 8080
  selector:
    app: itomdimonitoring
    release: RELEASE-NAME
    component: verticapromexporter
---
# Source: nomultimate/charts/itomdipostload/templates/postload-taskcontroller-service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app: itomdipostload
    app.kubernetes.io/name: itom-di-taskcontroller
    app.kubernetes.io/managed-by: RELEASE-NAME
    app.kubernetes.io/version: 2.5.0-50
    itom.microfocus.com/capability: itom-data-ingestion
    tier.itom.microfocus.com/backend: backend
  name: itom-di-taskcontroller-svc
spec:
  ports:
    - name: taskcontroller-port
      port: 8443
      protocol: TCP
      targetPort: 8443
    - name: metrics
      port: 5555
      protocol: TCP
      targetPort: 8443
  selector:
    app: itom-di-postload-taskcontroller
---
# Source: nomultimate/charts/itomdipostload/templates/postload-taskexecutor-service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app: itomdipostload
    app.kubernetes.io/name: itom-di-taskexecutor
    app.kubernetes.io/managed-by: RELEASE-NAME
    app.kubernetes.io/version: 2.5.0-50
    itom.microfocus.com/capability: itom-data-ingestion
    tier.itom.microfocus.com/backend: backend
  name: itom-di-taskexecutor-svc
spec:
  ports:
    - name: taskexecutor-port
      port: 8443
      protocol: TCP
      targetPort: 8443
    - name: metrics
      port: 5555
      protocol: TCP
      targetPort: 8443
  selector:
    app: itom-di-postload-taskexecutor
---
# Source: nomultimate/charts/itomdipulsar/templates/bastion/bastion-service.yaml
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

apiVersion: v1
kind: Service
metadata:
  name: "itomdipulsar-bastion"
  namespace: default
  labels:
    app: itomdipulsar
    chart: itomdipulsar-2.8.1-31
    release: RELEASE-NAME
    heritage: Helm
    cluster: itomdipulsar
    app.kubernetes.io/name: itomdipulsar-bastion
    app.kubernetes.io/managed-by: RELEASE-NAME
    app.kubernetes.io/version: 2.8.1-31
    itom.microfocus.com/capability: itom-data-ingestion
    tier.itom.microfocus.com/backend: backend
    component: bastion
spec:
  clusterIP: None
  selector:
    app: itomdipulsar
    release: RELEASE-NAME
    component: bastion
---
# Source: nomultimate/charts/itomdipulsar/templates/bookkeeper/bookkeeper-autorecovery-service.yaml
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

apiVersion: v1
kind: Service
metadata:
  name: "itomdipulsar-autorecovery"
  namespace: default
  labels:
    app: itomdipulsar
    chart: itomdipulsar-2.8.1-31
    release: RELEASE-NAME
    heritage: Helm
    cluster: itomdipulsar
    app.kubernetes.io/name: itomdipulsar-autorecovery
    app.kubernetes.io/managed-by: RELEASE-NAME
    app.kubernetes.io/version: 2.8.1-31
    itom.microfocus.com/capability: itom-data-ingestion
    tier.itom.microfocus.com/backend: backend
    component: autorecovery
spec:
  ports:
  - name: http
    port: 8000
  - name: metrics
    port: 5555
    targetPort: 8000
  clusterIP: None
  selector:
    app: itomdipulsar
    release: RELEASE-NAME
    component: autorecovery
---
# Source: nomultimate/charts/itomdipulsar/templates/bookkeeper/bookkeeper-service.yaml
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

apiVersion: v1
kind: Service
metadata:
  name: "itomdipulsar-bookkeeper"
  namespace: default
  labels:
    app: itomdipulsar
    chart: itomdipulsar-2.8.1-31
    release: RELEASE-NAME
    heritage: Helm
    cluster: itomdipulsar
    app.kubernetes.io/name: itomdipulsar-bookkeeper
    app.kubernetes.io/managed-by: RELEASE-NAME
    app.kubernetes.io/version: 2.8.1-31
    itom.microfocus.com/capability: itom-data-ingestion
    tier.itom.microfocus.com/backend: backend
    component: bookkeeper
  annotations:
    publishNotReadyAddresses: "true"
spec:
  ports:
  - name: bookie
    port: 3181
  - name: http
    port: 8000
  - name: metrics
    port: 5555
    targetPort: 8000
  clusterIP: None
  selector:
    app: itomdipulsar
    release: RELEASE-NAME
    component: bookkeeper
  # bookkeeper uses statefulset for getting stable bookie identifier.
  # it is okay to publish endpoints that are not ready because bookkeeper client
  # already has the ability to handle bookie failures.
  publishNotReadyAddresses: true
---
# Source: nomultimate/charts/itomdipulsar/templates/broker/broker-service.yaml
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

apiVersion: v1
kind: Service
metadata:
  name: "itomdipulsar-broker"
  namespace: default
  labels:
    app: itomdipulsar
    chart: itomdipulsar-2.8.1-31
    release: RELEASE-NAME
    heritage: Helm
    cluster: itomdipulsar
    app.kubernetes.io/name: itomdipulsar-broker
    app.kubernetes.io/managed-by: RELEASE-NAME
    app.kubernetes.io/version: 2.8.1-31
    itom.microfocus.com/capability: itom-data-ingestion
    tier.itom.microfocus.com/backend: backend
    component: broker
  annotations:
    {}
spec:
  ports:
  # prometheus needs to access /metrics endpoint
  - name: http
    port: 8080
  - name: https
    port: 8443
  - name: pulsarssl
    port: 6651
  - name: metrics
    port: 5555
    targetPort: 8443
  clusterIP: None
  selector:
    app: itomdipulsar
    release: RELEASE-NAME
    component: broker
---
# Source: nomultimate/charts/itomdipulsar/templates/proxy/proxy-service-nodeport.yaml
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

apiVersion: v1
kind: Service
metadata:
  name: "itomdipulsar-proxy"
  namespace: default
  labels:
    app: itomdipulsar
    chart: itomdipulsar-2.8.1-31
    release: RELEASE-NAME
    heritage: Helm
    cluster: itomdipulsar
    app.kubernetes.io/name: itomdipulsar-proxy
    app.kubernetes.io/managed-by: RELEASE-NAME
    app.kubernetes.io/version: 2.8.1-31
    itom.microfocus.com/capability: itom-data-ingestion
    tier.itom.microfocus.com/backend: backend
    component: proxy
  annotations:
       
      
spec:
  type: NodePort
  
  
  ports:
    - name: https
      port: 8443
      nodePort: 31001
      protocol: TCP
    - name: pulsarssl
      port: 6651
      nodePort: 31051
      protocol: TCP
  selector:
    app: itomdipulsar
    release: RELEASE-NAME
    component: proxy
---
# Source: nomultimate/charts/itomdipulsar/templates/proxy/proxy-service-svc.yaml
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

apiVersion: v1
kind: Service
metadata:
  name: "itomdipulsar-proxy-svc"
  namespace: default
  labels:
    app: itomdipulsar
    chart: itomdipulsar-2.8.1-31
    release: RELEASE-NAME
    heritage: Helm
    cluster: itomdipulsar
    app.kubernetes.io/name: itomdipulsar-proxy
    app.kubernetes.io/managed-by: RELEASE-NAME
    app.kubernetes.io/version: 2.8.1-31
    itom.microfocus.com/capability: itom-data-ingestion
    tier.itom.microfocus.com/backend: backend
    component: proxy
  annotations:
spec:
  ports:
    - name: metrics
      port: 8443
      protocol: TCP
  selector:
    app: itomdipulsar
    release: RELEASE-NAME
    component: proxy
---
# Source: nomultimate/charts/itomdipulsar/templates/zookeeper/zookeeper-service.yaml
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

# deploy zookeeper only when `components.zookeeper` is true
apiVersion: v1
kind: Service
metadata:
  name: "itomdipulsar-zookeeper"
  namespace: default
  labels:
    app: itomdipulsar
    chart: itomdipulsar-2.8.1-31
    release: RELEASE-NAME
    heritage: Helm
    cluster: itomdipulsar
    app.kubernetes.io/name: itomdipulsar-zookeeper
    app.kubernetes.io/managed-by: RELEASE-NAME
    app.kubernetes.io/version: 2.8.1-31
    itom.microfocus.com/capability: itom-data-ingestion
    tier.itom.microfocus.com/backend: backend
    component: zookeeper
  annotations:
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  ports:
    - name: follower
      port: 2888
    - name: leader-election
      port: 3888
    - name: client
      port: 2181
    - name: client-tls
      port: 2281
    - name: metrics
      port: 8000
  clusterIP: None
  selector:
    app: itomdipulsar
    release: RELEASE-NAME
    component: zookeeper
---
# Source: nomultimate/charts/itomnomcosodataaccess/templates/itom-coso-dl-data-access.yaml
apiVersion: v1
kind: Service
metadata:
  name: itom-coso-dl-data-access
  labels:
    name: itom-coso-dl-data-access
    configmap.reloader.stakater.com/reload: "default-ca-certificates"
spec:
  ports:
    - port: 8443
      protocol: TCP
      name: itom-coso-dl-data-access-https
      targetPort: 8443
  selector:
    name: itom-coso-dl-data-access
---
# Source: nomultimate/charts/nomapiserver/templates/itom-nom-api-server-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: itom-nom-api-server
  labels:
    name: itom-nom-api-server
spec:
  ports:
    - port: 8443
      protocol: TCP
      name: api-server-https
      targetPort: 8443
  selector:
    name: itom-nom-api-server
---
# Source: nomultimate/charts/nomhttp/templates/itom-nom-default-http-backend.yaml
apiVersion: v1
kind: Service
metadata:
  name: itom-nom-default-http-backend
  labels:
    name: itom-nom-default-http-backend
spec:
  ports:
  - port: 8080
    targetPort: 8080
  selector:
    name: itom-nom-default-http-backend
---
# Source: nomultimate/charts/nommetricstransform/templates/itom-nom-metric-transformation-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: itom-nom-metric-transformation
  labels:
    name: itom-nom-metric-transformation
    app.kubernetes.io/name: itom-nom-metric-transformation
    app.kubernetes.io/managed-by: RELEASE-NAME
    app.kubernetes.io/version: 1.4.90
    itom.microfocus.com/capability: itom-nom-metric-transformation
    tier.itom.microfocus.com/backend: backend
spec:
  ports:
  - port: 8443
    protocol: TCP
    name: metric-transformation-https
    targetPort: 8443
  - name: jmx-exporter-https
    port: 8686
    protocol: TCP
    targetPort: 8787
  selector:
    name: itom-nom-metric-transformation
---
# Source: nomultimate/charts/nomxui/templates/itom-nom-ui-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: itom-nom-ui
  labels:
    name: itom-nom-ui
spec:
  ports:
    - port: 8443
      protocol: TCP
      name: itom-nom-ui-port
      targetPort: 8443
  selector:
    name: itom-nom-ui
---
# Source: nomultimate/charts/nomzk/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: nomzk
  labels:
    app.kubernetes.io/name: nomzk
    name: itom-nom-zookeeper
spec:
  ports:
  - port: 2888
    name: server
  - port: 3888
    name: leader-election
  clusterIP: None
  selector:
    name: itom-nom-zookeeper
---
# Source: nomultimate/charts/nomzk/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: nomzk-client-svc
  labels:
    app.kubernetes.io/name: nomzk-client-svc
    name: itom-nom-zookeeper
spec:
  ports:
    - port: 2181
      name: client
  selector:
    name: itom-nom-zookeeper
---
# Source: nomultimate/charts/autopass/templates/apls-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: itom-autopass-lms
  labels:
    name: itom-autopass-lms
  annotations:
    deployment.microfocus.com/default-replica-count: "1"
    deployment.microfocus.com/runlevel: UP
    configmap.reloader.stakater.com/reload: "default-ca-certificates"
spec:
  replicas: 1
  selector:
    matchLabels:
      name: itom-autopass-lms
  template:
    metadata:
      labels:
        name: itom-autopass-lms
      annotations:
        pod.boostport.com/vault-approle: default-default
        pod.boostport.com/vault-init-container: install
    spec:
      serviceAccountName: itom-autopass-lms
      serviceAccount: itom-autopass-lms
      securityContext:
        runAsUser: 1999
        fsGroup: 1999
        runAsGroup: 1999
        supplementalGroups: [1999, 65000]
      initContainers:
      - name: waitfor-itom-vault-8200
        image: atf.intranet.bb.com.br:5001/hpeswitom/opensuse-base:15.3-0032
        imagePullPolicy: IfNotPresent
        command: [ "sh", "-c", "until nc -z itom-vault 8200 -w 5 ; do echo waiting for itom-vault:8200...; sleep 5; done; exit 0"]
        resources: {}

      - name: install
        image: atf.intranet.bb.com.br:5001/hpeswitom/kubernetes-vault-init:0.15.0-0038
        env:
        - name: CERT_COMMON_NAME
          value: Common_Name:itom-autopass-lms,Additional_SAN:itom-autopass-lms.default
        volumeMounts:
        - mountPath: /var/run/secrets/boostport.com
          name: vault-token
      - name: waitfor-silo12-master-postgr-5432
        image: atf.intranet.bb.com.br:5001/hpeswitom/opensuse-base:15.3-0032
        imagePullPolicy: IfNotPresent
        command: [ "sh", "-c", "until nc -z silo12-master.postgresql.bdh.servicos.bb.com.br 5432 -w 5 ; do echo waiting for silo12-master.postgresql.bdh.servicos.bb.com.br:5432...; sleep 5; done; exit 0"]
        resources: {}

      containers:
      - name: kubernetes-vault-renew
        image: atf.intranet.bb.com.br:5001/hpeswitom/kubernetes-vault-renew:0.15.0-0038
        imagePullPolicy: IfNotPresent
        volumeMounts:
        - name: vault-token
          mountPath: /var/run/secrets/boostport.com
      - name: itom-autopass-lms
        image: atf.intranet.bb.com.br:5001/hpeswitom/itom-autopass-lms:12.2.0-2021101213
        imagePullPolicy: IfNotPresent
        resources:
          requests:
            cpu: 100m
            memory: 500Mi
          limits:
            cpu: 2
            memory: 2048Mi
        volumeMounts:
        - name: apls-conf-vol
          mountPath: /config
          subPath: apls
        - name: apls-data-vol
          mountPath: /var/opt/autopass/apls/licenseserver/data
          subPath: apls/data
        - name: apls-log-vol
          mountPath: /logs
          subPath: apls
        - name: certificate-volume
          mountPath: /var/opt/autopass/apls/licenseserver/data/certs
        - name: vault-token
          mountPath: /var/run/secrets/boostport.com
        env:
        - name: DBTYPE
          value: postgres
        - name: schema
          value: "public"
        - name: DBNAME
          value: "autopass_db"
        - name: DBHOST
          value: "silo12-master.postgresql.bdh.servicos.bb.com.br"
        - name: DBPORT
          value: "5432"
        - name: DBUSER
          value: "user_autopass_db"
        - name: DBPASSWORD_KEY
          value: "APLS_DB_PASSWD_KEY"
        - name: DB_ENABLE_SSL
          value: "false"
        - name: tlsMode
          value: verify-full
        - name: LS_SERVER_CERT
          value: "server.crt"
        - name: LS_SERVER_KEY
          value: "server.key"
        - name: ENABLE_MULTITENANCY
          value: "false"
        - name: FIPS_ENABLED
          value: "false"
        - name: IDM_AUTH_PROVIDER_URL
          value: "https://autopass.homologacao.nuvem.hm.bb.com.br:443/idm-service"
        - name: IDM_SERVICE_URL
          value: "https://autopass.homologacao.nuvem.hm.bb.com.br:443/idm-service"
        - name: ENABLE_RBAC
          value: "true"
        - name: NUMRETRIES
          value: "6"
        - name: RETRYPERIOD
          value: "10"
        - name: RUNNING_MODE
          value: "DEV"
        - name: IDM_TENANT_ID
          value: "provider"
        - name: VAULT_SIGNING_KEY
          value: "IDM_SIGNING_KEY"
        - name: CALL_HOME
          value: "False"
        - name: IDM_TRANSPORT_USER
          value: transport_admin
        - name: VAULT_IDM_TRANSPORT_PASSWORD
          value: idm_transport_admin_password
        securityContext:
          privileged: false
        livenessProbe:
          httpGet:
          # when "host" is not defined, "PodIP" will be used
          # host: my-host
          # when "scheme" is not defined, "HTTP" scheme will be used. Only "HTTP" and "HTTPS" are allowed
            scheme: HTTPS
            path: /autopass/services/v10.1.1/version
            port: 5814
          initialDelaySeconds: 90
          timeoutSeconds: 1
          periodSeconds: 60
          successThreshold: 1
          failureThreshold: 5
        readinessProbe:
          httpGet:
          # when "host" is not defined, "PodIP" will be used
          # host: my-host
          # when "scheme" is not defined, "HTTP" scheme will be used. Only "HTTP" and "HTTPS" are allowed
            scheme: HTTPS
            path: /autopass/services/v10.1.1/version
            port: 5814
          initialDelaySeconds: 90
          timeoutSeconds: 15
          periodSeconds: 60
          failureThreshold: 20
      volumes:
      - name: apls-conf-vol
        persistentVolumeClaim:
          claimName: config-idm
      - name: apls-data-vol
        persistentVolumeClaim:
          claimName: data-idm
      - name: apls-log-vol
        persistentVolumeClaim:
          claimName: log-idm
      - name: vault-token
        emptyDir: {}
      - name: certificate-volume
        configMap:
          name: default-ca-certificates
---
# Source: nomultimate/charts/bvd/templates/ap-bridge-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: bvd-ap-bridge
  namespace: default
  annotations:
    deployment.microfocus.com/default-replica-count: "1"
    deployment.microfocus.com/runlevel: UP
  labels:
    service: bvd-ap-bridge
    app.kubernetes.io/name: bvd-ap-bridge
    app.kubernetes.io/managed-by: bvd-config
    app.kubernetes.io/version: 11.8.11
    itom.microfocus.com/capability: bvd  
    tier.itom.microfocus.com/middleware: middleware 
spec:
  replicas: 1
  selector:
    matchLabels:
      service: bvd-ap-bridge
      itom.microfocus.com/capability: bvd
  strategy: {}
  template:
    metadata:
      labels:
        service: bvd-ap-bridge
        itom.microfocus.com/capability: bvd
      annotations:
        pod.boostport.com/vault-approle: default-default
        pod.boostport.com/vault-init-container: install
    spec:
      serviceAccount: bvd
      serviceAccountName: bvd
      initContainers:
      - name: waitfor-bvd-controller-4000
        image: atf.intranet.bb.com.br:5001/hpeswitom/opensuse-base:15.3-0032
        imagePullPolicy: IfNotPresent
        command: [ "sh", "-c", "until nc -z bvd-controller 4000 -w 5 ; do echo waiting for bvd-controller:4000...; sleep 5; done; exit 0"]
        resources: {}
        securityContext:
          runAsNonRoot: true
          runAsUser: 1999
          runAsGroup: 1999
      - name: install
        image: atf.intranet.bb.com.br:5001/hpeswitom/kubernetes-vault-init:0.15.0-0038
        env:
        volumeMounts:
        - name: "vault-token"
          mountPath: /var/run/secrets/boostport.com
      containers:
      - image: atf.intranet.bb.com.br:5001/hpeswitom/itom-bvd-autopass-bridge:11.8.11
        name: bvd-ap-bridge
        ports:
        - containerPort: 8080
          protocol: TCP
        resources:
          requests:
            cpu: 0.1
            memory: 256Mi
          limits:
            memory: 1Gi
            cpu: 1
        volumeMounts:
          - name: vault-token
            mountPath: /var/run/secrets/boostport.com
        env:
          - name: "APLMS_DOMAIN_NAME"
            valueFrom:
              configMapKeyRef:
                name: bvd-services-config
                key: suite.autopasslicenseserver
      - name: kubernetes-vault-renew
        image: atf.intranet.bb.com.br:5001/hpeswitom/kubernetes-vault-renew:0.15.0-0038
        imagePullPolicy: IfNotPresent
        volumeMounts:
          - name: vault-token
            mountPath: /var/run/secrets/boostport.com
      securityContext:
        runAsNonRoot: true
        runAsUser: 1999
        runAsGroup: 1999
        fsGroup: 1999
      restartPolicy: Always

      volumes:
        - name: vault-token
          emptyDir: {}
---
# Source: nomultimate/charts/bvd/templates/controller-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: bvd-controller-deployment
  namespace: default
  annotations:
    deployment.microfocus.com/default-replica-count: "1"
    deployment.microfocus.com/runlevel: UP
  labels:
    service: bvd-controller
    app.kubernetes.io/name: bvdController
    app.kubernetes.io/managed-by: bvd-config
    app.kubernetes.io/version: 11.8.11
    itom.microfocus.com/capability: bvd
    tier.itom.microfocus.com/controller: controller
spec:
  replicas: 1
  selector:
    matchLabels:
      service: bvd-controller
      itom.microfocus.com/capability: bvd
  template:
    metadata:
      labels:
        service: bvd-controller
        itom.microfocus.com/capability: bvd
      annotations:
        pod.boostport.com/vault-approle: default-default
        pod.boostport.com/vault-init-container: install
        
        
        configmap.reloader.stakater.com/reload: "bvd-config,bvd-services-config,default-ca-certificates,default-ca-certificates"
    spec:
      serviceAccount: bvd
      serviceAccountName: bvd
      initContainers:
      - name: waitfor-bvd-redis-6380
        image: atf.intranet.bb.com.br:5001/hpeswitom/opensuse-base:15.3-0032
        imagePullPolicy: IfNotPresent
        command: [ "sh", "-c", "until nc -z bvd-redis 6380 -w 5 ; do echo waiting for bvd-redis:6380...; sleep 5; done; exit 0"]
        resources: {}
        securityContext:
          runAsNonRoot: true
          runAsUser: 1999
          runAsGroup: 1999
      - name: waitfor-itom-idm-svc-80
        image: atf.intranet.bb.com.br:5001/hpeswitom/opensuse-base:15.3-0032
        imagePullPolicy: IfNotPresent
        command: [ "sh", "-c", "until nc -z itom-idm-svc 80 -w 5 ; do echo waiting for itom-idm-svc:80...; sleep 5; done; exit 0"]
        resources: {}
        securityContext:
          runAsNonRoot: true
          runAsUser: 1999
          runAsGroup: 1999
      - name: install
        image: atf.intranet.bb.com.br:5001/hpeswitom/kubernetes-vault-init:0.15.0-0038
        env:
        - name: "CERT_COMMON_NAME"
          value: Realm:RID,Common_Name:bvd-controller,Additional_SAN:bvd-controller.default/bvd-controller.default.svc.cluster.local,File_Name:bvd-controller
        volumeMounts:
        - name: "vault-token"
          mountPath: /var/run/secrets/boostport.com
        securityContext:
          procMount: Default
          runAsNonRoot: true
          runAsUser: 1999
          runAsGroup: 1999
      - name: databasecreation
        args:
        - init
        env:
        - name: DEFAULT_FOUNDATION_ROLES
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.defaultFoundationRoles
              optional: true
        - name: BVD_ADMIN_LOGIN_KEY
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.adminname.key
        - name: BVD_ADMIN_PASSWORD_KEY
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.adminpassword.key
        - name: BVD_API_KEY_KEY
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.apikey.key
        - name: DEBUG
          value: "bvd:error*,bvd:audit*"
        - name: SYSTEM_USER_ID
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.systemUID
        - name: SYSTEM_GROUP_ID
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.systemGID
        - name: "BVD_CREATEDB"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.createDB
              optional: true
        - name: "BVD_DB_ADMIN_USER"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.dbAdminUser
              optional: true
        - name: "BVD_DB_ADMIN_PASSWORD_KEY"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.dbAdminPassword.key
              optional: true
        - name: "BVD_DB_ADMIN_DBNAME"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.dbAdmin.dbName
              optional: true
        - name: "BVD_DB_USER"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.dbuser
        - name: "POSTGRES_PASSWORD_KEY"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.dbpassword.key
        - name: "BVD_DB_HOST"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.dbhost
        - name: "BVD_DB_PORT"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.dbport
        - name: "BVD_DB_NAME"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.dbname
        - name: "BVD_DB_SID"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.dbsid
        - name: "BVD_DB_TYPE"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.dbType
        - name: "BVD_DB_CONNECTION_STRING"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.dbConnectionString
        - name: "BVD_DB_CA_BASE64_KEY"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.dbCa.base64.key
        - name: "BVD_DB_CA_BASE64"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.dbCa.base64
        - name: "BVD_DB_USE_TLS"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.dbUseTLS
        - name: "BVD_USE_TLS"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.UseTLS
        - name: "COLLECT_PROMETHEUS_METRICS"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.collectPrometheusMetrics
        - name: "NAMESPACE"
          valueFrom:
            configMapKeyRef:
              name: bvd-services-config
              key: suite.namespace
        - name: REDIS_PWD_KEY
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.redispassword.key
        - name: "APPROLE"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.approle
        - name: "ROLE_ID"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.approleid
        - name: "VAULT_ADDR"
          valueFrom:
            configMapKeyRef:
              name: bvd-services-config
              key: suite.vault_addr
        - name: "BVD_IDM_INTEGRATION_USER"
          valueFrom:
            configMapKeyRef:
              name: bvd-services-config
              key: suite.integration_user
        - name: "BVD_IDM_INTEGRATION_USER_PASSWORD_KEY"
          valueFrom:
            configMapKeyRef:
              name: bvd-services-config
              key: suite.integration_user_password_key
        - name: "IDM_ORGANIZATION"
          valueFrom:
            configMapKeyRef:
              name: bvd-services-config
              key: suite.idm_organization
        - name: "IDM_EXTERNAL_URL"
          valueFrom:
            configMapKeyRef:
              name: bvd-services-config
              key: suite.idm_external_url
        - name: "IDM_ADDR"
          valueFrom:
            configMapKeyRef:
              name: bvd-services-config
              key: suite.idm_addr
        - name: "EXTERNALNAME"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.externalname
        - name: "EXTERNALPORT"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.externalport
        volumeMounts:
          - name: vault-token
            mountPath: /var/run/secrets/boostport.com
          - name: bvd-controller-var
            mountPath: /var/bvd
            subPath: bvd/var/bvd
          
          - name: cert-volume
            mountPath: /var/bvd/certificates
          
          - name: global-cert-volume
            mountPath: /var/bvd/globalCertificates
          
        image: atf.intranet.bb.com.br:5001/hpeswitom/itom-bvd:11.8.11
        imagePullPolicy: IfNotPresent
        securityContext:
          runAsNonRoot: true
          procMount: Default
      terminationGracePeriodSeconds: 120
      containers:
      - name: bvd-controller
        args:
        - controller
        image: atf.intranet.bb.com.br:5001/hpeswitom/itom-bvd:11.8.11
        lifecycle:
          preStop:
            exec:
              command:
              - bash
              - -c
              - "kill -SIGTERM `ps -ef | grep -m 1 /bin/node | awk '{print $2}'`"
        livenessProbe:
          httpGet:
            path: /docker/alive
            port: 4000
            scheme: HTTPS
          initialDelaySeconds: 10
          timeoutSeconds: 10
          periodSeconds: 30
          successThreshold: 1
          failureThreshold: 10
        resources:
          requests:
            cpu: 0.1
            memory: 256Mi
          limits:
            memory: 1Gi
            cpu: 1
        ports:
        - containerPort: 4000
          protocol: TCP
        env:
        - name: BVD_ADMIN_LOGIN_KEY
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.adminname.key
        - name: BVD_ADMIN_PASSWORD_KEY
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.adminpassword.key
        - name: BVD_API_KEY_KEY
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.apikey.key
        - name: DEBUG
          value: "bvd:error*,bvd:audit*"
        - name: SYSTEM_USER_ID
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.systemUID
        - name: SYSTEM_GROUP_ID
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.systemGID
        - name: REDIS_PWD_KEY
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.redispassword.key
        - name: "BVD_DB_USER"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.dbuser
        - name: "POSTGRES_PASSWORD_KEY"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.dbpassword.key
        - name: "BVD_DB_HOST"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.dbhost
        - name: "BVD_DB_PORT"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.dbport
        - name: "BVD_DB_NAME"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.dbname
        - name: "BVD_DB_SID"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.dbsid
        - name: "BVD_DB_TYPE"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.dbType
        - name: "BVD_DB_CONNECTION_STRING"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.dbConnectionString
        - name: "BVD_DB_CA_BASE64_KEY"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.dbCa.base64.key
        - name: "BVD_DB_CA_BASE64"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.dbCa.base64
        - name: "BVD_DB_USE_TLS"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.dbUseTLS
        - name: "BVD_USE_TLS"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.UseTLS
        - name: "COLLECT_PROMETHEUS_METRICS"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.collectPrometheusMetrics
        - name: "NAMESPACE"
          valueFrom:
            configMapKeyRef:
              name: bvd-services-config
              key: suite.namespace
        - name: "APPROLE"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.approle
        - name: "ROLE_ID"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.approleid
        - name: "VAULT_ADDR"
          valueFrom:
            configMapKeyRef:
              name: bvd-services-config
              key: suite.vault_addr
        - name: "BVD_IDM_INTEGRATION_USER"
          valueFrom:
            configMapKeyRef:
              name: bvd-services-config
              key: suite.integration_user
        - name: "BVD_IDM_INTEGRATION_USER_PASSWORD_KEY"
          valueFrom:
            configMapKeyRef:
              name: bvd-services-config
              key: suite.integration_user_password_key
        - name: "IDM_ORGANIZATION"
          valueFrom:
            configMapKeyRef:
              name: bvd-services-config
              key: suite.idm_organization
        - name: "IDM_EXTERNAL_URL"
          valueFrom:
            configMapKeyRef:
              name: bvd-services-config
              key: suite.idm_external_url
        - name: "IDM_ADDR"
          valueFrom:
            configMapKeyRef:
              name: bvd-services-config
              key: suite.idm_addr
        - name: "EXTERNALNAME"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.externalname
        - name: "EXTERNALPORT"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.externalport
        volumeMounts:
          - name: vault-token
            mountPath: /var/run/secrets/boostport.com
          
          
          - name: cert-volume
            mountPath: /var/bvd/certificates
          
          - name: global-cert-volume
            mountPath: /var/bvd/globalCertificates
          
      - name: kubernetes-vault-renew
        image: atf.intranet.bb.com.br:5001/hpeswitom/kubernetes-vault-renew:0.15.0-0038
        imagePullPolicy: IfNotPresent
        volumeMounts:
          - name: vault-token
            mountPath: /var/run/secrets/boostport.com
        securityContext:
          runAsNonRoot: true
          runAsUser: 1999
          runAsGroup: 1999
      securityContext:
        runAsNonRoot: true
        runAsUser: 1999
        runAsGroup: 1999
        fsGroup: 1999

      volumes:
      - name: vault-token
        emptyDir: {}
      
      
      - name: cert-volume
        configMap:
          name: default-ca-certificates
      
      - name: global-cert-volume
        configMap:
          name: default-ca-certificates
      
      - name: bvd-controller-var
        persistentVolumeClaim:
          claimName: config-idm
---
# Source: nomultimate/charts/bvd/templates/explore-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: bvd-explore-deployment
  namespace: default
  labels:
    service: bvd-explore
    app.kubernetes.io/name: bvd-explore
    app.kubernetes.io/managed-by: bvd-config
    app.kubernetes.io/version: 11.8.11
    itom.microfocus.com/capability: bvd
    tier.itom.microfocus.com/explore: explore
  annotations:
    deployment.microfocus.com/default-replica-count: "1"
    deployment.microfocus.com/runlevel: UP
spec:
  replicas: 1
  selector:
    matchLabels:
      service: bvd-explore
      itom.microfocus.com/capability: bvd
  template:
    metadata:
      labels:
        service: bvd-explore
        itom.microfocus.com/capability: bvd
      annotations:
        pod.boostport.com/vault-approle: default-default
        pod.boostport.com/vault-init-container: install
        secret.reloader.stakater.com/reload: "nom-secret"
        
        
        configmap.reloader.stakater.com/reload: "bvd-config,bvd-services-config,default-ca-certificates,default-ca-certificates"
    spec:
      affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: service
                    operator: In
                    values:
                    - bvd-explore
                topologyKey: "kubernetes.io/hostname"
      serviceAccount: bvd
      serviceAccountName: bvd
      initContainers:
      - name: waitfor-bvd-controller-4000
        image: atf.intranet.bb.com.br:5001/hpeswitom/opensuse-base:15.3-0032
        imagePullPolicy: IfNotPresent
        command: [ "sh", "-c", "until nc -z bvd-controller 4000 -w 5 ; do echo waiting for bvd-controller:4000...; sleep 5; done; exit 0"]
        resources: {}
        securityContext:
          runAsNonRoot: true
          runAsUser: 1999
          runAsGroup: 1999
      - name: install
        image: atf.intranet.bb.com.br:5001/hpeswitom/kubernetes-vault-init:0.15.0-0038
        env:
        - name: "CERT_COMMON_NAME"
          value: Realm:RID,Common_Name:bvd-explore,Additional_SAN:bvd-explore.default/bvd-explore.default.svc.cluster.local,File_Name:bvd-explore
        volumeMounts:
        - name: "vault-token"
          mountPath: /var/run/secrets/boostport.com
        securityContext:
          runAsNonRoot: true
          runAsUser: 1999
          runAsGroup: 1999
      containers:
      - name: bvd-explore
        args:
        - explore
        image: atf.intranet.bb.com.br:5001/hpeswitom/itom-bvd:11.8.11
        lifecycle:
          preStop:
            exec:
              command:
              - bash
              - -c
              - "kill -SIGTERM `ps -ef | grep -m 1 /bin/node | awk '{print $2}'`"
        livenessProbe:
          httpGet:
            path: /docker/alive
            port: 4000
            scheme: HTTPS
          initialDelaySeconds: 5
          timeoutSeconds: 5
        readinessProbe:
          httpGet:
            path: /dashboard/docker/ready
            port: 4000
            scheme: HTTPS
          initialDelaySeconds: 10
          timeoutSeconds: 5
        resources:
          requests:
            cpu: 0.1
            memory: 256Mi
          limits:
            memory: 2Gi
            cpu: 2
        ports:
        - containerPort: 4000
          protocol: TCP
        env:
        - name: DEBUG
          value: "bvd:error*,bvd:audit*"
        - name: SYSTEM_USER_ID
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.systemUID
        - name: SYSTEM_GROUP_ID
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.systemGID
        - name: REDIS_PWD_KEY
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.redispassword.key
        - name: "BVD_DB_USER"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.dbuser
        - name: "POSTGRES_PASSWORD_KEY"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.dbpassword.key
        - name: "BVD_DB_HOST"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.dbhost
        - name: "BVD_DB_PORT"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.dbport
        - name: "BVD_DB_NAME"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.dbname
        - name: "BVD_DB_SID"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.dbsid
        - name: "BVD_DB_TYPE"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.dbType
        - name: "BVD_DB_CONNECTION_STRING"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.dbConnectionString
        - name: "BVD_DB_CA_BASE64_KEY"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.dbCa.base64.key
        - name: "BVD_DB_CA_BASE64"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.dbCa.base64
        - name: "BVD_DB_USE_TLS"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.dbUseTLS
        - name: "BVD_USE_TLS"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.UseTLS
        - name: "EXPLORE_CONTEXT_ROOT"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.exploreContextRoot
        - name: "TIME_FORMAT"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.timeFormat
              optional: true
        - name: "COLLECT_PROMETHEUS_METRICS"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.collectPrometheusMetrics
        - name: "NAMESPACE"
          valueFrom:
            configMapKeyRef:
              name: bvd-services-config
              key: suite.namespace
        - name: "APPROLE"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.approle
        - name: "ROLE_ID"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.approleid
        - name: "VAULT_ADDR"
          valueFrom:
            configMapKeyRef:
              name: bvd-services-config
              key: suite.vault_addr
        - name: "BVD_IDM_INTEGRATION_USER"
          valueFrom:
            configMapKeyRef:
              name: bvd-services-config
              key: suite.integration_user
        - name: "BVD_IDM_INTEGRATION_USER_PASSWORD_KEY"
          valueFrom:
            configMapKeyRef:
              name: bvd-services-config
              key: suite.integration_user_password_key
        - name: "IDM_TRANSPORT_KEY"
          valueFrom:
            configMapKeyRef:
              name: bvd-services-config
              key: suite.idm_transport_key
        - name: "BVD_IDM_TRANSPORT_USER"
          valueFrom:
            configMapKeyRef:
              name: bvd-services-config
              key: suite.idm_transport_user
        - name: "IDM_ORGANIZATION"
          valueFrom:
            configMapKeyRef:
              name: bvd-services-config
              key: suite.idm_organization
        - name: "IDM_EXTERNAL_URL"
          valueFrom:
            configMapKeyRef:
              name: bvd-services-config
              key: suite.idm_external_url
        - name: "PROM_HOST"
          value: 
        - name: "PROM_PORT"
          value: 
        - name: "BVD_EMBEDDING"
          value: "false"
        - name: "HTTPS_PROXY"
          value: 
        - name: "IDM_ADDR"
          valueFrom:
            configMapKeyRef:
              name: bvd-services-config
              key: suite.idm_addr
        - name: "EXTERNALNAME"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.externalname
        - name: "EXTERNALPORT"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.externalport
        - name: "SUITELOGO"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: suite.suitelogo
              optional: true
        - name: "SUITEFAVICON"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: suite.suitefavicon
              optional: true
        - name: "SUITENAME"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: suite.suitename
              optional: true
        - name: "SUITELOGO_LARGE"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: suite.logo_large
              optional: true
        - name: "SUITE_FAMILY_COLOR"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: suite.family_color
              optional: true
        - name: "SUITE_FAMILY_NAME"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: suite.family_name
              optional: true
        - name: "SUITE_FAMILY_ICON_LETTER"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: suite.family_icon_letter
              optional: true
        - name: "SUITE_RELEASE"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: suite.release
              optional: true
        - name: "BVD_VERTICA_TLS"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.defaultVerticaTLS
              optional: true
        - name: "BVD_VERTICA_USER_KEY"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.defaultVerticaUser.key
              optional: true
        - name: "BVD_VERTICA_USER"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.defaultVerticaUser
              optional: true
        - name: "BVD_VERTICA_PW_KEY"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.defaultVerticaPW.key
              optional: true
        - name: "BVD_VERTICA_CA_BASE64_KEY"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.defaultVerticaCABase64.key
              optional: true
        - name: "BVD_VERTICA_HOST"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.defaultVerticaHost
              optional: true
        - name: "BVD_VERTICA_PORT"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.defaultVerticaPort
              optional: true
        - name: "BVD_VERTICA_DB"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.defaultVerticaDB
              optional: true
        - name: BVD_ADMIN_ROLE
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.adminrole
        - name: FEATURE_TOGGLES
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.featureToggles
        - name: "START_OF_THE_WEEK"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.startOfTheWeek
              optional: true
        - name: "HELP_PRODUCT_ID"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.helpProductId
              optional: true
        - name: "BVD_SERVICE_URL"
          value: "https://bvd-www:4000/bvd"
        volumeMounts:
          - name: vault-token
            mountPath: /var/run/secrets/boostport.com
          
          
          - name: cert-volume
            mountPath: /var/bvd/certificates
          
          - name: global-cert-volume
            mountPath: /var/bvd/globalCertificates
          
          - name: bvd-explore-var
            mountPath: /var/bvd
            subPath: bvd/var/bvd
      - name: kubernetes-vault-renew
        image: atf.intranet.bb.com.br:5001/hpeswitom/kubernetes-vault-renew:0.15.0-0038
        imagePullPolicy: IfNotPresent
        volumeMounts:
          - name: vault-token
            mountPath: /var/run/secrets/boostport.com
        securityContext:
          runAsNonRoot: true
          runAsUser: 1999
          runAsGroup: 1999
      securityContext:
        runAsNonRoot: true
        runAsUser: 1999
        runAsGroup: 1999
        fsGroup: 1999

      volumes:
      - name: vault-token
        emptyDir: {}
      
      
      - name: cert-volume
        configMap:
          name: default-ca-certificates
      
      - name: global-cert-volume
        configMap:
          name: default-ca-certificates
      
      - name: bvd-explore-var
        persistentVolumeClaim:
          claimName: config-idm
---
# Source: nomultimate/charts/bvd/templates/quexserv-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: bvd-quexserv
  namespace: default
  annotations:
    deployment.microfocus.com/default-replica-count: "1"
    deployment.microfocus.com/runlevel: UP
  labels:
    service: bvd-quexserv
    app.kubernetes.io/name: bvd-quexserv
    app.kubernetes.io/managed-by: bvd-config
    app.kubernetes.io/version: 11.8.11
    itom.microfocus.com/capability: bvd
    tier.itom.microfocus.com/queryServer: queryServer
spec:
  replicas: 1
  selector:
    matchLabels:
      service: bvd-quexserv
      itom.microfocus.com/capability: bvd
  template:
    metadata:
      labels:
        service: bvd-quexserv
        itom.microfocus.com/capability: bvd
      annotations:
        pod.boostport.com/vault-approle: default-default
        pod.boostport.com/vault-init-container: install
        secret.reloader.stakater.com/reload: "nom-secret"
        
        
        configmap.reloader.stakater.com/reload: "bvd-config,bvd-services-config,default-ca-certificates,default-ca-certificates"
    spec:
      affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: service
                    operator: In
                    values:
                    - bvd-quexserv
                topologyKey: "kubernetes.io/hostname"
      serviceAccount: bvd
      serviceAccountName: bvd
      initContainers:
      - name: waitfor-bvd-controller-4000
        image: atf.intranet.bb.com.br:5001/hpeswitom/opensuse-base:15.3-0032
        imagePullPolicy: IfNotPresent
        command: [ "sh", "-c", "until nc -z bvd-controller 4000 -w 5 ; do echo waiting for bvd-controller:4000...; sleep 5; done; exit 0"]
        resources: {}
        securityContext:
          runAsNonRoot: true
          runAsUser: 1999
          runAsGroup: 1999
      - name: install
        image: atf.intranet.bb.com.br:5001/hpeswitom/kubernetes-vault-init:0.15.0-0038
        env:
        - name: "CERT_COMMON_NAME"
          value: Realm:RID,Common_Name:bvd-quexserv,Additional_SAN:bvd-quexserv.default/bvd-quexserv.default.svc.cluster.local,File_Name:bvd-quexserv
        volumeMounts:
        - name: "vault-token"
          mountPath: /var/run/secrets/boostport.com
      containers:
      - name: bvd-quexserv
        args:
        - quexserv
        image: atf.intranet.bb.com.br:5001/hpeswitom/itom-bvd:11.8.11
        lifecycle:
          preStop:
            exec:
              command:
              - bash
              - -c
              - "kill -SIGTERM `ps -ef | grep -m 1 /bin/node | awk '{print $2}'`"
        livenessProbe:
          httpGet:
            path: /docker/alive
            port: 4000
            scheme: HTTPS
          initialDelaySeconds: 5
          timeoutSeconds: 5
        resources:
          requests:
            cpu: 0.1
            memory: 256Mi
          limits:
            memory: 2Gi
            cpu: 2
        ports:
        - containerPort: 4000
          protocol: TCP
        env:
        - name: DEBUG
          value: "bvd:error*,bvd:audit*"
        - name: SYSTEM_USER_ID
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.systemUID
        - name: SYSTEM_GROUP_ID
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.systemGID
        - name: "NAMESPACE"
          valueFrom:
            configMapKeyRef:
              name: bvd-services-config
              key: suite.namespace
        - name: "APPROLE"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.approle
        - name: "ROLE_ID"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.approleid
        - name: "BVD_USE_TLS"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.UseTLS
        - name: "COLLECT_PROMETHEUS_METRICS"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.collectPrometheusMetrics
        - name: "VAULT_ADDR"
          valueFrom:
            configMapKeyRef:
              name: bvd-services-config
              key: suite.vault_addr
        - name: "REQ_TIMEOUT"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.reqTimeout
              optional: true
        - name: "VERTICA_POOL_MAX"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.dbMaxPoolSize
              optional: true
        - name: "VERTICA_POOL_MIN"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.dbMinPoolSize
              optional: true
        volumeMounts:
          - name: vault-token
            mountPath: /var/run/secrets/boostport.com
          
          
          
          - name: cert-volume
            mountPath: /var/bvd/certificates
          
          - name: global-cert-volume
            mountPath: /var/bvd/globalCertificates
      - name: kubernetes-vault-renew
        image: atf.intranet.bb.com.br:5001/hpeswitom/kubernetes-vault-renew:0.15.0-0038
        imagePullPolicy: IfNotPresent
        volumeMounts:
          - name: vault-token
            mountPath: /var/run/secrets/boostport.com
      securityContext:
        runAsNonRoot: true
        runAsUser: 1999
        runAsGroup: 1999
        fsGroup: 1999
      restartPolicy: Always
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: service
                  operator: In
                  values:
                  - bvd-quexserv
              topologyKey: "kubernetes.io/hostname"

      volumes:
      - name: vault-token
        emptyDir: {}
      
      
      
      - name: cert-volume
        configMap:
          name: default-ca-certificates
      
      - name: global-cert-volume
        configMap:
          name: default-ca-certificates
---
# Source: nomultimate/charts/bvd/templates/receiver-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: bvd-receiver-deployment
  namespace: default
  annotations:
    deployment.microfocus.com/default-replica-count: "1"
    deployment.microfocus.com/runlevel: UP
  labels:
    service: bvd-receiver
    app.kubernetes.io/name: bvd-receiver
    app.kubernetes.io/managed-by: bvd-config
    app.kubernetes.io/version: 11.8.11
    itom.microfocus.com/capability: bvd
    tier.itom.microfocus.com/receiver: receiver
spec:
  replicas: 1
  selector:
    matchLabels:
      service: bvd-receiver
      itom.microfocus.com/capability: bvd
  template:
    metadata:
      labels:
        service: bvd-receiver
        itom.microfocus.com/capability: bvd
      annotations:
        pod.boostport.com/vault-approle: default-default
        pod.boostport.com/vault-init-container: install
        secret.reloader.stakater.com/reload: "nom-secret"
        
        
        configmap.reloader.stakater.com/reload: "bvd-config,bvd-services-config,default-ca-certificates,default-ca-certificates"
    spec:
      affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: service
                    operator: In
                    values:
                    - bvd-receiver
                topologyKey: "kubernetes.io/hostname"
      serviceAccount: bvd
      serviceAccountName: bvd
      initContainers:
      - name: waitfor-bvd-controller-4000
        image: atf.intranet.bb.com.br:5001/hpeswitom/opensuse-base:15.3-0032
        imagePullPolicy: IfNotPresent
        command: [ "sh", "-c", "until nc -z bvd-controller 4000 -w 5 ; do echo waiting for bvd-controller:4000...; sleep 5; done; exit 0"]
        resources: {}
        securityContext:
          runAsNonRoot: true
          runAsUser: 1999
          runAsGroup: 1999
      - name: install
        image: atf.intranet.bb.com.br:5001/hpeswitom/kubernetes-vault-init:0.15.0-0038
        env:
        - name: "CERT_COMMON_NAME"
          value: Realm:RID,Common_Name:bvd-receiver,Additional_SAN:bvd-receiver.default/bvd-receiver.default.svc.cluster.local,File_Name:bvd-receiver
        volumeMounts:
        - name: "vault-token"
          mountPath: /var/run/secrets/boostport.com
        securityContext:
          runAsNonRoot: true
          runAsUser: 1999
          runAsGroup: 1999
      containers:
      - name: bvd-receiver
        args:
        - receiver
        image: atf.intranet.bb.com.br:5001/hpeswitom/itom-bvd:11.8.11
        lifecycle:
          preStop:
            exec:
              command:
              - bash
              - -c
              - "kill -SIGTERM `ps -ef | grep -m 1 /bin/node | awk '{print $2}'`"
        livenessProbe:
          httpGet:
            path: /docker/alive
            port: 4000
            scheme: HTTPS
          initialDelaySeconds: 5
          timeoutSeconds: 5
        readinessProbe:
          httpGet:
            path: /bvd-receiver/docker/ready
            port: 4000
            scheme: HTTPS
          initialDelaySeconds: 10
          timeoutSeconds: 5
        resources:
          requests:
            cpu: 0.1
            memory: 256Mi
          limits:
            memory: 2Gi
            cpu: 2
        ports:
        - containerPort: 4000
          protocol: TCP
        env:
        - name: DEBUG
          value: "bvd:error*,bvd:audit*"
        - name: SYSTEM_USER_ID
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.systemUID
        - name: SYSTEM_GROUP_ID
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.systemGID
        - name: REDIS_PWD_KEY
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.redispassword.key
        - name: "BVD_DB_USER"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.dbuser
        - name: "POSTGRES_PASSWORD_KEY"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.dbpassword.key
        - name: "BVD_DB_HOST"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.dbhost
        - name: "BVD_DB_PORT"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.dbport
        - name: "BVD_DB_NAME"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.dbname
        - name: "BVD_DB_SID"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.dbsid
        - name: "BVD_DB_TYPE"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.dbType
        - name: "BVD_DB_CONNECTION_STRING"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.dbConnectionString
        - name: "BVD_DB_CA_BASE64_KEY"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.dbCa.base64.key
        - name: "BVD_DB_CA_BASE64"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.dbCa.base64
        - name: "BVD_DB_USE_TLS"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.dbUseTLS
        - name: "BVD_USE_TLS"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.UseTLS
        - name: "BVD_CONTEXT_ROOT"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.bvdContextRoot
        - name: "COLLECT_PROMETHEUS_METRICS"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.collectPrometheusMetrics
        - name: "NAMESPACE"
          valueFrom:
            configMapKeyRef:
              name: bvd-services-config
              key: suite.namespace
        - name: "APPROLE"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.approle
        - name: "ROLE_ID"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.approleid
        - name: "VAULT_ADDR"
          valueFrom:
            configMapKeyRef:
              name: bvd-services-config
              key: suite.vault_addr
        - name: "BVD_IDM_INTEGRATION_USER"
          valueFrom:
            configMapKeyRef:
              name: bvd-services-config
              key: suite.integration_user
        - name: "BVD_IDM_INTEGRATION_USER_PASSWORD_KEY"
          valueFrom:
            configMapKeyRef:
              name: bvd-services-config
              key: suite.integration_user_password_key
        - name: "IDM_ORGANIZATION"
          valueFrom:
            configMapKeyRef:
              name: bvd-services-config
              key: suite.idm_organization
        - name: "IDM_EXTERNAL_URL"
          valueFrom:
            configMapKeyRef:
              name: bvd-services-config
              key: suite.idm_external_url
        - name: "IDM_ADDR"
          valueFrom:
            configMapKeyRef:
              name: bvd-services-config
              key: suite.idm_addr
        - name: "EXTERNALNAME"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.externalname
        - name: "EXTERNALPORT"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.externalport
        volumeMounts:
          - name: vault-token
            mountPath: /var/run/secrets/boostport.com
          
          
          - name: cert-volume
            mountPath: /var/bvd/certificates
          
          - name: global-cert-volume
            mountPath: /var/bvd/globalCertificates
          
      - name: kubernetes-vault-renew
        image: atf.intranet.bb.com.br:5001/hpeswitom/kubernetes-vault-renew:0.15.0-0038
        imagePullPolicy: IfNotPresent
        volumeMounts:
          - name: vault-token
            mountPath: /var/run/secrets/boostport.com
        securityContext:
          runAsNonRoot: true
          runAsUser: 1999
          runAsGroup: 1999
      securityContext:
        runAsNonRoot: true
        runAsUser: 1999
        runAsGroup: 1999
        fsGroup: 1999

      volumes:
      - name: vault-token
        emptyDir: {}
      
      
      - name: cert-volume
        configMap:
          name: default-ca-certificates
      
      - name: global-cert-volume
        configMap:
          name: default-ca-certificates
---
# Source: nomultimate/charts/bvd/templates/redis-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: bvd-redis
  namespace: default
  annotations:
    deployment.microfocus.com/default-replica-count: "1"
    deployment.microfocus.com/runlevel: DB
  labels:
    service: bvd-redis
    app.kubernetes.io/name: bvd-redis
    app.kubernetes.io/managed-by: bvd-config
    app.kubernetes.io/version: 11.8.11
    itom.microfocus.com/capability: bvd
    tier.itom.microfocus.com/redis: redis
spec:
  replicas: 1
  selector:
    matchLabels:
      service: bvd-redis
      itom.microfocus.com/capability: bvd
  strategy: {}
  template:
    metadata:
      labels:
        service: bvd-redis
        itom.microfocus.com/capability: bvd
      annotations:
        pod.boostport.com/vault-approle: default-default
        pod.boostport.com/vault-init-container: install
    spec:
      serviceAccount: bvd
      serviceAccountName: bvd
      initContainers:
      - name: waitfor-itom-vault-8200
        image: atf.intranet.bb.com.br:5001/hpeswitom/opensuse-base:15.3-0032
        imagePullPolicy: IfNotPresent
        command: [ "sh", "-c", "until nc -z itom-vault 8200 -w 5 ; do echo waiting for itom-vault:8200...; sleep 5; done; exit 0"]
        resources: {}
        securityContext:
          runAsNonRoot: true
          runAsUser: 1999
          runAsGroup: 1999
      - name: install
        image: atf.intranet.bb.com.br:5001/hpeswitom/kubernetes-vault-init:0.15.0-0038
        env:
        - name: "CERT_COMMON_NAME"
          value: Realm:RID,Common_Name:bvd-redis,Additional_SAN:bvd-redis.default/bvd-redis.default.svc.cluster.local,File_Name:bvd-redis
        volumeMounts:
        - name: "vault-token"
          mountPath: /var/run/secrets/boostport.com
      containers:
      - image: atf.intranet.bb.com.br:5001/hpeswitom/itom-redis:11.8.11
        livenessProbe:
          exec:
            command:
            - /bin/probeRedis.sh
          initialDelaySeconds: 5
          timeoutSeconds: 5
        name: bvd-redis
        env:
        - name: REDIS_CONF_KEY
          value: "redis_conf"
        - name: REDIS_PWD_KEY
          value: "redis_pwd"
        - name: CERT_COMMON_NAME
          value: "bvd-redis"
        - name: REDIS_APPENDONLY
          value: "no"
        ports:
        - containerPort: 6380
          protocol: TCP
        - containerPort: 9121
          name: redis-exporter
          protocol: TCP
        resources:
          requests:
            cpu: 0.1
            memory: 1Gi
          limits:
            memory: 2Gi
            cpu: 2
        volumeMounts:
          - name: bvd-redis-persistent-storage
            mountPath: /data
            subPath: redis-bvd/data
          - name: vault-token
            mountPath: /var/run/secrets/boostport.com
      - name: kubernetes-vault-renew
        image: atf.intranet.bb.com.br:5001/hpeswitom/kubernetes-vault-renew:0.15.0-0038
        imagePullPolicy: IfNotPresent
        volumeMounts:
          - name: vault-token
            mountPath: /var/run/secrets/boostport.com
      restartPolicy: Always
      securityContext:
        runAsNonRoot: true
        runAsUser: 1999
        runAsGroup: 1999
        fsGroup: 1999

      volumes:
      - name: vault-token
        emptyDir: {}
      - name: bvd-redis-persistent-storage
        persistentVolumeClaim:
          claimName: db-idm
---
# Source: nomultimate/charts/bvd/templates/webtopdf-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    description: itom print service server deployment
    deployment.microfocus.com/runlevel: UP
  labels:
    service: webtopdf
    app.kubernetes.io/name: webbtopdf
    app.kubernetes.io/managed-by: RELEASE-NAME
    app.kubernetes.io/version: 11.8.11
  name: webtopdf-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      service: webtopdf
  template:
    metadata:
      labels:
        service: webtopdf
        app.kubernetes.io/name: webtopdf
        app.kubernetes.io/managed-by: RELEASE-NAME
        app.kubernetes.io/version: 11.8.11
      annotations:
        pod.boostport.com/vault-approle: default-default
        pod.boostport.com/vault-init-container: install
        secret.reloader.stakater.com/reload: "nom-secret"
        
        
        configmap.reloader.stakater.com/reload: "bvd-config,bvd-services-config,default-ca-certificates,default-ca-certificates"


    spec:
      serviceAccount: bvd
      serviceAccountName: bvd
      securityContext:
        runAsNonRoot: true
        runAsUser: 1999
        fsGroup: 1999

      initContainers:
      - name: waitfor-itom-vault-8200
        image: atf.intranet.bb.com.br:5001/hpeswitom/opensuse-base:15.3-0032
        imagePullPolicy: IfNotPresent
        command: [ "sh", "-c", "until nc -z itom-vault 8200 -w 5 ; do echo waiting for itom-vault:8200...; sleep 5; done; exit 0"]
        resources: {}
        securityContext:
          runAsNonRoot: true
          runAsUser: 1999
          runAsGroup: 1999
      - name: waitfor-bvd-controller-4000
        image: atf.intranet.bb.com.br:5001/hpeswitom/opensuse-base:15.3-0032
        imagePullPolicy: IfNotPresent
        command: [ "sh", "-c", "until nc -z bvd-controller 4000 -w 5 ; do echo waiting for bvd-controller:4000...; sleep 5; done; exit 0"]
        resources: {}
        securityContext:
          runAsNonRoot: true
          runAsUser: 1999
          runAsGroup: 1999
      - name: install
        image: atf.intranet.bb.com.br:5001/hpeswitom/kubernetes-vault-init:0.15.0-0038
        env:
        - name: CERT_COMMON_NAME
          value: Realm:RID,Common_Name:webtopdf,File_Name:webtopdf
        volumeMounts:
        - mountPath: /var/run/secrets/boostport.com
          name: vault-token

      containers:
      - name: webtopdf
        args:
        - webtopdf
        image: atf.intranet.bb.com.br:5001/hpeswitom/itom-web-to-pdf:11.8.11
        imagePullPolicy: IfNotPresent
        readinessProbe:
          failureThreshold: 3
          httpGet:
            path: /webtopdf
            port: 3000
            scheme: HTTPS
          initialDelaySeconds: 1
          periodSeconds: 1
          successThreshold: 1
          timeoutSeconds: 5
        resources:
          null
        volumeMounts:
          - name: vault-token
            mountPath: /var/run/secrets/boostport.com
          
          
          - name: cert-volume
            mountPath: /var/bvd/certificates
          
          - name: global-cert-volume
            mountPath: /var/bvd/globalCertificates
          
          - name: ips-reports
            mountPath: /var/ips/reports
            subPath: ips/reports
          - name: ips-bufferfiles
            mountPath: /var/ips/bufferFiles
            subPath: ips/bufferFiles
          - name: dshm
            mountPath: /dev/shm
        ports:
        - containerPort: 3000
        env:
        - name: DEBUG
          value: "bvd:error*"
        - name: NAMESPACE
          value: "default"
        - name: VAULT_ADDR
          value: "https://itom-vault:8200"
        - name: IDM_TRANSPORT_KEY
          value: "idm_transport_admin_password"
        - name: IDM_TRANSPORT_USER
          value:  "transport_admin"
        - name: IDM_ORGANIZATION
          value: "Provider"
        - name: EXTERNAL_NAME
          value: "bvd.homologacao.nuvem.hm.bb.com.br"
        - name: IDM_EXTERNAL_NAME
          value: bvd.homologacao.nuvem.hm.bb.com.br
        - name: "IDM_EXTERNAL_PORT"
          value: "80"
        - name: "IPS_SERVER_PORT"
          value: "3000"
        - name: "BVD_IDM_INTEGRATION_USER"
          valueFrom:
            configMapKeyRef:
              name: bvd-services-config
              key: suite.integration_user
        - name: "BVD_IDM_INTEGRATION_USER_PASSWORD_KEY"
          valueFrom:
            configMapKeyRef:
              name: bvd-services-config
              key: suite.integration_user_password_key
        - name: REDIS_PWD_KEY
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.redispassword.key
        - name: "EXTERNALPORT"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.externalport
        - name: "BVD_CONTEXT_ROOT"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.bvdContextRoot
        - name: "POSTGRES_PASSWORD_KEY"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.dbpassword.key
        - name: "BVD_DB_USER"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.dbuser
        - name: "BVD_DB_HOST"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.dbhost
        - name: "BVD_DB_PORT"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.dbport
        - name: "BVD_DB_NAME"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.dbname
        - name: "BVD_DB_TYPE"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.dbType
        - name: "BVD_DB_SID"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.dbsid
        - name: "BVD_DB_CONNECTION_STRING"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.dbConnectionString
        - name: "BVD_DB_CA_BASE64_KEY"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.dbCa.base64.key
        - name: "BVD_DB_CA_BASE64"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.dbCa.base64
        - name: "BVD_DB_USE_TLS"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.dbUseTLS
        - name: "BVD_USE_TLS"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.UseTLS
        - name: SMTP_HOST
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.smtphost
              optional: true 
        - name: SMTP_PORT
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.smtpport
              optional: true 
        - name: SMTP_SECURITY
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.smtpsecurity
        - name: SMTP_USER
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.smtpuser
              optional: true
        - name: SMTP_PASSWORD_KEY
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.smtppassword.key
        - name: SYSTEM_USER_ID
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.systemUID
        - name: SYSTEM_GROUP_ID
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.systemGID
      - name: kubernetes-vault-renew
        image: atf.intranet.bb.com.br:5001/hpeswitom/kubernetes-vault-renew:0.15.0-0038
        imagePullPolicy: IfNotPresent
        volumeMounts:
        - name: vault-token
          mountPath: /var/run/secrets/boostport.com

      volumes:
      - name: ips-reports
        persistentVolumeClaim:
          claimName: config-idm
      - name: ips-bufferfiles
        persistentVolumeClaim:
          claimName: data-idm
      - name: vault-token
        emptyDir: {}
      
      
      - name: cert-volume
        configMap:
          name: default-ca-certificates
      
      - name: global-cert-volume
        configMap:
          name: default-ca-certificates
      
      - name: dshm
        emptyDir:
          medium: Memory
---
# Source: nomultimate/charts/bvd/templates/www-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: bvd-www-deployment
  namespace: default
  annotations:
    deployment.microfocus.com/default-replica-count: "1"
    deployment.microfocus.com/runlevel: UP
  labels:
    service: bvd-www
    app.kubernetes.io/name: bvd-www
    app.kubernetes.io/managed-by: bvd-config
    app.kubernetes.io/version: 11.8.11
    itom.microfocus.com/capability: bvd
    tier.itom.microfocus.com/frontend: frontend
spec:
  replicas: 1
  selector:
    matchLabels:
      service: bvd-www
      itom.microfocus.com/capability: bvd
  template:
    metadata:
      labels:
        service: bvd-www
        itom.microfocus.com/capability: bvd
      annotations:
        pod.boostport.com/vault-approle: default-default
        pod.boostport.com/vault-init-container: install
        secret.reloader.stakater.com/reload: "nom-secret"
        
        
        configmap.reloader.stakater.com/reload: "bvd-config,bvd-services-config,default-ca-certificates,default-ca-certificates"
    spec:
      affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: service
                    operator: In
                    values:
                    - bvd-www
                topologyKey: "kubernetes.io/hostname"
      serviceAccount: bvd
      serviceAccountName: bvd
      initContainers:
      - name: waitfor-bvd-controller-4000
        image: atf.intranet.bb.com.br:5001/hpeswitom/opensuse-base:15.3-0032
        imagePullPolicy: IfNotPresent
        command: [ "sh", "-c", "until nc -z bvd-controller 4000 -w 5 ; do echo waiting for bvd-controller:4000...; sleep 5; done; exit 0"]
        resources: {}
        securityContext:
          runAsNonRoot: true
          runAsUser: 1999
          runAsGroup: 1999
      - name: install
        image: atf.intranet.bb.com.br:5001/hpeswitom/kubernetes-vault-init:0.15.0-0038
        env:
        - name: "CERT_COMMON_NAME"
          value: Realm:RID,Common_Name:bvd-www,Additional_SAN:bvd-www.default/bvd-www.default.svc.cluster.local,File_Name:bvd-www
        volumeMounts:
        - name: "vault-token"
          mountPath: /var/run/secrets/boostport.com
        securityContext:
          runAsNonRoot: true
          runAsUser: 1999
          runAsGroup: 1999
      containers:
      - name: bvd-www
        args:
        - www
        image: atf.intranet.bb.com.br:5001/hpeswitom/itom-bvd:11.8.11
        lifecycle:
          preStop:
            exec:
              command:
              - bash
              - -c
              - "kill -SIGTERM `ps -ef | grep -m 1 /bin/node | awk '{print $2}'`"
        livenessProbe:
          httpGet:
            path: /docker/alive
            port: 4000
            scheme: HTTPS
          initialDelaySeconds: 5
          timeoutSeconds: 5
        readinessProbe:
          httpGet:
            path: /bvd/docker/ready
            port: 4000
            scheme: HTTPS
          initialDelaySeconds: 10
          timeoutSeconds: 5
        resources:
          requests:
            cpu: 0.1
            memory: 256Mi
          limits:
            memory: 2Gi
            cpu: 2
        ports:
        - containerPort: 4000
          protocol: TCP
        env:
        - name: DEBUG
          value: "bvd:error*,bvd:audit*"
        - name: SYSTEM_USER_ID
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.systemUID
        - name: SYSTEM_GROUP_ID
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.systemGID
        - name: REDIS_PWD_KEY
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.redispassword.key
        - name: "BVD_DB_USER"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.dbuser
        - name: "POSTGRES_PASSWORD_KEY"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.dbpassword.key
        - name: "BVD_DB_HOST"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.dbhost
        - name: "BVD_DB_PORT"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.dbport
        - name: "BVD_DB_NAME"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.dbname
        - name: "BVD_DB_SID"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.dbsid
        - name: "BVD_DB_TYPE"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.dbType
        - name: "BVD_DB_CONNECTION_STRING"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.dbConnectionString
        - name: "BVD_DB_CA_BASE64_KEY"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.dbCa.base64.key
        - name: "BVD_DB_CA_BASE64"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.dbCa.base64
        - name: "BVD_DB_USE_TLS"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.dbUseTLS
        - name: "BVD_USE_TLS"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.UseTLS
        - name: "COLLECT_PROMETHEUS_METRICS"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.collectPrometheusMetrics
        - name: "NAMESPACE"
          valueFrom:
            configMapKeyRef:
              name: bvd-services-config
              key: suite.namespace
        - name: "APPROLE"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.approle
        - name: "ROLE_ID"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.approleid
        - name: "BVD_CONTEXT_ROOT"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.bvdContextRoot
        - name: "SUITELOGO"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: suite.suitelogo
              optional: true
        - name: "SUITEFAVICON"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: suite.suitefavicon
              optional: true
        - name: "SUITENAME"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: suite.suitename
              optional: true
        - name: "SUITELOGO_LARGE"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: suite.logo_large
              optional: true
        - name: "SUITE_FAMILY_COLOR"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: suite.family_color
              optional: true
        - name: "SUITE_FAMILY_NAME"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: suite.family_name
              optional: true
        - name: "SUITE_FAMILY_ICON_LETTER"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: suite.family_icon_letter
              optional: true
        - name: "SUITE_PRINTTOOL"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: suite.printtool
              optional: true
        - name: "SUITE_RELEASE"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: suite.release
              optional: true
        - name: "VAULT_ADDR"
          valueFrom:
            configMapKeyRef:
              name: bvd-services-config
              key: suite.vault_addr
        - name: "BVD_IDM_INTEGRATION_USER"
          valueFrom:
            configMapKeyRef:
              name: bvd-services-config
              key: suite.integration_user
        - name: "BVD_IDM_INTEGRATION_USER_PASSWORD_KEY"
          valueFrom:
            configMapKeyRef:
              name: bvd-services-config
              key: suite.integration_user_password_key
        - name: "IDM_TRANSPORT_KEY"
          valueFrom:
            configMapKeyRef:
              name: bvd-services-config
              key: suite.idm_transport_key
        - name: "BVD_IDM_TRANSPORT_USER"
          valueFrom:
            configMapKeyRef:
              name: bvd-services-config
              key: suite.idm_transport_user
        - name: "IDM_ORGANIZATION"
          valueFrom:
            configMapKeyRef:
              name: bvd-services-config
              key: suite.idm_organization
        - name: "IDM_EXTERNAL_URL"
          valueFrom:
            configMapKeyRef:
              name: bvd-services-config
              key: suite.idm_external_url
        - name: "IDM_ADDR"
          valueFrom:
            configMapKeyRef:
              name: bvd-services-config
              key: suite.idm_addr
        - name: "EXTERNALNAME"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.externalname
        - name: "EXTERNALPORT"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.externalport
        - name: BVD_ADMIN_ROLE
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.adminrole
        - name: FEATURE_TOGGLES
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.featureToggles
        - name: "REQ_TIMEOUT"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.reqTimeout
              optional: true
        - name: "BVD_VERTICA_TLS"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.defaultVerticaTLS
              optional: true
        - name: "BVD_VERTICA_USER_KEY"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.defaultVerticaUser.key
              optional: true
        - name: "BVD_VERTICA_USER"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.defaultVerticaUser
              optional: true
        - name: "BVD_VERTICA_PW_KEY"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.defaultVerticaPW.key
              optional: true
        - name: "BVD_VERTICA_CA_BASE64_KEY"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.defaultVerticaCABase64.key
              optional: true
        - name: "BVD_VERTICA_HOST"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.defaultVerticaHost
              optional: true
        - name: "BVD_VERTICA_PORT"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.defaultVerticaPort
              optional: true
        - name: "BVD_VERTICA_DB"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.defaultVerticaDB
              optional: true
        - name: "BVD_EMBEDDING"
          value: "false"
        - name: "START_OF_THE_WEEK"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.startOfTheWeek
              optional: true
        - name: "HELP_PRODUCT_ID"
          valueFrom:
            configMapKeyRef:
              name: bvd-config
              key: bvd.helpProductId
              optional: true
        volumeMounts:
          - name: vault-token
            mountPath: /var/run/secrets/boostport.com
          
          
          - name: cert-volume
            mountPath: /var/bvd/certificates
          
          - name: global-cert-volume
            mountPath: /var/bvd/globalCertificates
          
          - name: bvd-www-var
            mountPath: /var/bvd
            subPath: bvd/var/bvd
      - name: kubernetes-vault-renew
        image: atf.intranet.bb.com.br:5001/hpeswitom/kubernetes-vault-renew:0.15.0-0038
        imagePullPolicy: IfNotPresent
        volumeMounts:
          - name: vault-token
            mountPath: /var/run/secrets/boostport.com
        securityContext:
          runAsNonRoot: true
          runAsUser: 1999
          runAsGroup: 1999
      securityContext:
        runAsNonRoot: true
        runAsUser: 1999
        runAsGroup: 1999
        fsGroup: 1999

      volumes:
      - name: vault-token
        emptyDir: {}
      
      
      - name: cert-volume
        configMap:
          name: default-ca-certificates
      
      - name: global-cert-volume
        configMap:
          name: default-ca-certificates
      
      - name: bvd-www-var
        persistentVolumeClaim:
          claimName: config-idm
---
# Source: nomultimate/charts/idm/templates/idm-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: itom-idm
  namespace: default
  labels:
    app: itom-idm-app
    deployments.microfocus.com/component: itom-idm
  annotations:
    deployment.microfocus.com/default-replica-count: "2"
    deployment.microfocus.com/runlevel: STANDBY
spec:
  replicas: 2
  selector:
    matchLabels:
      app: itom-idm-app
  template:
    metadata:
      labels:
        app: itom-idm-app
        deployments.microfocus.com/component: itom-idm
      annotations:
        pod.boostport.com/vault-approle: default-default
        pod.boostport.com/vault-init-container: install
    spec:
      securityContext:
        runAsUser: 1999
        runAsGroup: 1999
        fsGroup: 1999
        supplementalGroups: [1999]
      serviceAccountName: itom-idm
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - itom-idm-app
              topologyKey: "kubernetes.io/hostname"
      initContainers:
      - name: waitfor-itom-vault-8200
        image: atf.intranet.bb.com.br:5001/hpeswitom/itom-tools-base:1.0.0-0021
        imagePullPolicy: IfNotPresent
        command: [ "sh", "-c", "until nc -z itom-vault 8200 -w 5 ; do echo waiting for itom-vault:8200...; sleep 5; done; exit 0"]
        resources: {}
      - name: install
        image: atf.intranet.bb.com.br:5001/hpeswitom/kubernetes-vault-init:0.15.0-0038
        env:
        - name: CERT_COMMON_NAME
          value: Common_Name:itom-idm,Additional_SAN:itom-idm/itom-idm.default/itom-idm.default.svc.cluster.local/idm/idm.default/idm.default.svc.cluster.local/idm-svc/idm-svc.default/idm-svc.default.svc.cluster.local/itom-idm-svc/itom-idm-svc.default/itom-idm-svc.default.svc.cluster.local
        volumeMounts:
        - name: vault-token
          mountPath: /var/run/secrets/boostport.com
      - name: waitfor-silo12-master-postgr-5432
        image: atf.intranet.bb.com.br:5001/hpeswitom/itom-tools-base:1.0.0-0021
        imagePullPolicy: IfNotPresent
        command: [ "sh", "-c", "until nc -z silo12-master.postgresql.bdh.servicos.bb.com.br 5432 -w 5 ; do echo waiting for silo12-master.postgresql.bdh.servicos.bb.com.br:5432...; sleep 5; done; exit 0"]
        resources: {}
      containers:
      - name: kubernetes-vault-renew
        image: atf.intranet.bb.com.br:5001/hpeswitom/kubernetes-vault-renew:0.15.0-0038
        imagePullPolicy: IfNotPresent
        volumeMounts:
        - name: vault-token
          mountPath: /var/run/secrets/boostport.com
      - name: idm
        image: atf.intranet.bb.com.br:5001/hpeswitom/itom-idm:1.34.1-386
        imagePullPolicy: IfNotPresent
        resources:
          limits:
            cpu: 2
            memory: 1536Mi
          requests:
            cpu: 100m
            memory: 600Mi
        livenessProbe:
          httpGet:
            path: /idm-service/api/version-info
            port: 8443
            scheme: HTTPS
          initialDelaySeconds: 180
          periodSeconds: 30
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 30
        readinessProbe:
          httpGet:
            path: /idm-service/api/readiness
            port: 8443
            scheme: HTTPS
          initialDelaySeconds: 180
          periodSeconds: 30
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 3
        command:
        - sh
        - /startidm.sh
        env:
        - name: IDM_MOUNT_ROOT
          value: /etc/idm
        - name: CREATE_DATABASE
          value: "false"
        # DB connection details
        - name: DATABASE_HOST
          value: "silo12-master.postgresql.bdh.servicos.bb.com.br"
        - name: DATABASE_PORT
          value: "5432"
        - name: DATABASE_TYPE
          value: "postgresql"
        - name: DATABASE_NAME
          value: "idm"
        - name: DB_SSL_ENABLED
          value: "false"

        # IDM's database name, schema and user
        - name: DATABASE_USERNAME
          value: "user_idm_db"
        - name: DATABASE_PASSWORD_KEY
          value: "IDM_DB_PASSWD_KEY"
        - name: SYNC_PASSWORD_FROM_VAULT
          value: "true"
        - name: SEEDED_JSON_DIR
          value: /etc/seeded
        - name: HPSSO_INIT_STRING_ENCODING
          value: "Raw"
        - name: HPSSO_INIT_STRING_KEY
          value: "HPSSO_INIT_STRING_KEY"
        - name: VAULT_SIGNING_KEY
          value: "IDM_SIGNING_KEY"
        - name: HPSSO_DOMAIN_NAME
          value: "idm.homologacao.nuvem.hm.bb.com.br"
        - name: UPDATE_SYSTEM_RESOURCE_CONFIG
          value: "true"
        - name: IDM_CERT_FILE
          value: /var/run/secrets/boostport.com/server.crt
        - name: IDM_KEY_FILE
          value: /var/run/secrets/boostport.com/server.key
        - name: IDM_CA_FILE
          value: /var/run/secrets/boostport.com/issue_ca.crt
        - name: ENABLE_IDM_ADMIN
          value: "true"
        - name: ADMIN_IDM_SERVER_URL
          value: https://idm.homologacao.nuvem.hm.bb.com.br:/idm-service
        - name: ADMIN_IDM_SERVER_URL_INTERNAL
          value: https://itom-idm-svc:18443/idm-service
        - name: ADMIN_TENANT_NAME
          value: "provider"
        - name: ADMIN_INTEGRATION_USERNAME
          value: transport_admin
        - name: ADMIN_INTEGRATION_PASSWORD_VAULT_KEY
          value: idm_transport_admin_password
        - name: SYNC_VAULT_KEYS
          value: "idm.encryptedSigningKey=IDM_SIGNING_KEY,hpssoConfig.global.lwsso.crypto.initString=HPSSO_INIT_STRING_KEY"
        - name: MY_NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: MY_POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: MY_POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: MY_POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: MY_CONTAINER_NAME
          value: itom-idm
        - name: SECURITY_PROFILE
          value: legacy
        - name: ENABLE_FIPS
          value: 
        # Should be the one can be writen in container
        - name: IDM_LOG_PATH
          value: /var/logs/idm
        - name: NO_PROXY
          value: "127.0.0.1/localhost"
        # audit client configuration
        - name: AUDIT_CONFIG_PATH
          value: /var/logs/idm/audit_config
        - name: AUDIT_ENGINE_HOSTNAME
          value: 
        - name: AUDIT_ENGINE_PORT
          value: 
        - name: AUDIT_INTEGRATION_TENANT
          value: 
        - name: AUDIT_INTEGRATION_USER
          value: "integration_admin"
        - name: AUDIT_INTEGRATION_USER_PASSWORD_KEY
          value: "idm_integration_admin_password"
        - name: AUDIT_IDM_SVC_HOST
          value: 
        - name: AUDIT_IDM_SVC_PORT
          value: 
        - name: CREATE_AUDIT_ENABLED_FOR_CLIENT
          value: "true"
        - name: MOCK_AUDIT_ENABLE
          value: "false"
        - name: AUDIT_CLIENT_CONFIG_REFRESH_INTERVAL
          value: 
        volumeMounts:
        - name: suite-metadata
          mountPath: /var/data
        - name: cert-volume
          mountPath: /opt/assets/certificates
        - name: secret-volume
          mountPath: /opt/assets/secrets
        - name: seededdir
          mountPath: /etc/idm/seeded
          readOnly: true
        - name: vault-token
          mountPath: /var/run/secrets/boostport.com
        - name: log-dir
          mountPath: /var/logs
        - name: jvm-ext
          mountPath: /usr/lib/jvm/zulu-8-amd64/jre/lib/ext
        - name: jvm-security
          mountPath: /usr/lib/jvm/zulu-8-amd64/jre/lib/security
        - name: tomcat
          mountPath: /opt/apache-tomcat
        - name: idmtools
          mountPath: /idmtools
      volumes:
      - name: suite-metadata
        persistentVolumeClaim:
          claimName: data-idm
      - name: cert-volume
        configMap:
          name: default-ca-certificates
      - name: secret-volume
        secret:
          secretName: itom-idm
      - name: seededdir
        configMap:
          name: idm-conf-file
      - name: log-dir
        persistentVolumeClaim:
          claimName: log-idm
      - name: vault-token
        emptyDir: {}
      - name: jvm-ext
        emptyDir: {}
      - name: jvm-security
        emptyDir: {}
      - name: tomcat
        emptyDir: {}
      - name: idmtools
        emptyDir: {}
---
# Source: nomultimate/charts/itom-di-udx-scheduler/templates/scheduler-deployment.yaml
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

apiVersion: apps/v1
kind: Deployment
metadata:
  name: itom-di-scheduler-udx
  namespace: 
  labels:
    app: itom-di-scheduler-udx
    chart: itom-di-udx-scheduler-2.5.0-33
    release: RELEASE-NAME
    heritage: Helm
    component: scheduler
    cluster: itom-di-udx-scheduler
    app.kubernetes.io/name:  itom-di-udx-scheduler
    app.kubernetes.io/managed-by: RELEASE-NAME
    app.kubernetes.io/version: 2.5.0-33
  annotations:
    deployment.microfocus.com/default-replica-count: "1"
    deployment.microfocus.com/runlevel: UP
    configmap.reloader.stakater.com/reload: "default-ca-certificates"
spec:
  selector:
    matchLabels:
      app: itom-di-scheduler-udx
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
  template:
    metadata:
      labels:
        app: itom-di-scheduler-udx
        release: RELEASE-NAME
        component: scheduler
        cluster: itom-di-udx-scheduler
        app.kubernetes.io/name:  itom-di-udx-scheduler
        app.kubernetes.io/managed-by: RELEASE-NAME
        app.kubernetes.io/version: 2.5.0-33
      annotations:
        checksum/config: de2c44637c750cf1d5bad62eab29cebde15ceab4a081d46e344d928dfe79d194
        prometheus.io/port: "8443"
        prometheus.io/scrape: "true"
        prometheus.io/path: "/metrics"
        pod.boostport.com/vault-approle: default-default
        pod.boostport.com/vault-init-container: generate-certificates
    spec:      
      securityContext:
        runAsUser: 1999
        runAsGroup: 1999
        fsGroup: 1999
    
      serviceAccount: itom-di-scheduler-sa
      serviceAccountName: itom-di-scheduler-sa
    
      initContainers:
      - name: waitfor-itom-vault-8200
        image: atf.intranet.bb.com.br:5001/hpeswitom/opensuse-base:15.3-0032
        imagePullPolicy: IfNotPresent
        command: [ "sh", "-c", "until nc -z itom-vault 8200 -w 5 ; do echo waiting for itom-vault:8200...; sleep 5; done; exit 0"]
        resources: {}
        securityContext:
          runAsNonRoot: true
          runAsUser: 1999
          runAsGroup: 1999
      - name: waitfor-vertica-service
        image: atf.intranet.bb.com.br:5001/hpeswitom/itom-busybox:1.38.0-009
        imagePullPolicy: IfNotPresent
        command: [ "sh", "-c", "cmd=''; vh_local=pxl0nnmi0022.disposistivos.bb.com.br; vp_local=5443; for i in $(echo $vh_local | sed 's/,/ /g' | awk '{$1=$1};1'); do cmd=$(echo $cmd '|| nc -z '$i' '$vp_local' -w 5 '); done; cmd=${cmd:3}; echo 'command is : '$cmd''; until eval $(echo $cmd); do echo 'waiting for '$vh_local' with port '$vp_local' ... '; sleep 5; done; exit 0;"]
        resources: {}
        securityContext:
          runAsNonRoot: true
          runAsUser: 1999
          runAsGroup: 1999
      # This init container will generate certificates.
      - name: generate-certificates
        image: atf.intranet.bb.com.br:5001/hpeswitom/kubernetes-vault-init:0.15.0-0038
        imagePullPolicy: IfNotPresent
        env:
          - name: CERT_COMMON_NAME
            value: "itom-di-udx-scheduler-scheduler"
        volumeMounts:
        - name: vault-token
          mountPath: /var/run/secrets/boostport.com
        securityContext:
          runAsUser: 1999
          runAsGroup: 1999
      terminationGracePeriodSeconds: 90
      containers:
      - name: certificate-renew
        image: atf.intranet.bb.com.br:5001/hpeswitom/kubernetes-vault-renew:0.15.0-0038
        imagePullPolicy: IfNotPresent
        securityContext:
          runAsUser: 1999
          runAsGroup: 1999
        volumeMounts:
        - name: vault-token
          mountPath: /var/run/secrets/boostport.com



      - name: "itom-di-udx-scheduler-scheduler"
        image: atf.intranet.bb.com.br:5001/hpeswitom/itom-data-ingestion-udx-scheduler:2.5.0-33
        imagePullPolicy: IfNotPresent
        resources:
          limits:
            cpu: 1
            memory: 2Gi
          requests:
            cpu: 500m
            memory: 512Mi
        command: ["sh", "-c"]
        args:
        - >-
          /entrypoint.sh
        ports:
          - containerPort: 8443
        volumeMounts:
        - name: vault-token
          mountPath: /var/run/secrets/boostport.com
        - name: secret-volume
          mountPath: /mnt/itom/scheduler/certs
        - name: scheduler-conf-vol
          mountPath: /mnt/itom/scheduler/conf
          subPath: di/scheduler/conf
        - name: scheduler-log-vol
          mountPath: /mnt/itom/scheduler/log
        - name: cm-logback
          mountPath: /simplifiedscheduler/conf-local/
        envFrom:
          - configMapRef:
              name: "itom-di-udx-scheduler-scheduler"
        env:
        - name: SCHED_THREADS
          valueFrom:
            configMapKeyRef:
              name: "itom-di-udx-scheduler-scheduler"
              key: threads
        - name: SCHED_VERTICA_HOST
          valueFrom:
            configMapKeyRef:
              name: "itom-di-udx-scheduler-scheduler"
              key: vertica.datasource.host
        - name: SCHED_VERTICA_PORT
          valueFrom:
            configMapKeyRef:
              name: "itom-di-udx-scheduler-scheduler"
              key: vertica.datasource.port
        - name: SCHED_VERTICA_USER
          valueFrom:
            configMapKeyRef:
              name: "itom-di-udx-scheduler-scheduler"
              key: vertica.datasource.username
        - name: SCHED_VERTICA_DB
          valueFrom:
            configMapKeyRef:
              name: "itom-di-udx-scheduler-scheduler"
              key: vertica.datasource.database
        - name: SCHED_VERTICA_RESOURCE_POOL
          valueFrom:
            configMapKeyRef:
              name: "itom-di-udx-scheduler-scheduler"
              key: vertica.datasource.resourcepool
        - name: SCHED_VERTICA_PASSWORD_KEY
          valueFrom:
            configMapKeyRef:
              name: "itom-di-udx-scheduler-scheduler"
              key: vertica.datasource.password.key
        - name: SCHED_VERTICA_READONLY_USER
          valueFrom:
            configMapKeyRef:
              name: "itom-di-udx-scheduler-scheduler"
              key: vertica.datasource.readonly.username
        - name: SCHED_PULSAR_UDX_SCHEMA
          value: "itom_di_scheduler_provider_default"
        - name: SCHED_PULSAR_HOST
          valueFrom:
           configMapKeyRef:
             name: "itom-di-udx-scheduler-scheduler"
             key: pulsar.datasource.host
        - name: SCHED_PULSAR_PORT
          valueFrom:
            configMapKeyRef:
              name: "itom-di-udx-scheduler-scheduler"
              key: pulsar.datasource.port
        - name: SCHED_ADMIN_HOST
          valueFrom:
            configMapKeyRef:
              name: "itom-di-udx-scheduler-scheduler"
              key: administration.datasource.host
        - name: SCHED_ADMIN_PORT
          valueFrom:
            configMapKeyRef:
              name: "itom-di-udx-scheduler-scheduler"
              key: administration.datasource.port
        - name: SCHED_PULSAR_ADMIN_HOST
          valueFrom:
            configMapKeyRef:
              name: "itom-di-udx-scheduler-scheduler"
              key: pulsar.admin.datasource.host
        - name: SCHED_PULSAR_ADMIN_PORT
          valueFrom:
            configMapKeyRef:
              name: "itom-di-udx-scheduler-scheduler"
              key: pulsar.admin.datasource.port
        - name: SCHED_VERTICA_IS_TLS_ENABLED
          valueFrom:
            configMapKeyRef:
              name: "itom-di-udx-scheduler-scheduler"
              key: vertica.datasource.is.tls.enabled
        - name: SCHED_VERTICA_TLS_MODE
          value: "none"
        - name: SCHEDULER_LOG_LEVEL
          valueFrom:
            configMapKeyRef:
              name: "itom-di-udx-scheduler-scheduler"
              key: scheduler.log.level
        - name: UDX_LOG_LEVEL
          valueFrom:
            configMapKeyRef:
              name: "itom-di-udx-scheduler-scheduler"
              key: scheduler.udx.log.level
        - name: SCHEDULER_ENABLE_PERFORMANCE_TEST
          valueFrom:
            configMapKeyRef:
              name: "itom-di-udx-scheduler-scheduler"
              key: scheduler.enable.performance.test
        - name: SCHEDULER_JDBC_OPT
          valueFrom:
            configMapKeyRef:
              name: "itom-di-udx-scheduler-scheduler"
              key: scheduler.jdbc.opt
        - name: SCHEDULER_EXPLICIT_STATEMENT_EXECUTE
          valueFrom:
            configMapKeyRef:
              name: "itom-di-udx-scheduler-scheduler"
              key: scheduler.explicit.statement.execute
        - name: SCHEDULER_ENABLE_RESTART_SCHEDULER_JOB
          valueFrom:
            configMapKeyRef:
              name: "itom-di-udx-scheduler-scheduler"
              key: scheduler.enable.restart.scheduler.job
        - name: SCHEDULER_MAX_MEMORY_USAGE
          valueFrom:
            configMapKeyRef:
              name: "itom-di-udx-scheduler-scheduler"
              key: scheduler.max.memory.usage.percentage
        - name: SCHEDULER_DATA_RETENTION_JOB_CRON_SCHEDULE
          valueFrom:
            configMapKeyRef:
              name: "itom-di-udx-scheduler-scheduler"
              key: scheduler.data.retention.job.cron.schedule
        - name: SCHEDULER_ENABLE_DATA_RETENTION_JOB
          valueFrom:
            configMapKeyRef:
              name: "itom-di-udx-scheduler-scheduler"
              key: scheduler.enable.data.retention.job
        - name: SCHEDULER_MAX_MINUTES_PAST_LAST_BATCH
          valueFrom:
            configMapKeyRef:
              name: "itom-di-udx-scheduler-scheduler"
              key: scheduler.max.minutes.past.last.batch.update
        - name: SCHEDULER_RESTART_TIME_INTERVAL_IN_HOURS
          valueFrom:
            configMapKeyRef:
              name: "itom-di-udx-scheduler-scheduler"
              key: scheduler.restart.time.interval.in.hours
        - name: SCHEDULER_PARSER_PARAMETERS
          valueFrom:
            configMapKeyRef:
              name: "itom-di-udx-scheduler-scheduler"
              key: scheduler.parser.parameters
        - name: SCHEDULER_CONFIGSERVER_HOSTNAME
          valueFrom:
            configMapKeyRef:
              name: "itom-di-udx-scheduler-scheduler"
              key: scheduler.configserver.hostname
        - name: SCHEDULER_CONFIGSERVER_PORT
          valueFrom:
            configMapKeyRef:
              name: "itom-di-udx-scheduler-scheduler"
              key: scheduler.configserver.port
        - name: SCHEDULER_CONFIGSERVER_CONNECTION_RETRY_DELAY_MS
          valueFrom:
            configMapKeyRef:
              name: "itom-di-udx-scheduler-scheduler"
              key: scheduler.configserver.connection.retry.delay.ms
        - name: SCHEDULER_CONFIGSERVER_CLIENT_HEART_BEAT
          valueFrom:
            configMapKeyRef:
              name: "itom-di-udx-scheduler-scheduler"
              key: scheduler.configserver.client.heart.beat
        - name: SCHEDULER_CONFIGSERVER_SERVER_HEART_BEAT
          valueFrom:
            configMapKeyRef:
              name: "itom-di-udx-scheduler-scheduler"
              key: scheduler.configserver.server.heart.beat
        - name: SCHEDULER_CONFIGSERVER_MESSAGE_BUFFER_SIZE_LIMIT_MB
          valueFrom:
            configMapKeyRef:
              name: "itom-di-udx-scheduler-scheduler"
              key: scheduler.configserver.message.buffer.size.limit.mb
        - name: SCHEDULER_FRAME_DURATION
          valueFrom:
            configMapKeyRef:
              name: "itom-di-udx-scheduler-scheduler"
              key: scheduler.frame.duration
        - name: SCHEDULER_UDX_PULSAR_CLIENT_CLEANUP
          valueFrom:
            configMapKeyRef:
              name: "itom-di-udx-scheduler-scheduler"
              key: scheduler.udx.pulsar.client.cleanup
        - name: SCHEDULER_UDX_PULSAR_CLIENT_CLEANUP_THRESHOLD
          valueFrom:
            configMapKeyRef:
              name: "itom-di-udx-scheduler-scheduler"
              key: scheduler.udx.pulsar.client.cleanup.threshold
        - name: SCHEDULER_LANE_WORKER_CONNECTION_CLOSE_THRESHOLD
          valueFrom:
            configMapKeyRef:
              name: "itom-di-udx-scheduler-scheduler"
              key: scheduler.lane.worker.connection.close.threshold
        - name: SCHEDULER_VDB_CONNECTION_CLOSE_THRESHOLD
          valueFrom:
            configMapKeyRef:
              name: "itom-di-udx-scheduler-scheduler"
              key: scheduler.vdb.connection.close.threshold
        - name: SCHEDULER_ALIVE_CHECK_TIME_INTERVAL_SECONDS
          valueFrom:
            configMapKeyRef:
              name: "itom-di-udx-scheduler-scheduler"
              key: scheduler.alive.check.time.interval.seconds
        - name: PULSAR_TOPIC_PARTITION_AUTO_REFRESH_ENABLED
          valueFrom:
            configMapKeyRef:
              name: "itom-di-udx-scheduler-scheduler"
              key: scheduler.pulsar.topic.partition.auto.refresh
        - name: SCHED_HISTORY_RETENTION
          valueFrom:
            configMapKeyRef:
              name: "itom-di-udx-scheduler-scheduler"
              key: scheduler.retention
        - name: SCHED_PULSAR_RECEIVER_QUEUE_SIZE
          valueFrom:
            configMapKeyRef:
              name: "itom-di-udx-scheduler-scheduler"
              key: receiverQueue.size
        - name: SCHED_BATCH_DURATION
          valueFrom:
            configMapKeyRef:
              name: "itom-di-udx-scheduler-scheduler"
              key: batch.duration
        - name: SCHED_BATCH_DURATION_UNITS
          valueFrom:
            configMapKeyRef:
              name: "itom-di-udx-scheduler-scheduler"
              key: batch.units
        - name: SCHED_COPY_READ_TIMEOUT
          valueFrom:
            configMapKeyRef:
              name: "itom-di-udx-scheduler-scheduler"
              key: copyRead.timeout
        - name: SCHED_COPY_READ_TIMEOUT_UNITS
          valueFrom:
            configMapKeyRef:
              name: "itom-di-udx-scheduler-scheduler"
              key: copyRead.units
        - name: PULSAR_TENANT
          valueFrom:
            configMapKeyRef:
              name: "itom-di-udx-scheduler-scheduler"
              key: pulsar.tenant
        - name: PULSAR_NAMESPACE
          valueFrom:
            configMapKeyRef:
              name: "itom-di-udx-scheduler-scheduler"
              key: pulsar.namespace
        - name: SCHED_HEARTBEAT_TIMEOUT
          valueFrom:
            configMapKeyRef:
              name: "itom-di-udx-scheduler-scheduler"
              key: heartbeat.timeout
        - name: DI_TENANT
          valueFrom:
            configMapKeyRef:
              name: "itom-di-udx-scheduler-scheduler"
              key: di.tenant
        - name: DI_DEPLOYMENT
          valueFrom:
            configMapKeyRef:
              name: "itom-di-udx-scheduler-scheduler"
              key: di.deployment
        - name: FAILED_EVENT_RETRY_DELAY_MILLIS
          valueFrom:
            configMapKeyRef:
             name: "itom-di-udx-scheduler-scheduler"
             key: scheduler.failed.event.retry.delay.millis
        - name: FAILED_EVENT_INITIAL_DELAY_MILLIS
          valueFrom:
            configMapKeyRef:
             name: "itom-di-udx-scheduler-scheduler"
             key: scheduler.failed.event.initial.delay.millis
        - name: PULSAR_BROKER_SERVICE_PORT
          valueFrom:
            configMapKeyRef:
             name: "itom-di-udx-scheduler-scheduler"
             key: pulsar.broker.service.port
        - name: PULSAR_BROKER_SERVICE_PORT_TLS
          valueFrom:
            configMapKeyRef:
             name: "itom-di-udx-scheduler-scheduler"
             key: pulsar.broker.service.port.tls
        - name: PULSAR_WEB_SERVICE_PORT
          valueFrom:
            configMapKeyRef:
             name: "itom-di-udx-scheduler-scheduler"
             key: pulsar.web.service.port
        - name: PULSAR_WEB_SERVICE_PORT_TLS
          valueFrom:
            configMapKeyRef:
             name: "itom-di-udx-scheduler-scheduler"
             key: pulsar.web.service.port.tls
        - name: PULSAR_TLS_HOSTNAME_VERIFICATION
          valueFrom:
            configMapKeyRef:
             name: "itom-di-udx-scheduler-scheduler"
             key: pulsar.tls.hostname.verification
        - name: PULSAR_SERVICE_NAME
          valueFrom:
            configMapKeyRef:
             name: "itom-di-udx-scheduler-scheduler"
             key: pulsar.service.name
        - name: PULSAR_AUTH_CLASS
          valueFrom:
            configMapKeyRef:
             name: "itom-di-udx-scheduler-scheduler"
             key: pulsar.auth.class
        - name: ENABLE_MESSAGE_ACKNOWLEDGEMENT
          valueFrom:
            configMapKeyRef:
             name: "itom-di-udx-scheduler-scheduler"
             key: pulsar.enable.message.acknowledgement
        - name: SCHEDULER_LOAD_METHOD
          valueFrom:
            configMapKeyRef:
             name: "itom-di-udx-scheduler-scheduler"
             key: scheduler.load.method
        - name: SCHEDULER_ENABLE_UDX_HAS_MESSAGE_CHECK
          valueFrom:
            configMapKeyRef:
             name: "itom-di-udx-scheduler-scheduler"
             key: scheduler.enable.udx.has.message.check
        - name: SCHEDULER_ENABLE_NODE_PARALLELISM
          valueFrom:
            configMapKeyRef:
             name: "itom-di-udx-scheduler-scheduler"
             key: scheduler.enable.node.parallelism
        - name: SCHEDULER_ENABLE_FRAME_BACKLOG_CHECK
          valueFrom:
            configMapKeyRef:
             name: "itom-di-udx-scheduler-scheduler"
             key: scheduler.enable.frame.backlog.check
        - name: SCHEDULER_UDX_ACK_GROUPING_TIME_MILLIS
          valueFrom:
            configMapKeyRef:
             name: "itom-di-udx-scheduler-scheduler"
             key: scheduler.udx.ack.grouping.time.millis
        - name: SCHEDULER_UDX_MAX_MESSAGE_COUNT
          valueFrom:
            configMapKeyRef:
             name: "itom-di-udx-scheduler-scheduler"
             key: scheduler.udx.max.message.count
        - name: SCHEDULER_UDX_MAX_STREAM_SIZE_BYTES
          valueFrom:
            configMapKeyRef:
             name: "itom-di-udx-scheduler-scheduler"
             key: scheduler.udx.max.stream.size.bytes
        - name: SCHEDULER_STAGGERED_SLEEP_DURATION_MILLIS
          valueFrom:
            configMapKeyRef:
             name: "itom-di-udx-scheduler-scheduler"
             key: scheduler.staggered.sleep.duration.millis
        - name: COMPATIBLE_UDX_VERSION
          valueFrom:
            configMapKeyRef:
             name: "itom-di-udx-scheduler-scheduler"
             key: scheduler.rpm.version 
        - name: ENABLE_COMPATIBLE_UDX_VERSION_CHECK
          valueFrom:
            configMapKeyRef:
             name: "itom-di-udx-scheduler-scheduler"
             key: scheduler.enable.rpm.version.check             
        - name: SCHEDULER_SUPPORTED_VERTICA_VERSIONS
          valueFrom:
            configMapKeyRef:
             name: "itom-di-udx-scheduler-scheduler"
             key: scheduler.supported.vertica.versions
        - name: SCHEDULER_ENABLE_MICROBATCH_BACKLOG_CHECK
          valueFrom:
            configMapKeyRef:
             name: "itom-di-udx-scheduler-scheduler"
             key: scheduler.enable.microbatch.backlog.check
        - name: SCHEDULER_ENABLE_SUBSCRIPTION_CLEANER
          valueFrom:
            configMapKeyRef:
             name: "itom-di-udx-scheduler-scheduler"
             key: scheduler.enable.subscription.cleaner
        - name: LOGGING_FROM_CONFIGMAP
          valueFrom:
            configMapKeyRef:
             name: "itom-di-udx-scheduler-scheduler"
             key: scheduler.logconfig.from.configmap
        - name: LOG_TO_FILE
          valueFrom:
            configMapKeyRef:
             name: "itom-di-udx-scheduler-scheduler"
             key: scheduler.logconfig.log.to.file

        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: CONTAINER_NAME
          value: scheduler
      volumes:
        - name: scheduler-conf-vol
          persistentVolumeClaim:
            claimName: config-idm
        - name: scheduler-log-vol
          persistentVolumeClaim:
            claimName: log-idm
        - name: secret-volume
          projected:
            sources:
            - secret:
                name: scheduler-secret
            - configMap:
                name: default-ca-certificates
        - name: vault-token
          emptyDir: {}
        - name: cm-logback
          configMap:
            name: scheduler-logback-cm
---
# Source: nomultimate/charts/itom-ingress-controller/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: itom-ingress-controller
  labels:
    app.kubernetes.io/name: itom-ingress-controller
    helm.sh/chart: itom-ingress-controller
    app.kubernetes.io/instance: RELEASE-NAME
    app.kubernetes.io/version: "0.20.0-0017"
    app.kubernetes.io/managed-by: Helm
  annotations:
    deployment.microfocus.com/default-replica-count: "2"
    deployment.microfocus.com/runlevel: STANDBY
    deployment.microfocus.com/simple-update: ignore
spec:
  replicas: 2
  selector:
    matchLabels:
      app.kubernetes.io/name: itom-ingress-controller
      app.kubernetes.io/instance: RELEASE-NAME
  template:
    metadata:
      labels:
        app.kubernetes.io/name: itom-ingress-controller
        app.kubernetes.io/instance: RELEASE-NAME
      annotations:
        pod.boostport.com/vault-approle: default-default
        pod.boostport.com/vault-init-container: install
    spec:
      serviceAccountName: itom-ingress-controller
      securityContext:
        runAsUser: 1999
        runAsGroup: 1999
        fsGroup: 1999
        supplementalGroups: [1999]
      initContainers:
      - name: waitfor-itom-vault-8200
        image: atf.intranet.bb.com.br:5001/hpeswitom/opensuse-base:15.3-0032
        imagePullPolicy: IfNotPresent
        command: [ "sh", "-c", "until nc -z itom-vault 8200 -w 5 ; do echo waiting for itom-vault:8200...; sleep 5; done; exit 0"]
        resources: {}
      - name: install
        image: "atf.intranet.bb.com.br:5001/hpeswitom/kubernetes-vault-init:0.15.0-0038"
        env:
        - name: CERT_COMMON_NAME
          value: Realm:RE,Common_Name:itom.homologacao.nuvem.hm.bb.com.br,Secret:nginx-default-secret,UpdateSecret:false,File_Name:tls
        volumeMounts:
        - name: vault-token
          mountPath: /var/run/secrets/boostport.com
      containers:
        - name: nginx-ingress-lb
          image: "atf.intranet.bb.com.br:5001/hpeswitom/itom-nginx-ingress:0.20.0-0017"
          imagePullPolicy: IfNotPresent
          livenessProbe:
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 30
            timeoutSeconds: 5
          resources:
            requests:
              cpu: 100m
              memory: 200Mi
            limits:
              cpu: 1500m
              memory: 500Mi
          env:
            - name: ACCESS_LOG_SIZE
              value: "10M"
            - name: ACCESS_LOG_ROTATE
              value: "5"
            - name: NGINX_CONFIG
              value: itom-ingress-controller-conf
            - name: NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: CONTAINER_NAME
              value: nginx-ingress-lb
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: SYSTEM_USER_ID
              value: "1999"
          args:
            - /nginx-ingress-controller
            - --default-backend-service=default/itom-nom-default-http-backend
            - --default-ssl-certificate=default/nginx-default-secret
            - --v=0
            - --configmap=default/itom-ingress-controller-conf
            - --watch-namespace=default
            - --annotations-prefix=ingress.kubernetes.io
            - --enable-ssl-chain-completion=false
            - --http-port=8080
            - --https-port=8443
            - --update-status=false
            - --ingress-class=nginx
          volumeMounts:
            - name: vault-token
              mountPath: /var/run/secrets/boostport.com
            - name: etc-ingress-controller-ssl
              mountPath: /etc/ingress-controller/ssl
        - name: kubernetes-vault-renew
          image: "atf.intranet.bb.com.br:5001/hpeswitom/kubernetes-vault-renew:0.15.0-0038"
          imagePullPolicy: IfNotPresent
          volumeMounts:
            - name: vault-token
              mountPath: /var/run/secrets/boostport.com
          resources:
            limits:
              cpu: 100m
              memory: 200Mi
            requests:
              cpu: 5m
              memory: 5Mi
      volumes:
        - name: vault-token
          emptyDir: {}
        - name: etc-ingress-controller-ssl
          emptyDir: {}
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                    - key: app.kubernetes.io/name
                      operator: In
                      values:
                        - itom-ingress-controller
                topologyKey: kubernetes.io/hostname
              weight: 100
---
# Source: nomultimate/charts/itom-reloader/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: itom-reloader
  annotations:
    deployment.microfocus.com/default-replica-count: "1"
    deployment.microfocus.com/runlevel: UP
  labels:
    app.kubernetes.io/name: itom-reloader
    helm.sh/chart: itom-reloader
    app.kubernetes.io/instance: RELEASE-NAME
    app.kubernetes.io/version: "1.2.32"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  revisionHistoryLimit: 2
  selector:
    matchLabels:
      app.kubernetes.io/name: itom-reloader
      app.kubernetes.io/instance: RELEASE-NAME
  template:
    metadata:
      labels:
        app.kubernetes.io/name: itom-reloader
        app.kubernetes.io/instance: RELEASE-NAME
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                    - key: app.kubernetes.io/name
                      operator: In
                      values:
                        - itom-reloader
                topologyKey: kubernetes.io/hostname
              weight: 100
      containers:
        - image: "atf.intranet.bb.com.br:5001/hpeswitom/stakater-reloader:v0.0.99"
          imagePullPolicy: IfNotPresent
          name: itom-reloader
          env:
            - name: KUBERNETES_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          ports:
            - name: http
              containerPort: 9090
          livenessProbe:
            httpGet:
              path: /metrics
              port: http
          readinessProbe:
            httpGet:
              path: /metrics
              port: http
      securityContext:
        runAsUser: 1999
        runAsGroup: 1999
        fsGroup: 1999
        supplementalGroups: [1999]
      serviceAccountName: itom-cdf-reloader
---
# Source: nomultimate/charts/itom-vault/templates/vault-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: itom-vault
  namespace: default
  labels:
    app: itom-vault-app
    deployments.microfocus.com/component: itom-vault
  annotations:
    deployment.microfocus.com/default-replica-count: "1"
    deployment.microfocus.com/runlevel: DB
    deployment.microfocus.com/simple-update: ignore
spec:
  replicas: 1
  selector:
    matchLabels:
      app: itom-vault-app
  template:
    metadata:
      labels:
        app: itom-vault-app
        deployments.microfocus.com/component: itom-vault
    spec:
      serviceAccountName: itom-vault
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - itom-vault-app
              topologyKey: "kubernetes.io/hostname"
      hostname: itom-vault
      
      containers:
      - name: vault
        image: atf.intranet.bb.com.br:5001/hpeswitom/vault:0.19.0-0074
        imagePullPolicy: IfNotPresent
        livenessProbe:
          exec:
            command: ["/probe.sh", "liveness"]
          failureThreshold: 8
          initialDelaySeconds: 15
          periodSeconds: 15
          timeoutSeconds: 15
        readinessProbe:
          exec:
            command: ["/probe.sh", "readiness"]
          failureThreshold: 8
          initialDelaySeconds: 15
          periodSeconds: 20
          timeoutSeconds: 15
        env:
        - name: K8S_INSTALL_MODE
          value: "CLASSIC"
        - name: RELEASE_NAME
          value: RELEASE-NAME
        - name: VAULT_BACKEND
          value: "file"
        - name: SYSTEM_USER_ID
          value: "1999"
        - name: SYSTEM_GROUD_ID
          value: "1999"
        - name: ETCD_ADDR
          valueFrom:
            configMapKeyRef:
              name: cdf-cluster-host
              key: ETCD_ENDPOINT
              optional: true
        - name: ETCD_DATA_DIR
          value: ""
        - name: ETCD_TLS_CA_FILE
          value: ""
        - name: ETCD_TLS_CERT_FILE
          value: ""
        - name: ETCD_TLS_KEY_FILE
          value: ""
        - name: VAULT_TLS_CA_FILE
          value: "/ssl/ca.crt"
        - name: VAULT_REDIRECT_ADDR
          value: "https://itom-vault.default:8200"
        - name: VAULT_SERVER_IP
          value: "itom-vault.default"
        - name: ETCD_HA_ENABLED
          value: "1"
        - name: DEPLOYMENT_TYPE
          value: "CHART"
        - name: REALM_LIST
          value: "RID:365,RE:365"
        - name: SERVICE_PREFIX
          value: "itom"
        - name: DISABLE_MLOCK
          value: "true"
        - name: "ENABLE_KUBERNETES_AUTHENTICATION"
          value: "false"
        - name: INIT_SECRETS
          value: "nom-secret"
        resources:
          limits:
            cpu: 2
            memory: 2048Mi
          requests:
            cpu: 100m
            memory: 100Mi
        volumeMounts:
        - mountPath: /coreVolumeRoot
          name: core-volume
        - mountPath: /ssl
          name: vault-certs
      securityContext:
        runAsUser: 1999
        fsGroup: 1999
        runAsGroup: 1999
        supplementalGroups: [1999]
      volumes:
      - name: core-volume
        persistentVolumeClaim:
          claimName: data-idm
      - name: vault-certs
        emptyDir: {}
---
# Source: nomultimate/charts/itomdiadministration/templates/di-administration-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: itom-di-administration
  labels:
    app.kubernetes.io/name: itom-di-administration
    app.kubernetes.io/managed-by: RELEASE-NAME
    app.kubernetes.io/version: 2.5.0-46
    itom.microfocus.com/capability: itom-data-ingestion
    tier.itom.microfocus.com/backend: backend
    app: itom-di-administration
  annotations:
    deployment.microfocus.com/default-replica-count: "1"
    deployment.microfocus.com/runlevel: UP
    configmap.reloader.stakater.com/reload: "api-client-ca-certificates,default-ca-certificates"
spec:
  replicas: 1
  selector:
    matchLabels:
      app: itom-di-administration
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
  template:
    metadata:
      labels:
        app.kubernetes.io/name: itom-di-administration
        app.kubernetes.io/managed-by: RELEASE-NAME
        app.kubernetes.io/version: 2.5.0-46
        itom.microfocus.com/capability: itom-data-ingestion
        tier.itom.microfocus.com/backend: backend
        app: itom-di-administration
      annotations:
        checksum/config: f6d657a028738981d178bb4261ff83a65a876ff532244a73b4b3fbe2ab77917f
        pod.boostport.com/vault-approle: default-default
        pod.boostport.com/vault-init-container: install
    spec:
      securityContext:
        runAsUser: 1999
        runAsGroup: 1999
        fsGroup: 1999     
      serviceAccount: itom-di-administration-sa
      serviceAccountName: itom-di-administration-sa           
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                    - key: "app"
                      operator: In
                      values:
                        - itom-di-administration
                topologyKey: "kubernetes.io/hostname"
      terminationGracePeriodSeconds: 30
      initContainers:
      - name: waitfor-itom-vault-8200
        image: atf.intranet.bb.com.br:5001/hpeswitom/opensuse-base:15.3-0032
        imagePullPolicy: IfNotPresent
        command: [ "sh", "-c", "until nc -z itom-vault 8200 -w 5 ; do echo waiting for itom-vault:8200...; sleep 5; done; exit 0"]
        resources: {}
        securityContext:
          runAsNonRoot: true
          runAsUser: 1999
          runAsGroup: 1999
      - name: waitfor-vertica-service
        image: atf.intranet.bb.com.br:5001/hpeswitom/itom-busybox:1.38.0-009
        imagePullPolicy: IfNotPresent
        command: [ "sh", "-c", "cmd=''; vh_local=pxl0nnmi0022.disposistivos.bb.com.br; vp_local=5443; for i in $(echo $vh_local | sed 's/,/ /g' | awk '{$1=$1};1'); do cmd=$(echo $cmd '|| nc -z '$i' '$vp_local' -w 5 '); done; cmd=${cmd:3}; echo 'command is : '$cmd''; until eval $(echo $cmd); do echo 'waiting for '$vh_local' with port '$vp_local' ... '; sleep 5; done; exit 0;"]
        resources: {}
        securityContext:
          runAsNonRoot: true
          runAsUser: 1999
          runAsGroup: 1999
      - env:
        - name: CERT_COMMON_NAME
        #Nishant: Need to verify whetrher the CERT_COMMON_NAME is correct or we need external access host
          value: "Realm:RE,Common_Name:itom-di-administration.homologacao.nuvem.hm.bb.com.br,Additional_SAN:itom-di-administration-svc/itom-di-administration-svc.default/itom-di-administration-svc.default.svc,File_Name:server;Realm:RID,Common_Name:itom-di-message-bus-svc,Additional_SAN:itom-di-administration-svc/itom-di-administration-svc.default/itom-di-administration-svc.default.svc,File_Name:message_bus_server"
        image: atf.intranet.bb.com.br:5001/hpeswitom/kubernetes-vault-init:0.15.0-0038
        imagePullPolicy: IfNotPresent
        name: install
        resources: {}
        securityContext:
          runAsUser: 1999
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
          - mountPath: /var/run/secrets/boostport.com
            name: vault-token
      - name: itom-di-init-admin-cnt
        image: atf.intranet.bb.com.br:5001/hpeswitom/itom-data-ingestion-administration:2.5.0-46
        command: ["/administration/bin/run.sh","dependency"]
        env:
        - name: MESSAGE_BUS
          valueFrom:
            configMapKeyRef:
              name: itom-di-administration-cm
              key: admin.message.bus
        - name: PULSAR_SERVICE_NAME
          valueFrom:
            configMapKeyRef:
              name: itom-di-administration-cm
              key: pulsar.service.name
        - name: PULSAR_TLS_ENABLE
          valueFrom:
            configMapKeyRef:
              name: itom-di-administration-cm
              key: pulsar.tls.enable
        - name: PULSAR_WEB_SERVICE_PORT
          valueFrom:
            configMapKeyRef:
              name: itom-di-administration-cm
              key: pulsar.web.service.port
        - name: PULSAR_WEB_SERVICE_PORT_TLS
          valueFrom:
            configMapKeyRef:
              name: itom-di-administration-cm
              key: pulsar.web.service.port.tls
        - name: PULSAR_BROKER_SERVICE_PORT
          valueFrom:
            configMapKeyRef:
              name: itom-di-administration-cm
              key: pulsar.broker.service.port
        - name: PULSAR_BROKER_SERVICE_PORT_TLS
          valueFrom:
            configMapKeyRef:
              name: itom-di-administration-cm
              key: pulsar.broker.service.port.tls
        - name: EXPRESS_LOAD_ENABLED
          valueFrom:
            configMapKeyRef:
              name: itom-di-administration-cm
              key: admin.express.load.enable
        - name: MINIO_HOST
          valueFrom:
            configMapKeyRef:
              name: itom-di-administration-cm
              key: admin.minio.host
        - name: MINIO_PORT
          valueFrom:
            configMapKeyRef:
              name: itom-di-administration-cm
              key: admin.minio.port
      containers:
        - name: itom-di-administration
          image: atf.intranet.bb.com.br:5001/hpeswitom/itom-data-ingestion-administration:2.5.0-46
          livenessProbe:
            exec:
              command:
                - "/administration/bin/liveness.sh"
            initialDelaySeconds: 360
            periodSeconds: 20
            timeoutSeconds: 5
            failureThreshold: 3
            successThreshold: 1
          readinessProbe:
            exec:
              command:
                - "/administration/bin/readiness.sh"
            initialDelaySeconds: 30
            periodSeconds: 20
            timeoutSeconds: 5
            failureThreshold: 18
            successThreshold: 1
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 8443
          env:
            - name: gid
              value: "1999"
            - name: uid
              value: "1999"
            # SSl configurations
            - name: JAVA_OPTS
              valueFrom:
                configMapKeyRef:
                  name: itom-di-administration-cm
                  key: admin.jvm.args
            - name: ADMIN_CLIENT_AUTH_ENABLED
              valueFrom:
                configMapKeyRef:
                  name: itom-di-administration-cm
                  key: admin.client.auth.enabled
            - name: PULSAR_SERVICE_NAME
              valueFrom:
                configMapKeyRef:
                  name: itom-di-administration-cm
                  key: pulsar.service.name
            - name: PULSAR_TLS_ENABLE
              valueFrom:
                configMapKeyRef:
                  name: itom-di-administration-cm
                  key: pulsar.tls.enable
            - name: PULSAR_TENANT
              valueFrom:
                configMapKeyRef:
                  name: itom-di-administration-cm
                  key: pulsar.tenant
            - name: PULSAR_NAMESPACE
              valueFrom:
                configMapKeyRef:
                  name: itom-di-administration-cm
                  key: pulsar.namespace
            - name: PULSAR_WEB_SERVICE_PORT
              valueFrom:
                configMapKeyRef:
                  name: itom-di-administration-cm
                  key: pulsar.web.service.port
            - name: PULSAR_WEB_SERVICE_PORT_TLS
              valueFrom:
                configMapKeyRef:
                  name: itom-di-administration-cm
                  key: pulsar.web.service.port.tls
            - name: PULSAR_AUTH_ENABLE
              valueFrom:
                configMapKeyRef:
                  name: itom-di-administration-cm
                  key: pulsar.auth.enable
            - name: PULSAR_AUTH_CLASS
              valueFrom:
                configMapKeyRef:
                  name: itom-di-administration-cm
                  key: pulsar.auth.class
            - name: PULSAR_TLS_HOSTNAME_VERIFICATION
              valueFrom:
                configMapKeyRef:
                  name: itom-di-administration-cm
                  key: pulsar.tls.hostname.verification
            - name: MESSAGE_BUS
              valueFrom:
                configMapKeyRef:
                  name: itom-di-administration-cm
                  key: admin.message.bus
            - name: ADMIN_SERVICE_BASE_URL
              valueFrom:
                configMapKeyRef:
                  name: itom-di-administration-cm
                  key: admin.service.base.url
            - name: ADMIN_SYSTEM_LEVEL_THROTTLE
              valueFrom:
                configMapKeyRef:
                  name: itom-di-administration-cm
                  key: admin.system.level.throttle
            - name: ADMIN_PERMISSIBLE_REQUEST_LIMIT
              valueFrom:
                configMapKeyRef:
                  name: itom-di-administration-cm
                  key: admin.permissible.request.limit
            - name: ADMIN_REQUEST_THROTTLE_TIME
              valueFrom:
                configMapKeyRef:
                  name: itom-di-administration-cm
                  key: admin.request.throttle.time
            - name: DEFAULT_RESOURCE_POOL_NAME
              valueFrom:
                configMapKeyRef:
                  name: itom-di-administration-cm
                  key: vertica.resource.pool.name
            - name: DI_TENANT
              valueFrom:
                configMapKeyRef:
                  name: itom-di-administration-cm
                  key: di.tenant
            - name: DI_DEPLOYMENT
              valueFrom:
                configMapKeyRef:
                  name: itom-di-administration-cm
                  key: di.deployment
            - name: ADMIN_TOPIC_PARTITION_COUNT
              valueFrom:
                configMapKeyRef:
                  name: itom-di-administration-cm
                  key: admin.topic.partition.count
            - name: ADMIN_CONFIG_STORE_TYPE
              valueFrom:
                configMapKeyRef:
                  name: itom-di-administration-cm
                  key: admin.config.store.type
            - name: DB_SSL_ENABLE
              valueFrom:
                configMapKeyRef:
                  name: itom-di-administration-cm
                  key: admin.db.ssl.enable
            - name: ADMIN_CONFIGSTORE_HOSTNAME
              valueFrom:
                configMapKeyRef:
                  name: itom-di-administration-cm
                  key: admin.config.store.hostnames
            - name: ADMIN_CONFIGSTORE_USERNAME
              valueFrom:
                configMapKeyRef:
                  name: itom-di-administration-cm
                  key: admin.config.store.username
            - name: ADMIN_CONFIGSTORE_TOKEN_KEY
              valueFrom:
                configMapKeyRef:
                  name: itom-di-administration-cm
                  key: admin.config.store.token.key
            - name: ADMIN_CONFIGSTORE_DB
              valueFrom:
                configMapKeyRef:
                  name: itom-di-administration-cm
                  key: admin.config.store.db
            - name: ADMIN_CONFIGSTORE_PORT
              valueFrom:
                configMapKeyRef:
                  name: itom-di-administration-cm
                  key: admin.config.store.port
            - name: HIKARICP_CONNECTION_TIMEOUT
              valueFrom:
                configMapKeyRef:
                  name: itom-di-administration-cm
                  key: hikaricp.connection.timeout
            - name: HIKARICP_CONNECTION_MAXIMUM_LIFETIME
              valueFrom:
                configMapKeyRef:
                  name: itom-di-administration-cm
                  key: hikaricp.connection.maximum.lifetime
            - name: HIKARICP_MINIMUM_IDLE_CONNECTIONS
              valueFrom:
                configMapKeyRef:
                  name: itom-di-administration-cm
                  key: hikaricp.minimum.idle.connections
            - name: HIKARICP_MAXIMUM_POOL_SIZE
              valueFrom:
                configMapKeyRef:
                  name: itom-di-administration-cm
                  key: hikaricp.maximum.pool.size
            - name: HIKARICP_CONNECTION_POOL_NAME
              valueFrom:
                configMapKeyRef:
                  name: itom-di-administration-cm
                  key: hikaricp.connection.pool.name
            - name: DATASET_STATUS_POLLING_TIMEOUT_IN_SECONDS
              valueFrom:
                configMapKeyRef:
                  name: itom-di-administration-cm
                  key: dataset.status.polling.timeout.in.seconds
            - name: EXPRESS_LOAD_ENABLED
              valueFrom:
                configMapKeyRef:
                  name: itom-di-administration-cm
                  key: admin.express.load.enable
            - name: EXTERNAL_CA_SIGNED_CERTS_ENABLE
              valueFrom:
                configMapKeyRef:
                  name: itom-di-administration-cm
                  key: admin.use.external.ca.signed.certs
            - name: MINIO_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: itom-di-minio-secret
                  key: accesskey
                  optional: true
            - name: MINIO_SECRET_KEY
              valueFrom:
                secretKeyRef:
                  name: itom-di-minio-secret
                  key: secretkey
                  optional: true

            - name: MINIO_HOST
              valueFrom:
                configMapKeyRef:
                  name: itom-di-administration-cm
                  key: admin.minio.host
            - name: MINIO_EXTERNAL_HOST
              valueFrom:
                configMapKeyRef:
                  name: itom-di-administration-cm
                  key: admin.minio.externalAccessHost
            - name: MINIO_PORT
              valueFrom:
                configMapKeyRef:
                  name: itom-di-administration-cm
                  key: admin.minio.port
            - name: MINIO_SSL_ENABLE
              valueFrom:
                configMapKeyRef:
                  name: itom-di-administration-cm
                  key: admin.minio.ssl.enable
            - name: MINIO_WORKER_COUNT
              valueFrom:
                configMapKeyRef:
                  name: itom-di-administration-cm
                  key: admin.expressLoad.workerCount
            - name: MINIO_POLLING_INTERVAL_IN_SEC
              valueFrom:
                configMapKeyRef:
                  name: itom-di-administration-cm
                  key: admin.expressLoad.pollingIntervalInSec
            - name: MINIO_CSV_OBJECT_LIMIT_FOR_DIRECT_IN_MB
              valueFrom:
                configMapKeyRef:
                  name: itom-di-administration-cm
                  key: admin.expressLoad.csvObjectLimitForDirectInMB
            - name: MINIO_GZIP_OBJECT_LIMIT_FOR_DIRECT_IN_MB
              valueFrom:
                configMapKeyRef:
                  name: itom-di-administration-cm
                  key: admin.expressLoad.gzipObjectLimitForDirectInMB
            - name: MINIO_VERTICA_CONNECTION_POOL_SIZE
              valueFrom:
                configMapKeyRef:
                  name: itom-di-administration-cm
                  key: admin.expressLoad.vertica.connectionPoolSize
            - name: MINIO_VERTICA_CONNECTION_LOAD_BALANCING
              valueFrom:
                configMapKeyRef:
                  name: itom-di-administration-cm
                  key: admin.expressLoad.vertica.connectionLoadbalancing
            - name: MINIO_NODE_PORT
              valueFrom:
                configMapKeyRef:
                  name: itom-di-administration-cm
                  key: admin.minio.nodePort
            - name: EXPRESS_LOAD_OBJECT_SIZE_LIMIT_IN_MB
              valueFrom:
                configMapKeyRef:
                  name: itom-di-administration-cm
                  key: admin.expressLoad.objectSizeLimitInMB
            - name: EXPRESS_LOAD_PARAMETER_ENFORCE_LENGTH
              valueFrom:
                configMapKeyRef:
                  name: itom-di-administration-cm
                  key: admin.expressLoad.parameters.enforceLength
            - name: EXPRESS_LOAD_PARAMETER_ABORT_ON_ERROR
              valueFrom:
                configMapKeyRef:
                  name: itom-di-administration-cm
                  key: admin.expressLoad.parameters.abortOnError
            - name: EXPRESS_LOAD_PARAMETER_REJECT_ON_EMPTY_KEY
              valueFrom:
                configMapKeyRef:
                  name: itom-di-administration-cm
                  key: admin.expressLoad.parameters.rejectOnEmptyKey
            - name: EXPRESS_LOAD_PARAMETER_REJECT_ON_DATA_MISMATCH
              valueFrom:
                configMapKeyRef:
                  name: itom-di-administration-cm
                  key: admin.expressLoad.parameters.rejectOnDataMismatch
            - name: EXPRESS_LOAD_PARAMETER_MAX_REJECTIONS
              valueFrom:
                configMapKeyRef:
                  name: itom-di-administration-cm
                  key: admin.expressLoad.parameters.maxRejections
            - name: EXPRESS_LOAD_PARAMETER_HEADER
              valueFrom:
                configMapKeyRef:
                  name: itom-di-administration-cm
                  key: admin.expressLoad.parameters.header
            - name: EXPRESS_LOAD_VERTICA_RESOURCE_POOL
              valueFrom:
                configMapKeyRef:
                  name: itom-di-administration-cm
                  key: admin.expressLoad.verticaResourcePool
            - name: NODE_NAME
              valueFrom:
               fieldRef:
                fieldPath: spec.nodeName
            - name: POD_NAME
              valueFrom:
               fieldRef:
                fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
               fieldRef:
                fieldPath: metadata.namespace
            - name: CONTAINER_NAME
              value: administration
            - name: ADMIN_CLUSTER_K8SPROVIDER
              value: "cdf"
            - name: ADMIN_AUDIT_LOGS_ENABLED
              valueFrom:
                configMapKeyRef:
                  name: itom-di-administration-cm
                  key: admin.auditLogs.enabled
            - name: ADMIN_AUDIT_LOGS_DEFAULT_PATH
              value: "true"
            - name: LOGGING_FROM_CONFIGMAP
              valueFrom:
                configMapKeyRef:
                  name: itom-di-administration-cm
                  key: admin.logconfig.from.configmap
            - name: LOG_TO_FILE
              valueFrom:
                configMapKeyRef:
                  name: itom-di-administration-cm
                  key: admin.logconfig.log.to.file

          resources:
            limits:
              cpu: "1"
              memory: 4096Mi
            requests:
              cpu: "0.2"
              memory: 256Mi
          volumeMounts:
            - name: vault-token
              mountPath: /var/run/secrets/boostport.com
            - name: administration-log-vol
              mountPath: /administration/log
            - name: administration-vol
              mountPath: /mnt/itom/administration/conf
              subPath: di/administration/conf
            - name: administration-vol
              mountPath: /mnt/itom/administration/dlconf
              subPath: di/vertica-ingestion/conf
            - name: administration-vol
              mountPath: /mnt/itom/administration/dp-conf
              subPath: di/data-processor/conf
            - name: dp-data-vol
              mountPath: /mnt/itom/administration/dp-data
              subPath: di/data-processor/data
            - name: administration-vol
              mountPath: /mnt/itom/administration/dp-task-artifacts
              subPath: di/data-processor/bin/task-artifacts
            - name: admin-client-certs-volume
              mountPath: /administration/ssl/ca
            - name: config-store-client-certs-volume
              mountPath: /administration/ssl/config-store-ca
            - name: cm-client-certs-volume
              mountPath: /administration/ssl/api-client-cas/
            - name: cm-server-certs-volume
              mountPath: /administration/ssl/server-cas/
            - name: servercert-secret-volume
              mountPath: /administration/ssl/external-ca-signed-cert
            - name: cm-logback
              mountPath: /administration/conf-local/
        - name: kubernetes-vault-renew
          image: atf.intranet.bb.com.br:5001/hpeswitom/kubernetes-vault-renew:0.15.0-0038
          imagePullPolicy: IfNotPresent
          volumeMounts:
          - name: vault-token
            mountPath: /var/run/secrets/boostport.com
      volumes:
      - name: administration-vol
        persistentVolumeClaim:
          claimName: config-idm
      - name: dp-data-vol
        persistentVolumeClaim:
          claimName: data-idm
      - name: administration-log-vol
        persistentVolumeClaim:
          claimName: log-idm
      - name: vault-token
        emptyDir: {}
      - name: admin-client-certs-volume
        secret:
          secretName: administration-secret
      - name: config-store-client-certs-volume
        secret:
          secretName: configuration-secret
      - name: cm-client-certs-volume
        projected:
         sources:
          - configMap:
              name: api-client-ca-certificates
      - name: servercert-secret-volume
        projected:
          sources:
          - secret:
              name: "bbcert"
      - name: cm-server-certs-volume
        configMap:
          name: default-ca-certificates
      - name: cm-logback
        configMap:
          name: administration-logback-cm
---
# Source: nomultimate/charts/itomdidataaccess/templates/data-access.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/name: itom-di-data-access
    app.kubernetes.io/managed-by: RELEASE-NAME
    app.kubernetes.io/version: 2.5.0-34
    itom.microfocus.com/capability: itom-data-ingestion
    tier.itom.microfocus.com/backend: backend
  name: itom-di-data-access-dpl
  annotations:
    deployment.microfocus.com/default-replica-count: "1"
    deployment.microfocus.com/runlevel: UP
    configmap.reloader.stakater.com/reload: "api-client-ca-certificates,default-ca-certificates"
spec:
  replicas: 1
  selector:
    matchLabels:
      app: itom-di-data-access
  template:
    metadata:
      labels:
        app.kubernetes.io/name: itom-di-data-access
        app.kubernetes.io/managed-by: RELEASE-NAME
        app.kubernetes.io/version: 2.5.0-34
        itom.microfocus.com/capability: itom-data-ingestion
        tier.itom.microfocus.com/backend: backend
        app: itom-di-data-access
      annotations:
        pod.boostport.com/vault-approle: default-default
        pod.boostport.com/vault-init-container: install
        checksum/config: 848bdc49c9aaa0a913422e3094486d8575149c74f904faf0ef66b612fb567863
    spec:
      securityContext:
        runAsUser: 1999
        runAsGroup: 1999
        fsGroup: 1999
      
      serviceAccount: itom-di-data-access-sa
      serviceAccountName: itom-di-data-access-sa
      
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - itom-di-data-access
              topologyKey: kubernetes.io/hostname
            weight: 100
      terminationGracePeriodSeconds: 30
      containers:
        - name: itom-di-data-access-cnt
          image: atf.intranet.bb.com.br:5001/hpeswitom/itom-data-ingestion-data-access:2.5.0-34
          livenessProbe:
            exec:
              command:
              - "/dataaccess/bin/liveness.sh"
            initialDelaySeconds: 360
            periodSeconds: 20
            timeoutSeconds: 5
            failureThreshold: 3
            successThreshold: 1
          readinessProbe:
            exec:
              command:
              - "/dataaccess/bin/readiness.sh"
            initialDelaySeconds: 30
            periodSeconds: 20
            timeoutSeconds: 5
            failureThreshold: 18
            successThreshold: 1
          imagePullPolicy: IfNotPresent
          env:
          - name: gid
            value: "1999"
          - name: uid
            value: "1999"
          - name: DATAACCESS_JVM_ARGS
            valueFrom:
                configMapKeyRef:
                  name: itom-di-data-access-cm
                  key: dataaccess.jvm.args
          - name: DATAACCESS_VERTICA_INGESTION_SERVICE_NAME
            valueFrom:
                configMapKeyRef:
                  name: itom-di-data-access-cm
                  key: vertica.ingestion.service.name
          - name: DATAACCESS_VERTICA_INGESTION_SERVICE_PORT
            valueFrom:
                configMapKeyRef:
                  name: itom-di-data-access-cm
                  key: vertica.ingestion.service.port
          - name: DATAACCESS_VERTICA_DRIVER_CLASS_NAME
            valueFrom:
                configMapKeyRef:
                  name: itom-di-data-access-cm
                  key: vertica.datasource.driver-class-name
          - name: DATAACCESS_VERTICA_USERNAME
            valueFrom:
                configMapKeyRef:
                  name: itom-di-data-access-cm
                  key: vertica.datasource.username
          - name: DATAACCESS_VERTICA_CONNECTION_TIMEOUT
            valueFrom:
                configMapKeyRef:
                  name: itom-di-data-access-cm
                  key: vertica.datasource.connection-timeout
          - name: DATAACCESS_RESOURCE_POOL
            valueFrom:
                configMapKeyRef:
                  name: itom-di-data-access-cm
                  key: dataaccess.resource-pool
          - name: DATAACCESS_VERTICA_CONNECTION_MAX_RECONNECT_ATTEMPT_ON_FAILURE_DURING_STARTUP
            valueFrom:
                configMapKeyRef:
                  name: itom-di-data-access-cm
                  key: vertica.connection.max-reconnect-attempt-on-failure-during-startup
          - name: DATAACCESS_VERTICA_CONNECTION_TIME_DELAY_BETWEEN_RETRIES_DURING_STARTUP
            valueFrom:
                configMapKeyRef:
                  name: itom-di-data-access-cm
                  key: vertica.connection.time-delay-between-retries-during-startup
          - name: DATAACCESS_VERTICA_HOSTNAME
            valueFrom:
                configMapKeyRef:
                  name: itom-di-data-access-cm
                  key: vertica.datasource.hostname
          - name: DATAACCESS_VERTICA_PORT
            valueFrom:
                configMapKeyRef:
                  name: itom-di-data-access-cm
                  key: vertica.datasource.port
          - name: DATAACCESS_VERTICA_DATABASENAME
            valueFrom:
                configMapKeyRef:
                  name: itom-di-data-access-cm
                  key: vertica.datasource.databasename
          - name: DATAACCESS_VERTICA_DATABASE_PASSWORD_KEY
            valueFrom:
                configMapKeyRef:
                  name: itom-di-data-access-cm
                  key: vertica.datasource.password.key
          - name: DATAACCESS_CLIENT_AUTH_ENABLE
            valueFrom:
                configMapKeyRef:
                  name: itom-di-data-access-cm
                  key: dataaccess.client.auth.enable
          - name: EXTERNAL_CA_SIGNED_CERTS_ENABLE
            valueFrom:
                configMapKeyRef:
                  name: itom-di-data-access-cm
                  key: dataaccess.use.external.ca.signed.certs
          - name: VERTICA_SSL_ENABLE
            valueFrom:
                configMapKeyRef:
                  name: itom-di-data-access-cm
                  key: dataaccess.vertica.ssl.enable
          - name: DATAACCESS_VERTICA_INGESTION_TLS_ENABLE
            valueFrom:
                configMapKeyRef:
                  name: itom-di-data-access-cm
                  key: dataaccess.vertica.ingestion.tls.enable
          - name: HIKARICP_CONNECTION_MAXIMUM_LIFETIME
            valueFrom:
                configMapKeyRef:
                  name: itom-di-data-access-cm
                  key: hikaricp.connection.maximum.lifetime
          - name: HIKARICP_MINIMUM_IDLE_CONNECTIONS
            valueFrom:
                configMapKeyRef:
                  name: itom-di-data-access-cm
                  key: hikaricp.minimum.idle.connections
          - name: HIKARICP_IDLE_SESSION_TIMEOUT
            valueFrom:
                configMapKeyRef:
                  name: itom-di-data-access-cm
                  key: hikaricp.idle.session.timeout
          - name: HIKARICP_MAXIMUM_POOL_SIZE
            valueFrom:
                configMapKeyRef:
                  name: itom-di-data-access-cm
                  key: hikaricp.maximum.pool.size
          - name: HIKARICP_CONNECTION_POOL_NAME
            valueFrom:
                configMapKeyRef:
                  name: itom-di-data-access-cm
                  key: hikaricp.connection.pool.name
          - name: TIMEZONE
            valueFrom:
                configMapKeyRef:
                  name: itom-di-data-access-cm
                  key: timezone
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName
          - name: POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: CONTAINER_NAME
            value: data-access
          - name: LOGCONFIG_FROM_CONFIGMAP
            valueFrom:
              configMapKeyRef:
                name: itom-di-data-access-cm
                key: dataaccess.logconfig.from.configmap
          resources:
            limits:
              cpu: "1"
              memory: 2048Mi
            requests:
              cpu: "0.2"
              memory: 512Mi
          ports:
          - containerPort: 8443
          volumeMounts:
          - name: di-data-access-log-volume
            mountPath: /dataaccess/log
          - name: vault-token
            mountPath: /var/run/secrets/boostport.com
          - name: cm-client-certs-volume
            mountPath: /dataaccess/ssl/api-client-cas
          - name: cm-server-certs-volume
            mountPath: /dataaccess/ssl/server-cas
          - name: servercert-secret-volume
            mountPath: /dataaccess/ssl/external-ca-signed-cert
          - name: cm-logback
            mountPath: /dataaccess/app-conf/
        - name: kubernetes-vault-renew
          image: atf.intranet.bb.com.br:5001/hpeswitom/kubernetes-vault-renew:0.15.0-0038
          imagePullPolicy: IfNotPresent
          volumeMounts:
          - name: vault-token
            mountPath: /var/run/secrets/boostport.com
      initContainers:
      - name: waitfor-itom-vault-8200
        image: atf.intranet.bb.com.br:5001/hpeswitom/opensuse-base:15.3-0032
        imagePullPolicy: IfNotPresent
        command: [ "sh", "-c", "until nc -z itom-vault 8200 -w 5 ; do echo waiting for itom-vault:8200...; sleep 5; done; exit 0"]
        resources: {}
        securityContext:
          runAsNonRoot: true
          runAsUser: 1999
          runAsGroup: 1999
      - name: waitfor-vertica-service
        image: atf.intranet.bb.com.br:5001/hpeswitom/itom-busybox:1.38.0-009
        imagePullPolicy: IfNotPresent
        command: [ "sh", "-c", "cmd=''; vh_local=pxl0nnmi0022.disposistivos.bb.com.br; vp_local=5443; for i in $(echo $vh_local | sed 's/,/ /g' | awk '{$1=$1};1'); do cmd=$(echo $cmd '|| nc -z '$i' '$vp_local' -w 5 '); done; cmd=${cmd:3}; echo 'command is : '$cmd''; until eval $(echo $cmd); do echo 'waiting for '$vh_local' with port '$vp_local' ... '; sleep 5; done; exit 0;"]
        resources: {}
        securityContext:
          runAsNonRoot: true
          runAsUser: 1999
          runAsGroup: 1999
      - env:
        - name: CERT_COMMON_NAME
          value: "Realm:RE,Common_Name:itom-di-data-access-svc.default.svc.cluster.local,Additional_SAN:/itom-di-data-access-svc.default/itom-di-data-access-svc.default.svc;Realm:RID,Common_Name:itom-di-data-access-svc.default.svc.cluster.local,Additional_SAN:itom-di-data-access-svc/itom-di-data-access-svc.default/itom-di-data-access-svc.default.svc,File_Name:RID_client"
        image: atf.intranet.bb.com.br:5001/hpeswitom/kubernetes-vault-init:0.15.0-0038
        imagePullPolicy: IfNotPresent
        name: install
        resources: {}
        securityContext:
          runAsUser: 1999
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /var/run/secrets/boostport.com
          name: vault-token
      volumes:
      - name: di-data-access-log-volume
        persistentVolumeClaim:
          claimName: log-idm
      - name: vault-token
        emptyDir: {}
      - name: cm-client-certs-volume
        projected:
          sources:
          - configMap:
              name: api-client-ca-certificates
      - name: servercert-secret-volume
        projected:
          sources:
          - secret:
              name: "bbcert"
      - name: cm-server-certs-volume
        projected:
          sources:
          - secret:
              name: data-access-secret
          - configMap:
              name: default-ca-certificates
      - name: cm-logback
        configMap:
          name: data-access-logback-cm
---
# Source: nomultimate/charts/itomdimetadataserver/templates/metadata-server-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: itom-di-metadata-server
  labels:
    app.kubernetes.io/name: itom-di-metadata-server
    app.kubernetes.io/managed-by: RELEASE-NAME
    app.kubernetes.io/version: 2.5.0-25
    itom.microfocus.com/capability: itom-data-ingestion
    tier.itom.microfocus.com/backend: backend
    app: itom-di-metadata-server
  annotations:
    deployment.microfocus.com/default-replica-count: "1"
    deployment.microfocus.com/runlevel: UP
    configmap.reloader.stakater.com/reload: "default-ca-certificates"
spec:
  replicas: 1
  selector:
    matchLabels:
      app: itom-di-metadata-server
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
  template:
    metadata:
      labels:
        app.kubernetes.io/name: itom-di-metadata-server
        app.kubernetes.io/managed-by: RELEASE-NAME
        app.kubernetes.io/version: 2.5.0-25
        itom.microfocus.com/capability: itom-data-ingestion
        tier.itom.microfocus.com/backend: backend
        app: itom-di-metadata-server
      annotations:
        pod.boostport.com/vault-approle: default-default
        pod.boostport.com/vault-init-container: install
    spec:
      securityContext:
        runAsUser: 1999
        runAsGroup: 1999
        fsGroup: 1999      
      serviceAccount: itom-di-metadata-server-sa
      serviceAccountName: itom-di-metadata-server-sa
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - itom-di-metadata-server
              topologyKey: kubernetes.io/hostname
            weight: 100
      terminationGracePeriodSeconds: 30
      initContainers:
      - name: waitfor-itom-vault-8200
        image: atf.intranet.bb.com.br:5001/hpeswitom/opensuse-base:15.3-0032
        imagePullPolicy: IfNotPresent
        command: [ "sh", "-c", "until nc -z itom-vault 8200 -w 5 ; do echo waiting for itom-vault:8200...; sleep 5; done; exit 0"]
        resources: {}
        securityContext:
          runAsNonRoot: true
          runAsUser: 1999
          runAsGroup: 1999
      - name: waitfor-vertica-service
        image: atf.intranet.bb.com.br:5001/hpeswitom/itom-busybox:1.38.0-009
        imagePullPolicy: IfNotPresent
        command: [ "sh", "-c", "cmd=''; vh_local=pxl0nnmi0022.disposistivos.bb.com.br; vp_local=5443; for i in $(echo $vh_local | sed 's/,/ /g' | awk '{$1=$1};1'); do cmd=$(echo $cmd '|| nc -z '$i' '$vp_local' -w 5 '); done; cmd=${cmd:3}; echo 'command is : '$cmd''; until eval $(echo $cmd); do echo 'waiting for '$vh_local' with port '$vp_local' ... '; sleep 5; done; exit 0;"]
        resources: {}
        securityContext:
          runAsNonRoot: true
          runAsUser: 1999
          runAsGroup: 1999

      - name: install
        image: atf.intranet.bb.com.br:5001/hpeswitom/kubernetes-vault-init:0.15.0-0038
        imagePullPolicy: IfNotPresent
        env:
        - name: CERT_COMMON_NAME
          value: "itom-di-metadata-server-svc"
        resources: {}
        securityContext:
          runAsUser: 1999
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - name: vault-token
          mountPath: /var/run/secrets/boostport.com
      containers:
        - name: itom-di-metadata-server
          image: atf.intranet.bb.com.br:5001/hpeswitom/itom-data-ingestion-metadata-server:2.5.0-25
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - "/metadata-server/bin/liveness.sh"
            initialDelaySeconds: 360
            periodSeconds: 20
            timeoutSeconds: 5
            failureThreshold: 3
            successThreshold: 1
          readinessProbe:
            exec:
              command:
              - "/metadata-server/bin/readiness.sh"
            initialDelaySeconds: 60
            periodSeconds: 20
            timeoutSeconds: 5
            failureThreshold: 18
            successThreshold: 1
          env:
            # SSL
            # SSl mode Connection. Default Value is true
            - name: METADATA_SERVER_UID
              value: "1999"
            - name: METADATA_SERVER_GID
              value: "1999"
            - name: DATABASE_SSL
              value: "false"
            - name: DATABASE_HOST
              value: "pxl0nnmi0022.disposistivos.bb.com.br"
            - name: DATABASE_PORT
              value: "5443"
            - name: DATABASE_NAME
              value: "verticadb"
            - name: DATABASE_USER
              value: "vertica_rwuser"
            - name: DATABASE_RO_USER
              value: "vertica_rouser"
            - name: METADATA_SERVER_BASE_URL
              value: 
            - name: LOGCONFIG_FROM_CONFIGMAP
              value: "true"
            - name: DATABASE_USER_MANAGED_RESOURCE_POOL
              value: "false"
            # For accessing vertica password value from vault
            - name: DATABASE_PASSWORD_KEY
              value: "ITOMDI_DBA_PASSWORD_KEY"
            - name: DATABASE_RO_PASSWORD_KEY
              value: "ITOMDI_ROUSER_PASSWORD_KEY"
            - name: HIKARICP_CONNECTION_TIMEOUT
              value: "60000"
            - name: HIKARICP_CONNECTION_MAXIMUM_LIFETIME
              value: "1800000"
            - name: HIKARICP_MINIMUM_IDLE_CONNECTIONS
              value: "2"
            - name: HIKARICP_MAXIMUM_POOL_SIZE
              value: "10"
            - name: HIKARICP_CONNECTION_POOL_NAME
              value: "store_once_pool"
            - name: HIKARICP_IDLE_SESSION_TIMEOUT
              value:               
            # Data Retention parameters
            - name: SCHEDULED_DATA_CLEANUP_INTERVAL_DAYS
              value: "1"
            - name: DATABASE_ANALYZE_STATISTICS_INTERVAL_DAYS
              value: "-1"
            # For JVM args
            - name: JAVA_OPTS
              value: -Xmx1024m -XX:+HeapDumpOnOutOfMemoryError
            #Tenant and Deployment
            - name: DI_TENANT
              value: provider            
            - name: DI_DEPLOYMENT 
              value: default
            - name: PURGE_AFTER_DELETE
              value: "false"
            - name: PROCESS_DELETE_ON_INITIAL_CONFIGURATION
              value: "false"
            - name: NODE_NAME
              valueFrom:
               fieldRef:
                fieldPath: spec.nodeName
            - name: POD_NAME
              valueFrom:
               fieldRef:
                fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
               fieldRef:
                fieldPath: metadata.namespace
            - name: CONTAINER_NAME
              value: metadata-server
            # Hibernate batch size
            - name: JDBC_BATCH_SIZE
              value: "100"
            # Config Client Environment Variables
            - name: CONFIG_STORE_DB_ENABLE
              value: "true"
            - name: CONFIG_SERVER_SERVICE_NAME
              value: "itom-di-administration-svc"
            - name: CONFIG_SERVER_SERVICE_PORT
              value: "18443"
            - name: CLIENT_HEART_BEAT_MS
              value: "0"
            - name: SERVER_HEART_BEAT_MS
              value: "0"
            - name: CONNECT_RETRY_DELAY_MS
              value: "120000"
            - name: AUTO_RESTART_ON_ERROR
              value: "true"
            - name: CONFIG_CLIENT_NOTIFICATION_PROCESSING_NUMTHREADS
              value: "5"
            - name: ALLOWED_PERCENTAGE_OF_COLUMNS_TO_DELETE
              value: "40%"
          resources:
            limits:
              cpu: "1"
              memory: 3072Mi
            requests:
              cpu: "0.2"
              memory: 512Mi
          volumeMounts:
            - name: vault-token
              mountPath: /var/run/secrets/boostport.com
            - name: cm-logback
              mountPath: /mnt/itom/metadata-server/app-conf/logback
            - name: metadata-server-log-vol
              mountPath: /mnt/itom/metadata-server/log
            - name: secret-volume
              mountPath: /mnt/itom/metadata-server/ssl/server-cas
        - name: kubernetes-vault-renew
          image: atf.intranet.bb.com.br:5001/hpeswitom/kubernetes-vault-renew:0.15.0-0038
          imagePullPolicy: IfNotPresent
          volumeMounts:
          - name: vault-token
            mountPath: /var/run/secrets/boostport.com

      volumes:
        - name: cm-logback
          configMap:
            name: metadata-server-logback-cm
        - name: metadata-server-vol
          persistentVolumeClaim:
            claimName: config-idm
        - name: metadata-server-log-vol
          persistentVolumeClaim:
            claimName: log-idm
        - name: secret-volume
          projected:
            sources:
            - secret:
                name: metadata-server-secret
            - configMap:
                name: default-ca-certificates
        - name: vault-token
          emptyDir: {}
---
# Source: nomultimate/charts/itomdimonitoring/templates/vertica-prom-deployment.yaml
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

apiVersion: apps/v1
kind: Deployment
metadata:
  name: "itomdimonitoring-verticapromexporter"
  namespace: default
  labels:
    app.kubernetes.io/name: "itomdimonitoring-verticapromexporter"
    app: itomdimonitoring
    app.kubernetes.io/managed-by: RELEASE-NAME
    app.kubernetes.io/version: 2.5.0-61
    itom.microfocus.com/capability: itom-data-ingestion
    tier.itom.microfocus.com/backend: backend
    chart: itomdimonitoring-2.5.0-61
    release: RELEASE-NAME
    heritage: Helm
    component: verticapromexporter
    cluster: itomdimonitoring
  annotations:    
    deployment.microfocus.com/default-replica-count: "1"
    deployment.microfocus.com/runlevel: UP
    configmap.reloader.stakater.com/reload: "default-ca-certificates"
spec:
  selector:
    matchLabels:
      app: itomdimonitoring
      release: RELEASE-NAME
      component: verticapromexporter
  template:
    metadata:
      labels:
        app.kubernetes.io/name: "itomdimonitoring-verticapromexporter"
        app: itomdimonitoring
        app.kubernetes.io/managed-by: RELEASE-NAME
        app.kubernetes.io/version: 2.5.0-61
        itom.microfocus.com/capability: itom-data-ingestion
        tier.itom.microfocus.com/backend: backend
        release: RELEASE-NAME
        component: verticapromexporter
        cluster: itomdimonitoring
      annotations:
        checksum/config: b277f0cf473625a80c439f76896195e96652d21d115fdeb09b5fe77747810d63
        pod.boostport.com/vault-approle: default-default
        pod.boostport.com/vault-init-container: generate-certificates
    spec:    
      serviceAccount: itom-di-monitoring-sa
      serviceAccountName: itom-di-monitoring-sa    
      securityContext:
        runAsNonRoot: true
        runAsUser: 1999
        runAsGroup: 1999
      initContainers:
      - name: waitfor-itom-vault-8200
        image: atf.intranet.bb.com.br:5001/hpeswitom/opensuse-base:15.3-0032
        imagePullPolicy: IfNotPresent
        command: [ "sh", "-c", "until nc -z itom-vault 8200 -w 5 ; do echo waiting for itom-vault:8200...; sleep 5; done; exit 0"]
        resources: {}
        securityContext:
          runAsNonRoot: true
          runAsUser: 1999
          runAsGroup: 1999
      - name: waitfor-vertica-service
        image: atf.intranet.bb.com.br:5001/hpeswitom/itom-busybox:1.38.0-009
        imagePullPolicy: IfNotPresent
        command: [ "sh", "-c", "cmd=''; vh_local=pxl0nnmi0022.disposistivos.bb.com.br; vp_local=5443; for i in $(echo $vh_local | sed 's/,/ /g' | awk '{$1=$1};1'); do cmd=$(echo $cmd '|| nc -z '$i' '$vp_local' -w 5 '); done; cmd=${cmd:3}; echo 'Command is : '$cmd''; until eval $(echo $cmd); do echo 'waiting for '$vh_local' with port '$vp_local' ... '; sleep 5; done; exit 0;"]
        resources: {}
      # This init container will generate certificates. 
      - name: generate-certificates
        image: atf.intranet.bb.com.br:5001/hpeswitom/kubernetes-vault-init:0.15.0-0038
        imagePullPolicy: IfNotPresent
        env:
          - name: CERT_COMMON_NAME
            value: "itomdimonitoring-verticapromexporter"
        volumeMounts:
        - name: vault-token
          mountPath: /var/run/secrets/boostport.com
      containers:
      - name: certificate-renew
        image: atf.intranet.bb.com.br:5001/hpeswitom/kubernetes-vault-renew:0.15.0-0038
        imagePullPolicy: IfNotPresent
        volumeMounts:
        - name: vault-token
          mountPath: /var/run/secrets/boostport.com
      - name: "itomdimonitoring-verticapromexporter"
        image: atf.intranet.bb.com.br:5001/hpeswitom/itom-data-ingestion-vertica-prom-exporter:2.5.0-7
        imagePullPolicy: IfNotPresent
        readinessProbe:
          httpGet:
            path: /healthz
            port: health-port
          initialDelaySeconds: 20
          periodSeconds: 90
          timeoutSeconds: 5
          failureThreshold: 1
          successThreshold: 1
        resources:
          limits:
            cpu: 250m
            memory: 512Mi
          requests:
            cpu: 100m
            memory: 256Mi
        ports:
        - name: metrics-port
          containerPort: 8443
        - name: health-port
          containerPort: 8080
        volumeMounts:
        - name: vault-token
          mountPath: /var/run/secrets/boostport.com
        - name: custom-server-cert-volume
          mountPath: /mnt/itom/exporter/certs
        envFrom:
        - configMapRef:
            name: "itomdimonitoring-verticapromexporter"
        env:
        - name: TLS_CERT_FILE_PATH
          value: "/var/run/secrets/boostport.com/server.crt"
        - name: TLS_KEY_FILE_PATH
          value: "/var/run/secrets/boostport.com/server.key"
        - name: GODEBUG
          value: "x509ignoreCN=0"
      volumes:
        - name: custom-server-cert-volume
          projected:
            sources:
            - secret:
                name: vert-prom-secret
            - configMap:
                name: default-ca-certificates
        - name: vault-token
          emptyDir: {}
---
# Source: nomultimate/charts/itomdipostload/templates/postload-taskcontroller-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: itom-di-postload-taskcontroller
  labels:
    app.kubernetes.io/name: itom-di-postload-taskcontroller
    app.kubernetes.io/managed-by: RELEASE-NAME
    app.kubernetes.io/version: 2.5.0-50
    itom.microfocus.com/capability: itom-data-ingestion
    tier.itom.microfocus.com/backend: backend
    app: itom-di-postload-taskcontroller
  annotations:
    deployment.microfocus.com/default-replica-count: "1"
    deployment.microfocus.com/runlevel: UP
spec:
  replicas: 1
  selector:
    matchLabels:
      app: itom-di-postload-taskcontroller
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
  template:
    metadata:
      labels:
        app.kubernetes.io/name: itom-di-postload-taskcontroller
        app.kubernetes.io/managed-by: RELEASE-NAME
        app.kubernetes.io/version: 2.5.0-50
        itom.microfocus.com/capability: itom-data-ingestion
        tier.itom.microfocus.com/backend: backend
        app: itom-di-postload-taskcontroller
      annotations:
        checksum/config: 505eba336c80e2998cc2e1e46a7f5135cf163053da157a14e10296285eae4062
        prometheus.io/port: "8443"
        prometheus.io/scrape: "true"
        prometheus.io/path: "/metrics"
        pod.boostport.com/vault-approle: default-default
        pod.boostport.com/vault-init-container: install
    spec:
      securityContext:
        runAsUser: 1999
        runAsGroup: 1999
        fsGroup: 1999
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - itom-di-postload-processor-taskgenerator
              topologyKey: kubernetes.io/hostname
            weight: 100
      
      serviceAccount: itom-di-postload-sa
      serviceAccountName: itom-di-postload-sa
    
      terminationGracePeriodSeconds: 
      containers:
        - name: itom-di-postload-taskcontroller-cnt
          image: atf.intranet.bb.com.br:5001/hpeswitom/itom-data-ingestion-postload-taskcontroller:2.5.0-50
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - sh
              - /taskgenerator/bin/liveness.sh
            initialDelaySeconds: 420
            periodSeconds: 20
            timeoutSeconds: 5
            failureThreshold: 3
            successThreshold: 1
          readinessProbe:
            exec:
              command:
              - sh
              - /taskgenerator/bin/readiness.sh
            initialDelaySeconds: 30
            periodSeconds: 20
            timeoutSeconds: 5
            failureThreshold: 18
            successThreshold: 1
          env:
          - name: POSTLOAD_PULSAR_NAMESPACE
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: postload.pulsar.namespace
          - name: POSTLOAD_TASK_TOPIC
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: postload.task.topic
          - name: POSTLOAD_STATUS_TOPIC
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: postload.status.topic
          - name: POSTLOAD_STATE_TOPIC
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: postload.state.topic
          - name: POSTLOAD_TASK_EXECUTION_INTERVAL_MILLIS
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: postload.task.execution.interval.millis
          - name: CONSUMERS_PER_TE
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: postload.consumers.per.task.executor
          - name: POSTLOAD_USE_RECEIVE_TO_CONSUME
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: postload.use.receive.to.consume
          - name: POSTLOAD_ENABLE_TOPIC_MONITORING
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: postload.enable.topic.monitoring
          - name: POSTLOAD_CONFIG_STORE_DB_ENABLE
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: config.store.db.enable
          - name: CONFIG_SERVER_HOSTNAME
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: config.server.hostname
          - name: CONFIG_SERVER_PORT
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: config.server.port
          - name: CONFIG_SERVER_CONNECT_RETRY_DELAY_MS
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: config.server.connect.retry.delay.ms
          - name: CONFIG_SERVER_CLIENT_HEART_BEAT
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: config.server.client.heart.beat
          - name: CONFIG_SERVER_SERVER_HEART_BEAT
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: config.server.server.heart.beat
          - name: CONFIG_SERVER_MESSAGE_BUFFER_SIZE_LIMIT_IN_MB
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: config.server.message.buffer.size.limit.in.mb
          - name: POSTLOAD_PULSAR_SERVICE_NAME
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: postload.pulsar.service.name
          - name: POSTLOAD_PULSAR_BROKER_SERVICE_PORT
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: postload.pulsar.broker.service.port
          - name: POSTLOAD_PULSAR_BROKER_SERVICE_PORT_TLS
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: postload.pulsar.broker.service.port.tls
          - name: POSTLOAD_PULSAR_WEB_SERVICE_PORT
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: postload.pulsar.web.service.port
          - name: POSTLOAD_PULSAR_WEB_SERVICE_PORT_TLS
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: postload.pulsar.web.service.port.tls
          - name: POSTLOAD_PULSAR_TLS_ENABLE
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: postload.pulsar.tls.enable
          - name: POSTLOAD_PULSAR_AUTH_ENABLE
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: postload.pulsar.auth.enable
          - name: POSTLOAD_PULSAR_AUTH_CLASS
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: postload.pulsar.auth.class
          - name: POSTLOAD_PULSAR_TLS_HOSTNAME_VERIFICATION
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: postload.pulsar.tls.hostname.verification
          - name: POSTLOAD_PULSAR_CONNECTION_RETRY_INTERVAL_SECONDS
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: postload.pulsar.connection.retry.interval.seconds
          - name: VERTICA_HOSTNAMES
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: vertica.hostname
          - name: VERTICA_USERNAME
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: vertica.username
          - name: VERTICA_PASS_KEY
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: vertica.password.key
          - name: VERTICA_DB
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: vertica.db
          - name: VERTICA_PORT
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: vertica.port
          - name: VERTICA_TLS_ENABLED
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: vertica.tlsEnabled
          - name: VERTICA_TLS_MODE
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: vertica.tlsMode
          - name: VERTICA_CONNECTION_RETRY_INTERVAL_SECONDS
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: vertica.connection.retry.interval.seconds
          - name: DI_TENANT
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: di.tenant
          - name: DI_DEPLOYMENT
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: di.deployment
          - name: JAVA_OPTS
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: postload.task.controller.jvm.args
          - name: POSTLOAD_SCRAPE_INTERVAL_MILLIS
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: postload.scrape.interval.millis
          - name: POSTLOAD_ACCEPTABLE_MISSED_TRIGGER_DELAY_SECONDS
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: postload.acceptable.missed.trigger.delay.seconds
          - name: LOGGING_FROM_CONFIGMAP
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: taskGenerator.logconfig.from.configmap
          - name: LOG_TO_FILE
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: taskGenerator.logconfig.log.to.file
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName
          - name: POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: CONTAINER_NAME
            value: postload-taskcontroller
          ports:
            - containerPort: 8443
          resources:
            limits:
              cpu: "4"
              memory: 1024Mi
            requests:
              cpu: "0.5"
              memory: 512Mi
          volumeMounts:
          - name: vault-token
            mountPath: /var/run/secrets/boostport.com
          - name: di-taskcontroller-conf-vol
            mountPath: /mnt/itom/postload/conf
            subPath: di/postload/conf
          - name: di-taskcontroller-conf-vol
            mountPath: /mnt/itom/postload/bin/task-artifacts
            subPath: di/postload/bin/task-artifacts
          - name: di-taskcontroller-conf-vol
            mountPath: /mnt/itom/postload/samples
            subPath: di/postload/samples
          - name: di-taskcontroller-conf-vol
            mountPath: /mnt/itom/postload/ext
            subPath: di/postload/ext
          - name: di-taskcontroller-data-vol
            mountPath: /mnt/itom/postload/data
            subPath: di/data-processor/data
          - name: di-taskcontroller-log-vol
            mountPath: /mnt/itom/postload/log
          - name: itom-de
            mountPath: /taskgenerator/tasktype-ext
          - name: custom-server-ca-volume
            mountPath: /taskgenerator/ssl/ca
          - name: cm-logback
            mountPath: /taskgenerator/conf-local/logback
        - name: kubernetes-vault-renew
          image: atf.intranet.bb.com.br:5001/hpeswitom/kubernetes-vault-renew:0.15.0-0038
          imagePullPolicy: IfNotPresent
          volumeMounts:
          - name: vault-token
            mountPath: /var/run/secrets/boostport.com
      initContainers:
      - name: waitfor-itom-vault-8200
        image: atf.intranet.bb.com.br:5001/hpeswitom/opensuse-base:15.3-0032
        imagePullPolicy: IfNotPresent
        command: [ "sh", "-c", "until nc -z itom-vault 8200 -w 5 ; do echo waiting for itom-vault:8200...; sleep 5; done; exit 0"]
        resources: {}
        securityContext:
          runAsNonRoot: true
          runAsUser: 1999
          runAsGroup: 1999
      - name: waitfor-vertica-service
        image: atf.intranet.bb.com.br:5001/hpeswitom/itom-busybox:1.38.0-009
        imagePullPolicy: IfNotPresent
        command: [ "sh", "-c", "cmd=''; vh_local=pxl0nnmi0022.disposistivos.bb.com.br; vp_local=5443; for i in $(echo $vh_local | sed 's/,/ /g' | awk '{$1=$1};1'); do cmd=$(echo $cmd '|| nc -z '$i' '$vp_local' -w 5 '); done; cmd=${cmd:3}; echo 'command is : '$cmd''; until eval $(echo $cmd); do echo 'waiting for '$vh_local' with port '$vp_local' ... '; sleep 5; done; exit 0;"]
        resources: {}
        securityContext:
          runAsNonRoot: true
          runAsUser: 1999
          runAsGroup: 1999
          
      - env:
        - name: CERT_COMMON_NAME
          value: "Common_Name:itom-di-message-bus-svc,File_Name:server"
        image: atf.intranet.bb.com.br:5001/hpeswitom/kubernetes-vault-init:0.15.0-0038
        imagePullPolicy: IfNotPresent
        name: install
        resources: {}
        securityContext:
          runAsUser: 1999
          runAsGroup: 0
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
          - mountPath: /var/run/secrets/boostport.com
            name: vault-token
      - name: itom-di-init-enrichment-cnt
        image: atf.intranet.bb.com.br:5001/hpeswitom/itom-data-ingestion-enrichment:2.5.0-14
        imagePullPolicy: IfNotPresent
        command: ["/bin/sh", "-c"]
        args:
          - echo copying task type files to shared volume;
            cp /itomde/data-enrich/* /taskgenerator/tasktype-ext;
            echo done;
        volumeMounts:
          - mountPath: /taskgenerator/tasktype-ext
            name: itom-de
      volumes:
      - name: cm-logback
        configMap:
          name: taskcontroller-logback-cm
      - name: di-taskcontroller-conf-vol
        persistentVolumeClaim:
          claimName: config-idm
      - name: di-taskcontroller-data-vol
        persistentVolumeClaim:
          claimName: data-idm
      - name: di-taskcontroller-log-vol
        persistentVolumeClaim:
          claimName: log-idm
      - name: vault-token
        emptyDir: {}
      - name: itom-de
        emptyDir: {}
      - name: custom-server-ca-volume
        projected:
          sources:
          - secret:
              name: itom-di-postload-secret
          - configMap:
              name: default-ca-certificates
---
# Source: nomultimate/charts/itomdipostload/templates/postload-taskexecutor-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: itom-di-postload-taskexecutor
  labels:
    app.kubernetes.io/name: itom-di-postload-taskexecutor
    app.kubernetes.io/managed-by: RELEASE-NAME
    app.kubernetes.io/version: 2.5.0-50
    itom.microfocus.com/capability: itom-data-ingestion
    tier.itom.microfocus.com/backend: backend
    app: itom-di-postload-taskexecutor
  annotations:
    deployment.microfocus.com/default-replica-count: "1"
    deployment.microfocus.com/runlevel: UP
    configmap.reloader.stakater.com/reload: "default-ca-certificates"
spec:
  replicas: 1
  selector:
    matchLabels:
      app: itom-di-postload-taskexecutor
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
  template:
    metadata:
      labels:
        app.kubernetes.io/name: itom-di-postload-taskexecutor
        app.kubernetes.io/managed-by: RELEASE-NAME
        app.kubernetes.io/version: 2.5.0-50
        itom.microfocus.com/capability: itom-data-ingestion
        tier.itom.microfocus.com/backend: backend
        app: itom-di-postload-taskexecutor
      annotations:
        checksum/config: 505eba336c80e2998cc2e1e46a7f5135cf163053da157a14e10296285eae4062
        prometheus.io/port: "8443"
        prometheus.io/scrape: "true"
        prometheus.io/path: "/metrics"
        pod.boostport.com/vault-approle: default-default
        pod.boostport.com/vault-init-container: install
    spec:
      securityContext:
        runAsUser: 1999
        runAsGroup: 1999
        fsGroup: 1999
      
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - itom-di-postload-processor-taskexecutor
              topologyKey: kubernetes.io/hostname
            weight: 100
      
      serviceAccount: itom-di-postload-sa
      serviceAccountName: itom-di-postload-sa
      
      terminationGracePeriodSeconds: 
      containers:
        - name: itom-di-postload-taskexecutor-cnt
          image: atf.intranet.bb.com.br:5001/hpeswitom/itom-data-ingestion-postload-taskexecutor:2.5.0-50
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - sh
              - /taskexecutor/bin/liveness.sh
            initialDelaySeconds: 420
            periodSeconds: 20
            timeoutSeconds: 5
            failureThreshold: 3
            successThreshold: 1
          readinessProbe:
            exec:
              command:
              - sh
              - /taskexecutor/bin/readiness.sh
            initialDelaySeconds: 30
            periodSeconds: 20
            timeoutSeconds: 5
            failureThreshold: 18
            successThreshold: 1
          env:
          - name: POSTLOAD_PULSAR_NAMESPACE
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: postload.pulsar.namespace
          - name: POSTLOAD_TASK_TOPIC
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: postload.task.topic
          - name: POSTLOAD_STATUS_TOPIC
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: postload.status.topic
          - name: TIMEZONE
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: postload.timezone
          - name: BATCH_SIZE
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: postload.enrichmentbatchsize
          - name: RESOURCE_POOL
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: postload.postresourcepool
          - name: CONSUMERS_PER_TE
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: postload.consumers.per.task.executor
          - name: POSTLOAD_USE_RECEIVE_TO_CONSUME
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: postload.use.receive.to.consume
          - name: POSTLOAD_ENABLE_TE_PROCESS_MONITORING
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: postload.enable.te.process.monitoring
          - name: POSTLOAD_CONFIG_STORE_DB_ENABLE
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: config.store.db.enable
          - name: CSV_DIRECT_LOAD_COMPRESS_ARCHIVE
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: csvdirectload.compress.archive.files
          - name: CSV_DIRECT_LOAD_COMPRESS_FAILED
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: csvdirectload.compress.failed.files
          - name: CSV_DIRECT_LOAD_COMPRESS_EACH
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: csvdirectload.compress.each.file
          - name: CSV_DIRECT_LOAD_CLEANUP_RETENTION_PERIOD_DAYS
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: csvdirectload.cleanup.retention.period.days
          - name: CSV_DIRECT_LOAD_CLEANUP_RETENTION_SIZE_MB
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: csvdirectload.cleanup.retention.size.mb
          - name: CSV_DIRECT_LOAD_CLEANUP_PURGE_CONTROLTABLE_AFTER_DELETE
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: csvdirectload.cleanup.purge.controltable.after.delete
          - name: POSTLOAD_PULSAR_SERVICE_NAME
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: postload.pulsar.service.name
          - name: POSTLOAD_PULSAR_BROKER_SERVICE_PORT
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: postload.pulsar.broker.service.port
          - name: POSTLOAD_PULSAR_BROKER_SERVICE_PORT_TLS
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: postload.pulsar.broker.service.port.tls
          - name: POSTLOAD_PULSAR_WEB_SERVICE_PORT
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: postload.pulsar.web.service.port
          - name: POSTLOAD_PULSAR_WEB_SERVICE_PORT_TLS
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: postload.pulsar.web.service.port.tls
          - name: POSTLOAD_PULSAR_TLS_ENABLE
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: postload.pulsar.tls.enable
          - name: POSTLOAD_PULSAR_AUTH_ENABLE
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: postload.pulsar.auth.enable
          - name: POSTLOAD_PULSAR_AUTH_CLASS
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: postload.pulsar.auth.class
          - name: POSTLOAD_PULSAR_TLS_HOSTNAME_VERIFICATION
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: postload.pulsar.tls.hostname.verification
          - name: POSTLOAD_PULSAR_CONNECTION_RETRY_INTERVAL_SECONDS
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: postload.pulsar.connection.retry.interval.seconds
          - name: VERTICA_HOSTNAMES
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: vertica.hostname
          - name: VERTICA_USERNAME
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: vertica.username
          - name: VERTICA_PASS_KEY
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: vertica.password.key
          - name: VERTICA_DB
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: vertica.db
          - name: VERTICA_PORT
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: vertica.port
          - name: VERTICA_TLS_ENABLED
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: vertica.tlsEnabled
          - name: VERTICA_TLS_MODE
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: vertica.tlsMode
          - name: VERTICA_CONNECTION_RETRY_INTERVAL_SECONDS
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: vertica.connection.retry.interval.seconds
          - name: DI_TENANT
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: di.tenant
          - name: DI_DEPLOYMENT
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: di.deployment
          - name: JAVA_OPTS
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: postload.task.executor.jvm.args
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName
          - name: POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: CONTAINER_NAME
            value: postload-taskexecutor
          - name: LOGGING_FROM_CONFIGMAP
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: taskExecutor.logconfig.from.configmap
          - name: LOG_TO_FILE
            valueFrom:
              configMapKeyRef:
                name: itom-di-postload-cm
                key: taskExecutor.logconfig.log.to.file
          ports:
            - containerPort: 8443
          resources:
            limits:
              cpu: "4"
              memory: 2048Mi
            requests:
              cpu: "0.5"
              memory: 512Mi
          volumeMounts:
          - name: vault-token
            mountPath: /var/run/secrets/boostport.com
          - name: di-taskexecutor-conf-vol
            mountPath: /mnt/itom/postload/conf
            subPath: di/postload/conf
          - name: di-taskexecutor-conf-vol
            mountPath: /mnt/itom/postload/bin/task-artifacts
            subPath: di/postload/bin/task-artifacts
          - name: di-taskexecutor-conf-vol
            mountPath: /mnt/itom/postload/ext
            subPath: di/postload/ext
          - name: di-taskexecutor-conf-vol
            mountPath: /mnt/itom/postload/samples
            subPath: di/postload/samples
          - name: di-taskexecutor-data-vol
            mountPath: /mnt/itom/postload/data
            subPath: di/data-processor/data
          - name: di-taskexecutor-log-vol
            mountPath: /mnt/itom/postload/log
          - name: itom-de
            mountPath: /taskexecutor/tasktype-ext
          - name: custom-server-ca-volume
            mountPath: /taskexecutor/ssl/ca
          - name: cm-logback
            mountPath: /taskexecutor/conf-local/logback
        - name: kubernetes-vault-renew
          image: atf.intranet.bb.com.br:5001/hpeswitom/kubernetes-vault-renew:0.15.0-0038
          imagePullPolicy: IfNotPresent
          volumeMounts:
          - name: vault-token
            mountPath: /var/run/secrets/boostport.com
      initContainers:
      - name: waitfor-itom-vault-8200
        image: atf.intranet.bb.com.br:5001/hpeswitom/opensuse-base:15.3-0032
        imagePullPolicy: IfNotPresent
        command: [ "sh", "-c", "until nc -z itom-vault 8200 -w 5 ; do echo waiting for itom-vault:8200...; sleep 5; done; exit 0"]
        resources: {}
        securityContext:
          runAsNonRoot: true
          runAsUser: 1999
          runAsGroup: 1999
      - name: waitfor-vertica-service
        image: atf.intranet.bb.com.br:5001/hpeswitom/itom-busybox:1.38.0-009
        imagePullPolicy: IfNotPresent
        command: [ "sh", "-c", "cmd=''; vh_local=pxl0nnmi0022.disposistivos.bb.com.br; vp_local=5443; for i in $(echo $vh_local | sed 's/,/ /g' | awk '{$1=$1};1'); do cmd=$(echo $cmd '|| nc -z '$i' '$vp_local' -w 5 '); done; cmd=${cmd:3}; echo 'command is : '$cmd''; until eval $(echo $cmd); do echo 'waiting for '$vh_local' with port '$vp_local' ... '; sleep 5; done; exit 0;"]
        resources: {}
        securityContext:
          runAsNonRoot: true
          runAsUser: 1999
          runAsGroup: 1999
             
      - env:
        - name: CERT_COMMON_NAME
          value: "Common_Name:itom-di-message-bus-svc,File_Name:server"
        image: atf.intranet.bb.com.br:5001/hpeswitom/kubernetes-vault-init:0.15.0-0038
        imagePullPolicy: IfNotPresent
        name: install
        resources: {}
        securityContext:
          runAsUser: 1999
          runAsGroup: 0
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
          - mountPath: /var/run/secrets/boostport.com
            name: vault-token
      - name: itom-di-init-enrichment-cnt
        image: atf.intranet.bb.com.br:5001/hpeswitom/itom-data-ingestion-enrichment:2.5.0-14
        imagePullPolicy: IfNotPresent
        command: ["/bin/sh", "-c"]
        args:
          - echo copying task type files to shared volume;
            cp /itomde/data-enrich/* /taskexecutor/tasktype-ext;
            echo done;
        volumeMounts:
          - mountPath: /taskexecutor/tasktype-ext
            name: itom-de
      volumes:
      - name: cm-logback
        configMap:
          name: taskexecutor-logback-cm
      - name: di-taskexecutor-conf-vol
        persistentVolumeClaim:
          claimName: config-idm
      - name: di-taskexecutor-data-vol
        persistentVolumeClaim:
          claimName: data-idm
      - name: di-taskexecutor-log-vol
        persistentVolumeClaim:
          claimName: log-idm
      - name: vault-token
        emptyDir: {}
      - name: itom-de
        emptyDir: {}
      - name: custom-server-ca-volume
        projected:
          sources:
          - secret:
              name: itom-di-postload-secret
          - configMap:
              name: default-ca-certificates
---
# Source: nomultimate/charts/itomdipulsar/templates/broker/broker-deployment.yaml
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

apiVersion: apps/v1
kind: Deployment
metadata:
  name: "itomdipulsar-broker"
  namespace: default
  labels:
    app: itomdipulsar
    chart: itomdipulsar-2.8.1-31
    release: RELEASE-NAME
    heritage: Helm
    cluster: itomdipulsar
    app.kubernetes.io/name: itomdipulsar-broker
    app.kubernetes.io/managed-by: RELEASE-NAME
    app.kubernetes.io/version: 2.8.1-31
    itom.microfocus.com/capability: itom-data-ingestion
    tier.itom.microfocus.com/backend: backend
    component: broker
  annotations:
    deployment.microfocus.com/default-replica-count: "1"
    deployment.microfocus.com/runlevel: UP
    
    configmap.reloader.stakater.com/reload: "api-client-ca-certificates"
spec:
  replicas: 3
  selector:
    matchLabels:
      app: itomdipulsar
      release: RELEASE-NAME
      component: broker
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1        # how many pods we can add at a time
      maxUnavailable: 1
  template:
    metadata:
      labels:
        app: itomdipulsar
        chart: itomdipulsar-2.8.1-31
        release: RELEASE-NAME
        heritage: Helm
        cluster: itomdipulsar
        app.kubernetes.io/name: itomdipulsar-broker
        app.kubernetes.io/managed-by: RELEASE-NAME
        app.kubernetes.io/version: 2.8.1-31
        itom.microfocus.com/capability: itom-data-ingestion
        tier.itom.microfocus.com/backend: backend
        component: broker
        itom.microfocus.com/tls-metrics: "true"
      annotations:
        pod.boostport.com/vault-init-container: generate-certificates
        pod.boostport.com/vault-approle: default-default
        prometheus.io/scrape: "true"
        prometheus.io/port: "8443"
        checksum/config: b9fe6c7a0aa508afec38465492bc4cab1500382cb2315867e6db816e66921ef3
        deployment.microfocus.com/default-replica-count: "1"
        deployment.microfocus.com/runlevel: UP
    spec:
      securityContext:
        runAsUser: 1999
        runAsGroup: 1999
        fsGroup: 1999

      serviceAccount: itomdipulsar-broker-sa
      serviceAccountName: itomdipulsar-broker-sa
      
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                    - key: "app"
                      operator: In
                      values:
                      - "itomdipulsar"
                    - key: "release"
                      operator: In
                      values:
                      - RELEASE-NAME
                    - key: "component"
                      operator: In
                      values:
                      - broker
                topologyKey: "kubernetes.io/hostname"
          
      terminationGracePeriodSeconds: 30
      initContainers:
      - name: waitfor-itom-vault-8200
        image: atf.intranet.bb.com.br:5001/hpeswitom/opensuse-base:15.3-0032
        imagePullPolicy: IfNotPresent
        command: [ "sh", "-c", "until nc -z itom-vault 8200 -w 5 ; do echo waiting for itom-vault:8200...; sleep 5; done; exit 0"]
        resources: {}
        securityContext:
          runAsNonRoot: true
          runAsUser: 1999
          runAsGroup: 1999
      # This init container will generate certificates.
      - name: generate-certificates
        image: atf.intranet.bb.com.br:5001/hpeswitom/kubernetes-vault-init:0.15.0-0038
        # command: ["sh", "-c"]
        # args:
        # - >
        #   sleep 5
        imagePullPolicy: IfNotPresent
        env:
          - name: CERT_COMMON_NAME
            value: "Realm:RID,Common_Name:,Additional_SAN:itomdipulsar-broker;Realm:RE,Common_Name:,Additional_SAN:itomdipulsar-broker/itomdipulsar-broker.default/itomdipulsar-broker.default.svc"
        volumeMounts:
        - name: vault-token
          mountPath: /var/run/secrets/boostport.com
      # This init container will wait for zookeeper to be ready before
      # deploying the bookies
      - name: wait-zookeeper-ready
        image: atf.intranet.bb.com.br:5001/hpeswitom/itom-pulsar-core:2.8.1-26
        imagePullPolicy: IfNotPresent
        command: ["sh", "-c"]
        args:
          - >-
            source bin/coso-init.sh;
            until bin/pulsar zookeeper-shell -server itomdipulsar-zookeeper:2281 ls /admin/clusters/itomdipulsar; do
              sleep 3;
            done;
        volumeMounts:
        - name: cosoinit
          mountPath: "/pulsar/bin/coso-init.sh"
          subPath: coso-init.sh
        
        - name: keytool
          mountPath: "/pulsar/keytool/keytool.sh"
          subPath: keytool.sh
        - name: tmp
          mountPath: /pulsar/tmp
        - name: conf
          mountPath: /pulsar/conf
        - name: logs
          mountPath: /pulsar/logs
        - name: ssl
          mountPath: /pulsar/ssl
        - name: vault-token
          mountPath: /var/run/secrets/boostport.com
      # This init container will wait for bookkeeper to be ready before
      # deploying the broker
      - name: wait-bookkeeper-ready
        image: atf.intranet.bb.com.br:5001/hpeswitom/itom-pulsar-core:2.8.1-26
        imagePullPolicy: IfNotPresent
        command: ["sh", "-c"]
        args:
          - >
            source bin/coso-init.sh;
            bin/apply-config-from-env.py conf/client.conf conf/bookkeeper.conf;
            until bin/bookkeeper shell whatisinstanceid; do
              echo "bookkeeper cluster is not initialized yet. backoff for 3 seconds ...";
              sleep 3;
            done;
            echo "bookkeeper cluster is already initialized";
            bookieServiceNumber="$(nslookup -timeout=10 itomdipulsar-bookkeeper | grep Name | wc -l)";
            until [ ${bookieServiceNumber} -ge 2 ]; do
              echo "bookkeeper cluster itomdipulsar isn't ready yet ... check in 10 seconds ...";
              sleep 10;
              bookieServiceNumber="$(nslookup -timeout=10 itomdipulsar-bookkeeper | grep Name | wc -l)";
            done;
            echo "bookkeeper cluster is ready";
        envFrom:
          - configMapRef:
              name: "itomdipulsar-bookkeeper"
        volumeMounts:
          - name: tmp
            mountPath: /pulsar/tmp
          - name: conf
            mountPath: /pulsar/conf
          - name: logs
            mountPath: /pulsar/logs
          - name: ssl
            mountPath: /pulsar/ssl
          - name: vault-token
            mountPath: /var/run/secrets/boostport.com
          - name: cosoinit
            mountPath: "/pulsar/bin/coso-init.sh"
            subPath: coso-init.sh
          
          - name: keytool
            mountPath: "/pulsar/keytool/keytool.sh"
            subPath: keytool.sh
      containers:
      - name: certificate-renew
        image: atf.intranet.bb.com.br:5001/hpeswitom/kubernetes-vault-renew:0.15.0-0038
        imagePullPolicy: IfNotPresent
        volumeMounts:
        - name: vault-token
          mountPath: /var/run/secrets/boostport.com
      - name: "itomdipulsar-broker"
        image: atf.intranet.bb.com.br:5001/hpeswitom/itom-pulsar-core:2.8.1-26
        imagePullPolicy: IfNotPresent
        livenessProbe:
          httpGet:
            path: /status.html
            port: 8443
            scheme: HTTPS
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 10
          successThreshold: 1
        readinessProbe:
          httpGet:
            path: /status.html
            port: 8443
            scheme: HTTPS
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 10
          successThreshold: 1
        resources:
          limits:
            cpu: 4
            memory: 4Gi
          requests:
            cpu: 0.5
            memory: 2Gi
        command: ["sh", "-c"]
        args:
        - >
          source bin/coso-init.sh;
          
          bin/apply-config-from-env.py conf/client.conf;
          bin/apply-config-from-env.py conf/broker.conf;
          echo "OK" > /pulsar/tmp/status;
          bin/pulsar zookeeper-shell -server itomdipulsar-zookeeper:2281 get /loadbalance/brokers/${HOSTNAME}.itomdipulsar-broker.default.svc.cluster.local:8080;
          while [ $? -eq 0 ]; do
            echo "broker ${HOSTNAME}.itomdipulsar-broker.default.svc.cluster.local znode still exists ... check in 10 seconds ...";
            sleep 10;
            bin/pulsar zookeeper-shell -server itomdipulsar-zookeeper:2281 get /loadbalance/brokers/${HOSTNAME}.itomdipulsar-broker.default.svc.cluster.local:8080;
          done;
          cd /pulsar/ssl/vault;
          /pulsar/bin/pulsar broker;
        ports:
        # prometheus needs to access /metrics endpoint
        - name: http
          containerPort: 8080
        - name: https
          containerPort: 8443
        - name: pulsarssl
          containerPort: 6651
        env:
        - name: advertisedAddress
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        envFrom:
        - configMapRef:
            name: "itomdipulsar-broker"
        volumeMounts:
          - name: tmp
            mountPath: /pulsar/tmp
          - name: conf
            mountPath: /pulsar/conf
          - name: download
            mountPath: /pulsar/download
          - name: logs
            mountPath: /pulsar/logs
          - name: ssl
            mountPath: /pulsar/ssl
          - name: vault-token
            mountPath: /var/run/secrets/boostport.com
          - name: cosoinit
            mountPath: "/pulsar/bin/coso-init.sh"
            subPath: coso-init.sh
          - name: cosoexternalcert
            mountPath: "/pulsar/ssl/custom/ca"
          - name: "itomdipulsar-broker-log4j2"
            mountPath: "/pulsar/conf/log4j2.yaml"
            subPath: log4j2.yaml
          
          - name: keytool
            mountPath: "/pulsar/keytool/keytool.sh"
            subPath: keytool.sh
          
          
          
          - name: "function-worker-config"
            mountPath: "/pulsar/conf/functions_worker.yml"
            subPath: functions_worker.yml
      volumes:
      - name: tmp
        emptyDir: {}
      - name: conf
        emptyDir: {}
      - name: ssl
        emptyDir: {}
      - name: logs
        emptyDir: {}
      - name: download
        emptyDir: {}
      - name: vault-token
        emptyDir: {}
      - name: cosoinit
        configMap:
          name: "itomdipulsar-cosoinit-configmap"
          defaultMode: 0755
      
      - name: cosoexternalcert
        configMap:
          name: api-client-ca-certificates
      
      - name: keytool
        configMap:
          name: "itomdipulsar-keytool-configmap"
          defaultMode: 0755
      - name: "itomdipulsar-broker-log4j2"
        configMap:
          name: "itomdipulsar-broker"
      
      
      
      - name: "function-worker-config"
        configMap:
          name: "itomdipulsar-functions-worker-configfile"
---
# Source: nomultimate/charts/itomdipulsar/templates/proxy/proxy-deployment.yaml
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

apiVersion: apps/v1
kind: Deployment
metadata:
  name: "itomdipulsar-proxy"
  namespace: default
  labels:
    app: itomdipulsar
    chart: itomdipulsar-2.8.1-31
    release: RELEASE-NAME
    heritage: Helm
    cluster: itomdipulsar
    app.kubernetes.io/name: itomdipulsar-proxy
    app.kubernetes.io/managed-by: RELEASE-NAME
    app.kubernetes.io/version: 2.8.1-31
    itom.microfocus.com/capability: itom-data-ingestion
    tier.itom.microfocus.com/backend: backend
    component: proxy
  annotations:
    deployment.microfocus.com/default-replica-count: "1"
    deployment.microfocus.com/runlevel: UP
    
    configmap.reloader.stakater.com/reload: "api-client-ca-certificates"
spec:
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1        # how many pods we can add at a time
      maxUnavailable: 1
  replicas: 2
  selector:
    matchLabels:
      app: itomdipulsar
      release: RELEASE-NAME
      component: proxy
  template:
    metadata:
      labels:
        app: itomdipulsar
        chart: itomdipulsar-2.8.1-31
        release: RELEASE-NAME
        heritage: Helm
        cluster: itomdipulsar
        app.kubernetes.io/name: itomdipulsar-proxy
        app.kubernetes.io/managed-by: RELEASE-NAME
        app.kubernetes.io/version: 2.8.1-31
        itom.microfocus.com/capability: itom-data-ingestion
        tier.itom.microfocus.com/backend: backend
        component: proxy
      annotations:
        pod.boostport.com/vault-init-container: generate-certificates
        pod.boostport.com/vault-approle: default-default
        prometheus.io/scrape: "true"
        prometheus.io/port: "8443"
        checksum/config: 2030be6478862918e543d76bf59947ebba5904903a46b434009b7ed2be9e5aee
        deployment.microfocus.com/default-replica-count: "1"
        deployment.microfocus.com/runlevel: UP
    spec:
      securityContext:
        runAsUser: 1999
        runAsGroup: 1999
        fsGroup: 1999

      serviceAccount: itomdipulsar-sa
      serviceAccountName: itomdipulsar-sa
      
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                    - key: "app"
                      operator: In
                      values:
                      - "itomdipulsar"
                    - key: "release"
                      operator: In
                      values:
                      - RELEASE-NAME
                    - key: "component"
                      operator: In
                      values:
                      - proxy
                topologyKey: "kubernetes.io/hostname"
          
      terminationGracePeriodSeconds: 30
      initContainers:
      - name: waitfor-itom-vault-8200
        image: atf.intranet.bb.com.br:5001/hpeswitom/opensuse-base:15.3-0032
        imagePullPolicy: IfNotPresent
        command: [ "sh", "-c", "until nc -z itom-vault 8200 -w 5 ; do echo waiting for itom-vault:8200...; sleep 5; done; exit 0"]
        resources: {}
        securityContext:
          runAsNonRoot: true
          runAsUser: 1999
          runAsGroup: 1999
      # This init container will generate certificates.
      - name: generate-certificates
        image: atf.intranet.bb.com.br:5001/hpeswitom/kubernetes-vault-init:0.15.0-0038
        # command: ["sh", "-c"]
        # args:
        # - >
        #   sleep 5
        imagePullPolicy: IfNotPresent
        env:
          - name: CERT_COMMON_NAME
            value: "Realm:RID,Common_Name:,Additional_SAN:itomdipulsar-proxy;Realm:RE,Common_Name:,Additional_SAN:itomdipulsar-proxy/itomdipulsar-proxy.default/itomdipulsar-proxy.default.svc"
        volumeMounts:
        - name: vault-token
          mountPath: /var/run/secrets/boostport.com
      - name: wait-zookeeper-ready
        image: atf.intranet.bb.com.br:5001/hpeswitom/itom-pulsar-core:2.8.1-26
        imagePullPolicy: IfNotPresent
        command: ["sh", "-c"]
        args:
          - >-
            source bin/coso-init.sh;
            until bin/pulsar zookeeper-shell -server itomdipulsar-zookeeper:2281 ls /admin/clusters/itomdipulsar; do
              sleep 3;
            done;
        volumeMounts:
        - name: cosoinit
          mountPath: "/pulsar/bin/coso-init.sh"
          subPath: coso-init.sh
        
        - name: keytool
          mountPath: "/pulsar/keytool/keytool.sh"
          subPath: keytool.sh
        - name: tmp
          mountPath: /pulsar/tmp
        - name: conf
          mountPath: /pulsar/conf
        - name: logs
          mountPath: /pulsar/logs
        - name: ssl
          mountPath: /pulsar/ssl
        - name: vault-token
          mountPath: /var/run/secrets/boostport.com
      # This init container will wait for at least one broker to be ready before
      # deploying the proxy
      - name: wait-broker-ready
        image: atf.intranet.bb.com.br:5001/hpeswitom/itom-pulsar-core:2.8.1-26
        imagePullPolicy: IfNotPresent
        command: ["sh", "-c"]
        args:
          - >-
            source bin/coso-init.sh;
            set -e;
            brokerServiceNumber="$(nslookup -timeout=10 itomdipulsar-broker | grep Name | wc -l)";
            until [ ${brokerServiceNumber} -ge 1 ]; do
              echo "pulsar cluster itomdipulsar isn't initialized yet ... check in 10 seconds ...";
              sleep 10;
              brokerServiceNumber="$(nslookup -timeout=10 itomdipulsar-broker | grep Name | wc -l)";
            done;
        volumeMounts:
        - name: tmp
          mountPath: /pulsar/tmp
        - name: conf
          mountPath: /pulsar/conf
        - name: logs
          mountPath: /pulsar/logs
        - name: ssl
          mountPath: /pulsar/ssl
        - name: vault-token
          mountPath: /var/run/secrets/boostport.com
        - name: cosoinit
          mountPath: "/pulsar/bin/coso-init.sh"
          subPath: coso-init.sh
      containers:
      - name: certificate-renew
        image: atf.intranet.bb.com.br:5001/hpeswitom/kubernetes-vault-renew:0.15.0-0038
        imagePullPolicy: IfNotPresent
        volumeMounts:
        - name: vault-token
          mountPath: /var/run/secrets/boostport.com
      - name: "itomdipulsar-proxy"
        image: atf.intranet.bb.com.br:5001/hpeswitom/itom-pulsar-core:2.8.1-26
        imagePullPolicy: IfNotPresent
        livenessProbe:
          httpGet:
            path: /status.html
            port: 8443
            scheme: HTTPS
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 10
          successThreshold: 1
        readinessProbe:
          httpGet:
            path: /status.html
            port: 8443
            scheme: HTTPS
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 10
          successThreshold: 1
        resources:
          limits:
            cpu: 4
            memory: 4Gi
          requests:
            cpu: 0.5
            memory: 2Gi
        command: ["sh", "-c"]
        args:
        - >
          source bin/coso-init.sh;
          bin/apply-config-from-env.py conf/proxy.conf conf/client.conf;
          echo "OK" > /pulsar/tmp/status;
          bin/pulsar proxy;
        ports:
        # prometheus needs to access /metrics endpoint
        - name: http
          containerPort: 8080
        - name: https
          containerPort: 8443
        - name: pulsarssl
          containerPort: 6651
        envFrom:
        - configMapRef:
            name: "itomdipulsar-proxy"
        volumeMounts:
          - name: tmp
            mountPath: /pulsar/tmp
          - name: conf
            mountPath: /pulsar/conf
          - name: logs
            mountPath: /pulsar/logs
          - name: ssl
            mountPath: /pulsar/ssl
          - name: vault-token
            mountPath: /var/run/secrets/boostport.com
          - name: cosoinit
            mountPath: "/pulsar/bin/coso-init.sh"
            subPath: coso-init.sh
          - name: cosoexternalcert
            mountPath: "/pulsar/ssl/custom/ca"
          - name: "itomdipulsar-proxy-log4j2"
            mountPath: "/pulsar/conf/log4j2.yaml"
            subPath: log4j2.yaml
          
          - name: keytool
            mountPath: "/pulsar/keytool/keytool.sh"
            subPath: keytool.sh
      volumes:
        - name: tmp
          emptyDir: {}
        - name: conf
          emptyDir: {}
        - name: ssl
          emptyDir: {}
        - name: logs
          emptyDir: {}
        - name: vault-token
          emptyDir: {}
        - name: cosoinit
          configMap:
            name: "itomdipulsar-cosoinit-configmap"
            defaultMode: 0755
        
        - name: cosoexternalcert
          configMap:
            name: api-client-ca-certificates
        - name: "itomdipulsar-proxy-log4j2"
          configMap:
            name: "itomdipulsar-proxy"
        
        - name: keytool
          configMap:
            name: "itomdipulsar-keytool-configmap"
            defaultMode: 0755
---
# Source: nomultimate/charts/nomapiserver/templates/itom-nom-api-server.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: itom-nom-api-server
  labels:
    name: itom-nom-api-server
  annotations:
    deployment.microfocus.com/default-replica-count: "1"
    deployment.microfocus.com/runlevel: UP
spec:
  selector:
    matchLabels:
      name: itom-nom-api-server
  replicas: 3
  strategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        name: itom-nom-api-server
      annotations:
        pod.boostport.com/vault-approle: default-default
        pod.boostport.com/vault-init-container: install
    spec:
      serviceAccountName: itom-nom-api-server
      nodeSelector:
        Worker: label
      securityContext:
        runAsNonRoot: true
        runAsUser: 1999
        runAsGroup: 1999
        fsGroup: 1999
      terminationGracePeriodSeconds: 30
      initContainers:
        - name: waitfor-itom-vault-8200
          image: atf.intranet.bb.com.br:5001/hpeswitom/opensuse-base:15.3-0032
          imagePullPolicy: IfNotPresent
          command: [ "sh", "-c", "until nc -z itom-vault 8200 -w 5 ; do echo waiting for itom-vault:8200...; sleep 5; done; exit 0"]
          resources: {}
          securityContext:
            runAsNonRoot: true
            runAsUser: 1999
            runAsGroup: 1999
        - name: waitfor-nomzk-client-svc-2181
          image: atf.intranet.bb.com.br:5001/hpeswitom/opensuse-base:15.3-0032
          imagePullPolicy: IfNotPresent
          command: [ "sh", "-c", "until nc -z nomzk-client-svc 2181 -w 5 ; do echo waiting for nomzk-client-svc:2181...; sleep 5; done; exit 0"]
          resources: {}
          securityContext:
            runAsNonRoot: true
            runAsUser: 1999
            runAsGroup: 1999
        - name: waitfor-itom-idm-svc-default-80
          image: atf.intranet.bb.com.br:5001/hpeswitom/opensuse-base:15.3-0032
          imagePullPolicy: IfNotPresent
          command: [ "sh", "-c", "until nc -z itom-idm-svc.default 80 -w 5 ; do echo waiting for itom-idm-svc.default:80...; sleep 5; done; exit 0"]
          resources: {}
          securityContext:
            runAsNonRoot: true
            runAsUser: 1999
            runAsGroup: 1999

        - name: install
          image: atf.intranet.bb.com.br:5001/hpeswitom/kubernetes-vault-init:0.15.0-0038
          env:
          - name: CERT_COMMON_NAME
            value: Realm:RE,Common_Name:itom-nom-api-server,Additional_SAN:nom-api-server.homologacao.nuvem.hm.bb.com.br/itom-nom-api-server/itom-nom-api-server.default
          volumeMounts:
            - mountPath: /var/run/secrets/boostport.com
              name: vault-token

        - name: store-monitoring-data
          image: atf.intranet.bb.com.br:5001/hpeswitom/itom-nom-config:2.2.73
          args: ["/tasks/apigateway/600-store-monitoring-data-zk.json"]
          env:
            - name: IDM_HOST
              value: "itom-idm-svc.default"
            - name: IDM_PORT
              value: "80"
            - name: IDM_USER
              value: "nomadmin"
            - name: IDM_PASSWORD_KEY
              value: "idm_nom_admin_password"
            - name: ZK_SVC_NAME
              value: "nomzk-client-svc.default"
            - name: ZK_PORT
              value: "2181"
            - name: ZK_NAMESPACE
              value: "nom"
            - name: MIXED_MODE
              valueFrom:
                configMapKeyRef:
                  name: nom-ultimate-cm
                  key: nom.mixedMode
            - name: NNM_USER_NAME
              valueFrom:
                configMapKeyRef:
                  name: nom-ultimate-cm
                  key: nnmi.user
            - name: NNM_USER_PASSWORD_KEY
              valueFrom:
                configMapKeyRef:
                  name: nom-ultimate-cm
                  key: nnmi.passwordKey
            - name: AUTOMATION_URL
              value: "<none>"
            - name: MONITORING_URL
              valueFrom:
                configMapKeyRef:
                  name: nom-ultimate-cm
                  key: nnmi.url
            - name: MONITORING_FAILOVER_URL
              valueFrom:
                configMapKeyRef:
                  name: nom-ultimate-cm
                  key: nnmi.failoverUrl
          volumeMounts:
            - mountPath: /var/run/secrets/boostport.com
              name: vault-token
            - mountPath: /data
              name: apiserver-data-vol
            - name: certs-volume
              mountPath: /data/nom/certificates

        - name: store-automation-data
          image: atf.intranet.bb.com.br:5001/hpeswitom/itom-nom-config:2.2.73
          args: ["/tasks/apigateway/650-store-automation-data-zk.json"]
          env:
            # This job does not have to run in legacy OpsB environment????
            - name: IDM_HOST
              value: "itom-idm-svc.default"
            - name: IDM_PORT
              value: "80"
            - name: IDM_USER
              value: "nomadmin"
            - name: IDM_PASSWORD_KEY
              value: "idm_nom_admin_password"
            - name: ZK_SVC_NAME
              value: "nomzk-client-svc.default"
            - name: ZK_PORT
              value: "2181"
            - name: ZK_NAMESPACE
              value: "nom"
            - name: MIXED_MODE
              valueFrom:
                configMapKeyRef:
                  name: nom-ultimate-cm
                  key: nom.mixedMode
            - name: NNM_USER_NAME
              valueFrom:
                configMapKeyRef:
                  name: nom-ultimate-cm
                  key: nnmi.user
            - name: NNM_USER_PASSWORD_KEY
              valueFrom:
                configMapKeyRef:
                  name: nom-ultimate-cm
                  key: nnmi.passwordKey
            - name: AUTOMATION_URL
              valueFrom:
                configMapKeyRef:
                  name: nom-ultimate-cm
                  key: na.url
            - name: MONITORING_URL
              value: "<none>"
            - name: MONITORING_FAILOVER_URL
              value: "<none>"
          volumeMounts:
            - mountPath: /var/run/secrets/boostport.com
              name: vault-token
            - mountPath: /data
              name: apiserver-data-vol
            - name: certs-volume
              mountPath: /data/nom/certificates

      containers:
        

        - name: itom-nom-api-server
          image: atf.intranet.bb.com.br:5001/hpeswitom/itom-nom-api-server:1.1.192
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 8443
            - containerPort: 8080
          livenessProbe:
            httpGet:
              scheme: HTTPS
              path: /apiserver/health/
              port: 8443
            initialDelaySeconds: 20
            timeoutSeconds: 10
            periodSeconds: 60
            successThreshold: 1
            failureThreshold: 5
          readinessProbe:
            httpGet:
              scheme: HTTPS
              path: /apiserver/health/
              port: 8443
            initialDelaySeconds: 5
            timeoutSeconds: 15
            periodSeconds: 5
            failureThreshold: 2
          resources:
            limits:
              cpu: "2"
              memory: "1024Mi"
            requests:
              cpu: "100m"
              memory: "512Mi"
          securityContext:
            readOnlyRootFilesystem: true
            capabilities:
              drop: ["CHOWN", "DAC_OVERRIDE", "FSETID", "FOWNER", "MKNOD", "SETGID", "SETUID", "SETFCAP", "SETPCAP", "NET_BIND_SERVICE", "SYS_CHROOT", "KILL", "AUDIT_WRITE","NET_RAW"]
          env:
          - name: VAULT_SIGNING_KEY
            value: "VAULT_SIGNING_KEY"
          - name: IDM_HOST
            value: "itom-idm-svc.default"
          - name: IDM_PORT
            value: "80"
          - name: APLMS_INGRESS_PATH
            value: "https://itom-autopass-lms:80/autopass"
          - name: MIXED_MODE
            valueFrom:
              configMapKeyRef:
                name: nom-ultimate-cm
                key: nom.mixedMode
          - name: SERVER_MAX_IO_THREADS
            value: "10"
          - name: SERVER_MAX_WORKER_THREADS
            value: "100"
          - name: API_SERVICE_MAX_CONNECTION
            value: "50"
          - name: API_SERVICE_CACHED_CONNECTIONS
            value: "25"
          - name: API_SERVICE_MAX_REQUEST_QUEUE_SIZE
            value: "25"
          - name: API_REQUEST_TIMEOUT_MILLIS
            value: "60000"
          - name: API_SERVICE_IDLE_CONNECTION_TIMEOUT_MILLIS
            value: "-1"
          - name: ITOM_COSO_DI_ADMIN_SERVICE_HOST
            value: "itom-di-administration-svc"
          - name: ITOM_COSO_DI_ADMIN_SERVICE_PORT
            value: "18443"
          - name: ITOM_COSO_DI_DATA_ACCESS_SERVICE_HOST
            value: "itom-di-data-access-svc"
          - name: ITOM_COSO_DI_DATA_ACCESS_SERVICE_PORT
            value: "28443"


          envFrom:
            - configMapRef:
                name: itom-nom-api-server-config
          volumeMounts:
          - name: apiserver-log-vol
            mountPath: /var/opt/OV/log
            subPath: api-server
          - name: apiserver-data-vol
            mountPath: /var/opt/OV/server
            subPath: api-server
          - name: certs-volume
            mountPath: /var/opt/OV/shared/certs
          - name: vault-token
            mountPath: /var/run/secrets/boostport.com
          - name: temp
            mountPath: /tmp
        - name: kubernetes-vault-renew
          image: atf.intranet.bb.com.br:5001/hpeswitom/kubernetes-vault-renew:0.15.0-0038
          imagePullPolicy: IfNotPresent
          volumeMounts:
            - name: vault-token
              mountPath: /var/run/secrets/boostport.com
      restartPolicy: Always
      volumes:
      - name: apiserver-data-vol
        persistentVolumeClaim:
          claimName: data-idm
      - name: apiserver-log-vol
        persistentVolumeClaim:
          claimName: log-idm
      - name: certs-volume
        configMap:
          name: default-ca-certificates
      - name: vault-token
        emptyDir: {}
      - name: temp
        emptyDir: {}
---
# Source: nomultimate/charts/nomhttp/templates/itom-nom-default-http-backend.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: itom-nom-default-http-backend
  labels:
    name: itom-nom-default-http-backend
  annotations:
    deployment.microfocus.com/default-replica-count: "1"
    deployment.microfocus.com/runlevel: UP
spec:
  replicas: 1
  selector:
    matchLabels:
      name: itom-nom-default-http-backend
  template:
    metadata:
      labels:
        name: itom-nom-default-http-backend
    spec:
      serviceAccountName: itom-nom-http-backend
      securityContext:
        runAsNonRoot: true
        runAsUser: 1999
        runAsGroup: 1999
        fsGroup: 1999
      containers:
        - name: itom-nom-default-http-backend
          image: atf.intranet.bb.com.br:5001/hpeswitom/itom-nom-web-server:1.0.55
          imagePullPolicy: IfNotPresent
          livenessProbe:
            httpGet:
              path: /healthz
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 30
            timeoutSeconds: 5
          ports:
          - containerPort: 8080
      nodeSelector:
        Worker: label
---
# Source: nomultimate/charts/nomxui/templates/itom-nom-ui.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: itom-nom-ui
  labels:
    name: itom-nom-ui
  annotations:
    deployment.microfocus.com/default-replica-count: "3"
    deployment.microfocus.com/runlevel: UP
spec:
  selector:
    matchLabels:
      name: itom-nom-ui
  replicas: 3
  strategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        name: itom-nom-ui
      annotations:
        pod.boostport.com/vault-approle: default-default
        pod.boostport.com/vault-init-container: install
    spec:
      serviceAccountName: nom-ui
      nodeSelector:
        Worker: label

      securityContext:
        runAsNonRoot: true
        runAsUser: 1999
        runAsGroup: 1999
        fsGroup: 1999
      terminationGracePeriodSeconds: 300
      initContainers:
      - name: waitfor-itom-vault-8200
        image: atf.intranet.bb.com.br:5001/hpeswitom/opensuse-base:15.3-0032
        imagePullPolicy: IfNotPresent
        command: [ "sh", "-c", "until nc -z itom-vault 8200 -w 5 ; do echo waiting for itom-vault:8200...; sleep 5; done; exit 0"]
        resources: {}
        securityContext:
          runAsNonRoot: true
          runAsUser: 1999
          runAsGroup: 1999
      
      
      - name: waitfor-bvd-explore-4000
        image: atf.intranet.bb.com.br:5001/hpeswitom/opensuse-base:15.3-0032
        imagePullPolicy: IfNotPresent
        command: [ "sh", "-c", "until nc -z bvd-explore 4000 -w 5 ; do echo waiting for bvd-explore:4000...; sleep 5; done; exit 0"]
        resources: {}
        securityContext:
          runAsNonRoot: true
          runAsUser: 1999
          runAsGroup: 1999
      
      
      
      - name: waitfor-itom-idm-svc-80
        image: atf.intranet.bb.com.br:5001/hpeswitom/opensuse-base:15.3-0032
        imagePullPolicy: IfNotPresent
        command: [ "sh", "-c", "until nc -z itom-idm-svc 80 -w 5 ; do echo waiting for itom-idm-svc:80...; sleep 5; done; exit 0"]
        resources: {}
        securityContext:
          runAsNonRoot: true
          runAsUser: 1999
          runAsGroup: 1999
      

      - name: install
        image: atf.intranet.bb.com.br:5001/hpeswitom/kubernetes-vault-init:0.15.0-0038
        env:
        - name: CERT_COMMON_NAME
          value: itom-nom-ui
        volumeMounts:
        - mountPath: /var/run/secrets/boostport.com
          name: vault-token
      containers:
      - name: itom-nom-ui
        image: atf.intranet.bb.com.br:5001/hpeswitom/itom-nom-ui:3.4.77
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 8443
        livenessProbe:
          httpGet:
            path: /health/
            port: 8443
            scheme: HTTPS
          initialDelaySeconds: 660
          periodSeconds: 60
          timeoutSeconds: 15
        readinessProbe:
          httpGet:
            path: /health/
            port: 8443
            scheme: HTTPS
          initialDelaySeconds: 60
          timeoutSeconds: 15
          periodSeconds: 30
          failureThreshold: 20
        resources:
          limits:
            cpu: "2"
            memory: "2048Mi"
          requests:
            cpu: "100m"
            memory: "500Mi"
        securityContext:
          readOnlyRootFilesystem: true
          capabilities:
            drop: ["CHOWN", "DAC_OVERRIDE", "FSETID", "FOWNER", "MKNOD", "SETGID", "SETUID", "SETFCAP", "SETPCAP", "NET_BIND_SERVICE", "SYS_CHROOT", "KILL", "AUDIT_WRITE", "NET_RAW"]
        env:
        - name: INTEGRATION_ADMIN_USER_KEY
          value: "nomadmin"
        - name: INTEGRATION_ADMIN_PASSWORD_VAULT_KEY
          value: "idm_nom_admin_password"
        - name: isCoso
          value: "true"
        - name: PERF_DATASOURCE
          valueFrom:
            configMapKeyRef:
              name: nom-ultimate-cm
              key: perfTroubleshooting.datasource
        - name: isEpr
          value: "false"
        - name: NOM_CERTIFICATES_DIR
          value: "/var/opt/nom/certificates"
        - name: FQDN
          value: 
        - name: NOM_UI_HTTPS_PORT
          value: "8443"

        volumeMounts:
        - name: ui-data-vol
          mountPath: /var/opt/OV
          subPath: ui-app
        - name: ui-log-vol
          mountPath: /var/opt/OV/log
          subPath: ui-app
        - name: vault-token
          mountPath: /var/run/secrets/boostport.com
        - name: temp
          mountPath: /tmp
        - name: bvd-explore-var
          mountPath: /var/bvd
          subPath: bvd/var/bvd
      - name: kubernetes-vault-renew
        image: atf.intranet.bb.com.br:5001/hpeswitom/kubernetes-vault-renew:0.15.0-0038
        imagePullPolicy: IfNotPresent
        volumeMounts:
        - name: vault-token
          mountPath: /var/run/secrets/boostport.com
      restartPolicy: Always
      volumes:
      - name: ui-conf-vol
        persistentVolumeClaim:
          claimName: config-idm
      - name: ui-data-vol
        persistentVolumeClaim:
          claimName: data-idm
      - name: ui-log-vol
        persistentVolumeClaim:
          claimName: log-idm
      - name: bvd-explore-var
        persistentVolumeClaim:
          claimName: config-idm
      - name: vault-token
        emptyDir: {}
      - name: temp
        emptyDir: {}
---
# Source: nomultimate/charts/itomdiminio/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: itom-di-minio
  labels:
    app: itomdiminio
    chart: itomdiminio-2.5.0-21
    release: RELEASE-NAME
    heritage: Helm
    cluster: itom-di-minio
    app.kubernetes.io/name:  itom-di-minio
    app.kubernetes.io/managed-by: RELEASE-NAME
    app.kubernetes.io/version: 2.5.0-21
    itom.microfocus.com/capability: minio
    tier.itom.microfocus.com/backend: backend
  annotations:
        deployment.microfocus.com/default-replica-count: "4"
        deployment.microfocus.com/runlevel: UP
spec:
  updateStrategy:
    type: RollingUpdate
  podManagementPolicy: "Parallel"
  serviceName: itom-di-minio-svc
  replicas: 4
  selector:
    matchLabels:
      app: itomdiminio
      release: RELEASE-NAME
  template:
    metadata:
      name: itom-di-minio
      labels:
        app: itomdiminio
        release: RELEASE-NAME
        app: itomdiminio
        chart: itomdiminio-2.5.0-21
        release: RELEASE-NAME
        heritage: Helm
        cluster: itom-di-minio
        app.kubernetes.io/name:  itom-di-minio
        app.kubernetes.io/managed-by: RELEASE-NAME
        app.kubernetes.io/version: 2.5.0-21
        itom.microfocus.com/capability: minio
        tier.itom.microfocus.com/backend: backend
      annotations:
        pod.boostport.com/vault-approle: default-default
        pod.boostport.com/vault-init-container: generate-certificates
        checksum/secrets: 3bb48f6596fc6bcc5704aec9e9475c261c299efed07fe5b18d5cb24a34076cd4
        checksum/config: 1c8ce5cb512b93aaad70623ca4c261677f7447f3081a4759db48a64779e3082d
        deployment.microfocus.com/default-replica-count: "4"
        deployment.microfocus.com/runlevel: UP
        prometheus.io/path: /minio/prometheus/metrics
        prometheus.io/port: "9000"
        prometheus.io/scrape: "true"
    spec:
      securityContext:
        runAsUser: 1999
        runAsGroup: 1999
        fsGroup: 1999
      
      serviceAccount: itom-di-minio-sa
      serviceAccountName: itom-di-minio-sa
      
      initContainers:
      - name: waitfor-itom-vault-8200
        image: atf.intranet.bb.com.br:5001/hpeswitom/opensuse-base:15.3-0032
        imagePullPolicy: IfNotPresent
        command: [ "sh", "-c", "until nc -z itom-vault 8200 -w 5 ; do echo waiting for itom-vault:8200...; sleep 5; done; exit 0"]
        resources: {}
        securityContext:
          runAsNonRoot: true
          runAsUser: 1999
          runAsGroup: 1999
      - env:
        - name: CERT_COMMON_NAME
          value: "Realm:RID,Common_Name:,Additional_SAN:*.itom-di-minio-svc.default.svc.cluster.local;Realm:RE,Common_Name:,Additional_SAN:*.itom-di-minio-svc.default.svc.cluster.local"
        image: atf.intranet.bb.com.br:5001/hpeswitom/kubernetes-vault-init:0.15.0-0038
        imagePullPolicy: IfNotPresent
        name: generate-certificates
        resources: {}
        securityContext:
          runAsUser: 1999
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
          - mountPath: /var/run/secrets/boostport.com
            name: vault-token  
      containers:
        - name: certificate-renew
          image: atf.intranet.bb.com.br:5001/hpeswitom/kubernetes-vault-renew:0.15.0-0038
          imagePullPolicy: IfNotPresent
          volumeMounts:
          - name: vault-token
            mountPath: /var/run/secrets/boostport.com
        - name: minio
          image: atf.intranet.bb.com.br:5001/hpeswitom/itom-data-ingestion-minio:2.5.0-21
          imagePullPolicy: IfNotPresent
          command: [ "/bin/sh", "-ce", "mkdir -p certs/CAs && ln -s /var/run/secrets/boostport.com/issue_ca.crt certs/CAs/public.crt && ln -s /var/run/secrets/boostport.com/RE/server1.crt certs/public.crt && ln -s /var/run/secrets/boostport.com/RE/server1.key certs/private.key && ./minio -S certs  server  https://itom-di-minio-{0...3}.itom-di-minio-svc.default.svc.cluster.local/export" ]
          volumeMounts:
            - name: export
              mountPath: /export
            
            - name: minio-config-dir
              mountPath: /root/.minio/
            - name: vault-token
              mountPath: /var/run/secrets/boostport.com
          ports:
            - name: service
              containerPort: 9000
          env:
            - name: MINIO_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: itom-di-minio-secret
                  key: accesskey

            - name: MINIO_SECRET_KEY
              valueFrom:
                secretKeyRef:
                  name: itom-di-minio-secret
                  key: secretkey
            - name: MINIO_BROWSER
              value: "off"
            - name: MINIO_MULTIPART_CLEANUP_MINS
              value: "30"
            - name: MINIO_MULTIPART_EXPIRY_HOURS
              value: "1"
            - name: MINIO_PROMETHEUS_AUTH_TYPE
              value: "public"
          livenessProbe:
            httpGet:
              path: /minio/health/live
              port: service
              scheme: HTTPS
              
            initialDelaySeconds: 60
            periodSeconds: 20
            timeoutSeconds: 
            successThreshold: 
            failureThreshold: 36
          resources:
            limits:
              cpu: 4
              memory: 8Gi
            requests:
              cpu: 250m
              memory: 256Mi
      terminationGracePeriodSeconds: 0
      tolerations:
      - effect: NoExecute
        key: node.kubernetes.io/not-ready
        operator: Exists
        tolerationSeconds: 30
      - effect: NoExecute
        key: node.alpha.kubernetes.io/notReady
        operator: Exists
        tolerationSeconds: 30
      - effect: NoExecute
        key: node.alpha.kubernetes.io/unreachable
        operator: Exists
        tolerationSeconds: 30
      volumes:
        - name: vault-token
          emptyDir: {}
        - name: minio-user
          secret:
            secretName: itom-di-minio
        - name: minio-config-dir
          emptyDir: {}
  volumeClaimTemplates:
    - metadata:
        name: export
        labels:
          app: itomdiminio
      spec:
        accessModes: [ "ReadWriteOnce" ]
        storageClassName: minio-data
        resources:
          requests:
            storage: 10Gi
---
# Source: nomultimate/charts/itomdipulsar/templates/bastion/bastion-statefulset.yaml
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: "itomdipulsar-bastion"
  namespace: default
  labels:
    app: itomdipulsar
    chart: itomdipulsar-2.8.1-31
    release: RELEASE-NAME
    heritage: Helm
    cluster: itomdipulsar
    app.kubernetes.io/name: itomdipulsar-bastion
    app.kubernetes.io/managed-by: RELEASE-NAME
    app.kubernetes.io/version: 2.8.1-31
    itom.microfocus.com/capability: itom-data-ingestion
    tier.itom.microfocus.com/backend: backend
    component: bastion
  annotations:
    deployment.microfocus.com/default-replica-count: "1"
    deployment.microfocus.com/runlevel: UP
    
    configmap.reloader.stakater.com/reload: "api-client-ca-certificates"
spec:
  serviceName: "itomdipulsar-bastion"
  replicas: 1
  updateStrategy:
    type: RollingUpdate
  podManagementPolicy: Parallel
  selector:
    matchLabels:
      app: itomdipulsar
      release: RELEASE-NAME
      component: bastion
  template:
    metadata:
      labels:
        app: itomdipulsar
        chart: itomdipulsar-2.8.1-31
        release: RELEASE-NAME
        heritage: Helm
        cluster: itomdipulsar
        app.kubernetes.io/name: itomdipulsar-bastion
        app.kubernetes.io/managed-by: RELEASE-NAME
        app.kubernetes.io/version: 2.8.1-31
        itom.microfocus.com/capability: itom-data-ingestion
        tier.itom.microfocus.com/backend: backend
        component: bastion
      annotations:
        pod.boostport.com/vault-init-container: generate-certificates
        pod.boostport.com/vault-approle: default-default
        checksum/config: 2517f0b0058211f5c6ad863758ef073a4443d15c0bbe36f2dbd890cef4496a50
        deployment.microfocus.com/default-replica-count: "1"
        deployment.microfocus.com/runlevel: UP
    spec:
      securityContext:
        runAsUser: 1999
        runAsGroup: 1999
        fsGroup: 1999    
      serviceAccount: itomdipulsar-sa
      serviceAccountName: itomdipulsar-sa    
      terminationGracePeriodSeconds: 0
      initContainers:
      - name: waitfor-itom-vault-8200
        image: atf.intranet.bb.com.br:5001/hpeswitom/opensuse-base:15.3-0032
        imagePullPolicy: IfNotPresent
        command: [ "sh", "-c", "until nc -z itom-vault 8200 -w 5 ; do echo waiting for itom-vault:8200...; sleep 5; done; exit 0"]
        resources: {}
        securityContext:
          runAsNonRoot: true
          runAsUser: 1999
          runAsGroup: 1999
      # This init container will generate certificates. 
      - name: generate-certificates
        image: atf.intranet.bb.com.br:5001/hpeswitom/kubernetes-vault-init:0.15.0-0038
        # command: ["sh", "-c"]
        # args:
        # - >
        #   sleep 5
        imagePullPolicy: IfNotPresent
        env:
          - name: CERT_COMMON_NAME
            value: "Realm:RID,Common_Name:,Additional_SAN:itomdipulsar-bastion;Realm:RE,Common_Name:,Additional_SAN:itomdipulsar-bastion/itomdipulsar-bastion.default/itomdipulsar-bastion.default.svc"
        volumeMounts:
        - name: vault-token
          mountPath: /var/run/secrets/boostport.com
      containers:
      - name: certificate-renew
        image: atf.intranet.bb.com.br:5001/hpeswitom/kubernetes-vault-renew:0.15.0-0038
        imagePullPolicy: IfNotPresent
        volumeMounts:
        - name: vault-token
          mountPath: /var/run/secrets/boostport.com
      - name: "pulsar"
        image: atf.intranet.bb.com.br:5001/hpeswitom/itom-pulsar-core:2.8.1-26
        imagePullPolicy: IfNotPresent
        command: ["sh", "-c"]
        args:
        - >
          source bin/coso-init.sh;
          bin/apply-config-from-env.py conf/client.conf;
          bin/apply-config-from-env.py conf/bookkeeper.conf;
          sleep 10000000000
        envFrom:
        - configMapRef:
            name: "itomdipulsar-bastion"
        volumeMounts:
        - name: tmp
          mountPath: /pulsar/tmp
        - name: conf
          mountPath: /pulsar/conf
        - name: logs
          mountPath: /pulsar/logs
        - name: ssl
          mountPath: /pulsar/ssl
        - name: vault-token
          mountPath: /var/run/secrets/boostport.com
        - name: cosoinit
          mountPath: "/pulsar/bin/coso-init.sh"
          subPath: coso-init.sh
        - name: cosoexternalcert
          mountPath: "/pulsar/ssl/custom/ca"
        
        - name: keytool
          mountPath: "/pulsar/keytool/keytool.sh"
          subPath: keytool.sh
        - name: "itomdipulsar-bastion-log4j2"
          mountPath: "/pulsar/conf/log4j2.yaml"
          subPath: log4j2.yaml
      volumes:
      - name: tmp
        emptyDir: {}
      - name: conf
        emptyDir: {}
      - name: ssl
        emptyDir: {}
      - name: logs
        emptyDir: {}
      - name: vault-token
        emptyDir: {}
      - name: cosoinit
        configMap:
          name: "itomdipulsar-cosoinit-configmap"
          defaultMode: 0755
      
      - name: cosoexternalcert
        configMap:
          name: api-client-ca-certificates
      
      - name: keytool
        configMap:
          name: "itomdipulsar-keytool-configmap"
          defaultMode: 0755
      
      - name: "itomdipulsar-bastion-log4j2"
        configMap:
          name: "itomdipulsar-bastion"
---
# Source: nomultimate/charts/itomdipulsar/templates/bookkeeper/bookkeeper-autorecovery-statefulset.yaml
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: "itomdipulsar-autorecovery"
  namespace: default
  labels:
    app: itomdipulsar
    chart: itomdipulsar-2.8.1-31
    release: RELEASE-NAME
    heritage: Helm
    cluster: itomdipulsar
    app.kubernetes.io/name: itomdipulsar-autorecovery
    app.kubernetes.io/managed-by: RELEASE-NAME
    app.kubernetes.io/version: 2.8.1-31
    itom.microfocus.com/capability: itom-data-ingestion
    tier.itom.microfocus.com/backend: backend
    component: autorecovery
  annotations:
    deployment.microfocus.com/default-replica-count: "1"
    deployment.microfocus.com/runlevel: UP
spec:
  serviceName: "itomdipulsar-autorecovery"
  replicas: 1
  updateStrategy:
    type: RollingUpdate
  podManagementPolicy: Parallel
  selector:
    matchLabels:
      app: itomdipulsar
      release: RELEASE-NAME
      component: autorecovery
  template:
    metadata:
      labels:
        app: itomdipulsar
        chart: itomdipulsar-2.8.1-31
        release: RELEASE-NAME
        heritage: Helm
        cluster: itomdipulsar
        app.kubernetes.io/name: itomdipulsar-autorecovery
        app.kubernetes.io/managed-by: RELEASE-NAME
        app.kubernetes.io/version: 2.8.1-31
        itom.microfocus.com/capability: itom-data-ingestion
        tier.itom.microfocus.com/backend: backend
        component: autorecovery
      annotations:
        pod.boostport.com/vault-init-container: generate-certificates
        pod.boostport.com/vault-approle: default-default
        prometheus.io/scrape: "true"
        prometheus.io/port: "8000"
        checksum/config: fb04c7e51122005fc9e6f89a3b48747653b4aa36c2bbbdd59e076080252a8598
        deployment.microfocus.com/default-replica-count: "1"
        deployment.microfocus.com/runlevel: UP
    spec:
      securityContext:
        runAsUser: 1999
        runAsGroup: 1999
        fsGroup: 1999

      serviceAccount: itomdipulsar-sa
      serviceAccountName: itomdipulsar-sa

      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                    - key: "app"
                      operator: In
                      values:
                      - "itomdipulsar"
                    - key: "release"
                      operator: In
                      values:
                      - RELEASE-NAME
                    - key: "component"
                      operator: In
                      values:
                      - autorecovery
                topologyKey: "kubernetes.io/hostname"
          
      terminationGracePeriodSeconds: 30
      initContainers:
      - name: waitfor-itom-vault-8200
        image: atf.intranet.bb.com.br:5001/hpeswitom/opensuse-base:15.3-0032
        imagePullPolicy: IfNotPresent
        command: [ "sh", "-c", "until nc -z itom-vault 8200 -w 5 ; do echo waiting for itom-vault:8200...; sleep 5; done; exit 0"]
        resources: {}
        securityContext:
          runAsNonRoot: true
          runAsUser: 1999
          runAsGroup: 1999
      # This init container will generate certificates.
      - name: generate-certificates
        image: atf.intranet.bb.com.br:5001/hpeswitom/kubernetes-vault-init:0.15.0-0038
        # command: ["sh", "-c"]
        # args:
        # - >
        #   sleep 5
        imagePullPolicy: IfNotPresent
        env:
          - name: CERT_COMMON_NAME
            value: "Realm:RID,Common_Name:,Additional_SAN:itomdipulsar-autorecovery;Realm:RE,Common_Name:,Additional_SAN:itomdipulsar-autorecovery/itomdipulsar-autorecovery.default/itomdipulsar-autorecovery.default.svc"
        volumeMounts:
        - name: vault-token
          mountPath: /var/run/secrets/boostport.com
      # This initContainer will wait for bookkeeper initnewcluster to complete
      # before deploying the bookies
      - name: pulsar-bookkeeper-verify-clusterid
        image: atf.intranet.bb.com.br:5001/hpeswitom/itom-pulsar-core:2.8.1-26
        imagePullPolicy: IfNotPresent
        command: ["sh", "-c"]
        args:
        - >
          source bin/coso-init.sh;
          bin/apply-config-from-env.py conf/bookkeeper.conf;
          until bin/bookkeeper shell whatisinstanceid; do
            sleep 3;
          done;
        envFrom:
        - configMapRef:
            name: "itomdipulsar-autorecovery"
        volumeMounts:
        - name: tmp
          mountPath: /pulsar/tmp
        - name: conf
          mountPath: /pulsar/conf
        - name: logs
          mountPath: /pulsar/logs
        - name: ssl
          mountPath: /pulsar/ssl
        - name: vault-token
          mountPath: /var/run/secrets/boostport.com
        - name: cosoinit
          mountPath: "/pulsar/bin/coso-init.sh"
          subPath: coso-init.sh
        
        - name: keytool
          mountPath: "/pulsar/keytool/keytool.sh"
          subPath: keytool.sh
      containers:
      - name: certificate-renew
        image: atf.intranet.bb.com.br:5001/hpeswitom/kubernetes-vault-renew:0.15.0-0038
        imagePullPolicy: IfNotPresent
        volumeMounts:
          - name: vault-token
            mountPath: /var/run/secrets/boostport.com
      - name: "itomdipulsar-autorecovery"
        image: atf.intranet.bb.com.br:5001/hpeswitom/itom-pulsar-core:2.8.1-26
        imagePullPolicy: IfNotPresent
        resources:
          limits:
            cpu: 0.5
            memory: 2Gi
          requests:
            cpu: 0.05
            memory: 512Mi
        command: ["sh", "-c"]
        args:
        - >
          source bin/coso-init.sh;
          bin/apply-config-from-env.py conf/bookkeeper.conf;
          bin/bookkeeper autorecovery
        ports:
        - name: http
          containerPort: 8000
        envFrom:
        - configMapRef:
            name: "itomdipulsar-autorecovery"
        volumeMounts:
        - name: tmp
          mountPath: /pulsar/tmp
        - name: conf
          mountPath: /pulsar/conf
        - name: logs
          mountPath: /pulsar/logs
        - name: ssl
          mountPath: /pulsar/ssl
        - name: vault-token
          mountPath: /var/run/secrets/boostport.com
        - name: cosoinit
          mountPath: "/pulsar/bin/coso-init.sh"
          subPath: coso-init.sh
        
        - name: keytool
          mountPath: "/pulsar/keytool/keytool.sh"
          subPath: keytool.sh
        - name: "itomdipulsar-autorecovery-log4j2"
          mountPath: "/pulsar/conf/log4j2.yaml"
          subPath: log4j2.yaml
      volumes:
      - name: tmp
        emptyDir: {}
      - name: conf
        emptyDir: {}
      - name: ssl
        emptyDir: {}
      - name: logs
        emptyDir: {}
      - name: vault-token
        emptyDir: {}
      - name: cosoinit
        configMap:
          name: "itomdipulsar-cosoinit-configmap"
          defaultMode: 0755
      
      - name: keytool
        configMap:
          name: "itomdipulsar-keytool-configmap"
          defaultMode: 0755
      - name: "itomdipulsar-autorecovery-log4j2"
        configMap:
          name: "itomdipulsar-autorecovery"
---
# Source: nomultimate/charts/itomdipulsar/templates/bookkeeper/bookkeeper-statefulset.yaml
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: "itomdipulsar-bookkeeper"
  namespace: default
  labels:
    app: itomdipulsar
    chart: itomdipulsar-2.8.1-31
    release: RELEASE-NAME
    heritage: Helm
    cluster: itomdipulsar
    app.kubernetes.io/name: itomdipulsar-bookkeeper
    app.kubernetes.io/managed-by: RELEASE-NAME
    app.kubernetes.io/version: 2.8.1-31
    itom.microfocus.com/capability: itom-data-ingestion
    tier.itom.microfocus.com/backend: backend
    component: bookkeeper
  annotations:
    deployment.microfocus.com/default-replica-count: "1"
    deployment.microfocus.com/runlevel: UP
spec:
  serviceName: "itomdipulsar-bookkeeper"
  replicas: 3
  selector:
    matchLabels:
      app: itomdipulsar
      release: RELEASE-NAME
      component: bookkeeper
  updateStrategy:
    type: RollingUpdate
  podManagementPolicy: OrderedReady
  template:
    metadata:
      labels:
        app: itomdipulsar
        chart: itomdipulsar-2.8.1-31
        release: RELEASE-NAME
        heritage: Helm
        cluster: itomdipulsar
        app.kubernetes.io/name: itomdipulsar-bookkeeper
        app.kubernetes.io/managed-by: RELEASE-NAME
        app.kubernetes.io/version: 2.8.1-31
        itom.microfocus.com/capability: itom-data-ingestion
        tier.itom.microfocus.com/backend: backend
        component: bookkeeper
      annotations:
        pod.boostport.com/vault-init-container: generate-certificates
        pod.boostport.com/vault-approle: default-default
        prometheus.io/scrape: "true"
        prometheus.io/port: "8000"
        checksum/config: 900118a1014d32d850445c0c25c4076d5c0fea10e061a45b313237d68c7b1db4
        deployment.microfocus.com/default-replica-count: "1"
        deployment.microfocus.com/runlevel: UP
    spec:
      securityContext:
        runAsUser: 1999
        runAsGroup: 1999
        fsGroup: 1999

      serviceAccount: itomdipulsar-sa
      serviceAccountName: itomdipulsar-sa
      
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                    - key: "app"
                      operator: In
                      values:
                      - "itomdipulsar"
                    - key: "release"
                      operator: In
                      values:
                      - RELEASE-NAME
                    - key: "component"
                      operator: In
                      values:
                      - bookkeeper
                topologyKey: "kubernetes.io/hostname"
          
      terminationGracePeriodSeconds: 30
      initContainers:
      - name: waitfor-itom-vault-8200
        image: atf.intranet.bb.com.br:5001/hpeswitom/opensuse-base:15.3-0032
        imagePullPolicy: IfNotPresent
        command: [ "sh", "-c", "until nc -z itom-vault 8200 -w 5 ; do echo waiting for itom-vault:8200...; sleep 5; done; exit 0"]
        resources: {}
        securityContext:
          runAsNonRoot: true
          runAsUser: 1999
          runAsGroup: 1999
      # This init container will generate certificates.
      - name: generate-certificates
        image: atf.intranet.bb.com.br:5001/hpeswitom/kubernetes-vault-init:0.15.0-0038
        # command: ["sh", "-c"]
        # args:
        # - >
        #   sleep 5
        imagePullPolicy: IfNotPresent
        env:
          - name: CERT_COMMON_NAME
            value: "Realm:RID,Common_Name:,Additional_SAN:itomdipulsar-bookkeeper;Realm:RE,Common_Name:,Additional_SAN:itomdipulsar-bookkeeper/itomdipulsar-bookkeeper.default/itomdipulsar-bookkeeper.default.svc"
        volumeMounts:
        - name: vault-token
          mountPath: /var/run/secrets/boostport.com
      # This initContainer will wait for bookkeeper initnewcluster to complete
      # before deploying the bookies
      - name: pulsar-bookkeeper-verify-clusterid

        image: atf.intranet.bb.com.br:5001/hpeswitom/itom-pulsar-core:2.8.1-26
        imagePullPolicy: IfNotPresent
        command: ["sh", "-c"]
        args:
        # only reformat bookie if bookkeeper is running without persistence
        - >
          source bin/coso-init.sh;
          
          set -e;
          bin/apply-config-from-env.py conf/bookkeeper.conf;
          until bin/bookkeeper shell whatisinstanceid; do
            sleep 3;
          done;
        envFrom:
        - configMapRef:
            name: "itomdipulsar-bookkeeper"
        volumeMounts:
        - name: tmp
          mountPath: /pulsar/tmp
        - name: conf
          mountPath: /pulsar/conf
        - name: logs
          mountPath: /pulsar/logs
        - name: ssl
          mountPath: /pulsar/ssl
        - name: vault-token
          mountPath: /var/run/secrets/boostport.com
        - name: cosoinit
          mountPath: "/pulsar/bin/coso-init.sh"
          subPath: coso-init.sh
        
        - name: keytool
          mountPath: "/pulsar/keytool/keytool.sh"
          subPath: keytool.sh
      containers:
      - name: certificate-renew
        image: atf.intranet.bb.com.br:5001/hpeswitom/kubernetes-vault-renew:0.15.0-0038
        imagePullPolicy: IfNotPresent
        volumeMounts:
        - name: vault-token
          mountPath: /var/run/secrets/boostport.com
      - name: "itomdipulsar-bookkeeper"
        image: atf.intranet.bb.com.br:5001/hpeswitom/itom-pulsar-core:2.8.1-26
        imagePullPolicy: IfNotPresent
        resources:
          limits:
            cpu: 4
            memory: 4Gi
          requests:
            cpu: 0.5
            memory: 2Gi
        command: ["bash", "-c"]
        args:
        - >
          source bin/coso-init.sh;
          bin/apply-config-from-env.py conf/bookkeeper.conf;
          bin/pulsar bookie;
        ports:
        - name: bookie
          containerPort: 3181
        - name: http
          containerPort: 8000
        envFrom:
        - configMapRef:
            name: "itomdipulsar-bookkeeper"
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: dbStorage_rocksDB_logPath
          value: /pulsar/logs
        - name: VOLUME_NAME
          value: "itomdipulsar-bookkeeper-journal"
        - name: BOOKIE_PORT
          value: "3181"
        - name: BOOKIE_RACK_AWARE_ENABLED
          value: "true"
        volumeMounts:
        - name: tmp
          mountPath: /pulsar/tmp
        - name: conf
          mountPath: /pulsar/conf
        - name: logs
          mountPath: /pulsar/logs
        - name: ssl
          mountPath: /pulsar/ssl
        - name: vault-token
          mountPath: /var/run/secrets/boostport.com
        - name: cosoinit
          mountPath: "/pulsar/bin/coso-init.sh"
          subPath: coso-init.sh
        - name: "itomdipulsar-bookkeeper-journal"
          mountPath: /pulsar/data/bookkeeper/journal
        - name: "itomdipulsar-bookkeeper-ledgers"
          mountPath: /pulsar/data/bookkeeper/ledgers
        
        - name: keytool
          mountPath: "/pulsar/keytool/keytool.sh"
          subPath: keytool.sh
        - name: "itomdipulsar-bookkeeper-log4j2"
          mountPath: "/pulsar/conf/log4j2.yaml"
          subPath: log4j2.yaml
      volumes:
      - name: tmp
        emptyDir: {}
      - name: conf
        emptyDir: {}
      - name: ssl
        emptyDir: {}
      - name: logs
        emptyDir: {}
      - name: vault-token
        emptyDir: {}
      - name: cosoinit
        configMap:
          name: "itomdipulsar-cosoinit-configmap"
          defaultMode: 0755
      
      - name: keytool
        configMap:
          name: "itomdipulsar-keytool-configmap"
          defaultMode: 0755
      - name: "itomdipulsar-bookkeeper-log4j2"
        configMap:
          name: "itomdipulsar-bookkeeper"
  volumeClaimTemplates:
  - metadata:
      name: "itomdipulsar-bookkeeper-journal"
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 19Gi
      storageClassName: fast-disks
  - metadata:
      name: "itomdipulsar-bookkeeper-ledgers"
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 140Gi
      storageClassName: fast-disks
---
# Source: nomultimate/charts/itomdipulsar/templates/zookeeper/zookeeper-statefulset.yaml
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

# deploy zookeeper only when `components.zookeeper` is true
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: "itomdipulsar-zookeeper"
  namespace: default
  labels:
    app: itomdipulsar
    chart: itomdipulsar-2.8.1-31
    release: RELEASE-NAME
    heritage: Helm
    cluster: itomdipulsar
    app.kubernetes.io/name: itomdipulsar-zookeeper
    app.kubernetes.io/managed-by: RELEASE-NAME
    app.kubernetes.io/version: 2.8.1-31
    itom.microfocus.com/capability: itom-data-ingestion
    tier.itom.microfocus.com/backend: backend
    component: zookeeper
  annotations:
    deployment.microfocus.com/default-replica-count: "1"
    deployment.microfocus.com/runlevel: UP
spec:
  serviceName: "itomdipulsar-zookeeper"
  replicas: 3
  selector:
    matchLabels:
      app: itomdipulsar
      release: RELEASE-NAME
      component: zookeeper
  updateStrategy:
    type: RollingUpdate
  podManagementPolicy: OrderedReady
  template:
    metadata:
      labels:
        app: itomdipulsar
        chart: itomdipulsar-2.8.1-31
        release: RELEASE-NAME
        heritage: Helm
        cluster: itomdipulsar
        app.kubernetes.io/name: itomdipulsar-zookeeper
        app.kubernetes.io/managed-by: RELEASE-NAME
        app.kubernetes.io/version: 2.8.1-31
        itom.microfocus.com/capability: itom-data-ingestion
        tier.itom.microfocus.com/backend: backend
        component: zookeeper
      annotations:
        pod.boostport.com/vault-approle: default-default
        checksum/config: 5adbf850d50000f1ee72e0a81677bc3acdf684fd828be5acf5ff1ebe441881ff
        deployment.microfocus.com/default-replica-count: "1"
        deployment.microfocus.com/runlevel: UP
        pod.boostport.com/vault-init-container: generate-certificates
        prometheus.io/port: "8000"
        prometheus.io/scrape: "true"
    spec:
      securityContext:
        runAsUser: 1999
        runAsGroup: 1999
        fsGroup: 1999

      serviceAccount: itomdipulsar-sa
      serviceAccountName: itomdipulsar-sa
      
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                    - key: "app"
                      operator: In
                      values:
                      - "itomdipulsar"
                    - key: "release"
                      operator: In
                      values:
                      - RELEASE-NAME
                    - key: "component"
                      operator: In
                      values:
                      - zookeeper
                topologyKey: "kubernetes.io/hostname"
          
      terminationGracePeriodSeconds: 30
      initContainers:
      - name: waitfor-itom-vault-8200
        image: atf.intranet.bb.com.br:5001/hpeswitom/opensuse-base:15.3-0032
        imagePullPolicy: IfNotPresent
        command: [ "sh", "-c", "until nc -z itom-vault 8200 -w 5 ; do echo waiting for itom-vault:8200...; sleep 5; done; exit 0"]
        resources: {}
        securityContext:
          runAsNonRoot: true
          runAsUser: 1999
          runAsGroup: 1999
      # This init container will generate certificates.
      - name: generate-certificates
        image: atf.intranet.bb.com.br:5001/hpeswitom/kubernetes-vault-init:0.15.0-0038
        imagePullPolicy: IfNotPresent
        env:
          - name: CERT_COMMON_NAME
            value: "Realm:RID,Common_Name:,Additional_SAN:itomdipulsar-zookeeper;Realm:RE,Common_Name:,Additional_SAN:itomdipulsar-zookeeper/itomdipulsar-zookeeper.default/itomdipulsar-zookeeper.default.svc"
        volumeMounts:
        - name: vault-token
          mountPath: /var/run/secrets/boostport.com
      containers:
      - name: certificate-renew
        image: atf.intranet.bb.com.br:5001/hpeswitom/kubernetes-vault-renew:0.15.0-0038
        imagePullPolicy: IfNotPresent
        volumeMounts:
        - name: vault-token
          mountPath: /var/run/secrets/boostport.com
      - name: "itomdipulsar-zookeeper"
        image: atf.intranet.bb.com.br:5001/hpeswitom/itom-pulsar-core:2.8.1-26
        imagePullPolicy: IfNotPresent
        workingDir: /pulsar
        resources:
          limits:
            cpu: 2
            memory: 4Gi
          requests:
            cpu: 0.5
            memory: 512Mi
        command: ["sh", "-c"]
        args:
        - >
          source bin/coso-init.sh false;
          bin/apply-config-from-env.py conf/zookeeper.conf;
          bin/apply-config-from-env.py conf/pulsar_env.sh;
          bin/generate-zookeeper-config.sh conf/zookeeper.conf;
          bin/pulsar zookeeper
        ports:
        - name: metrics
          containerPort: 8000
        - name: client
          containerPort: 2181
        - name: follower
          containerPort: 2888
        - name: leader-election
          containerPort: 3888
        - name: client-tls
          containerPort: 2281
        env:
        - name: ZOOKEEPER_SERVERS
          value:
            itomdipulsar-zookeeper-0,itomdipulsar-zookeeper-1,itomdipulsar-zookeeper-2
        envFrom:
        - configMapRef:
            name: "itomdipulsar-zookeeper"
        volumeMounts:
        - name: tmp
          mountPath: /pulsar/tmp
        - name: conf
          mountPath: /pulsar/conf
        - name: logs
          mountPath: /pulsar/logs
        - name: ssl
          mountPath: /pulsar/ssl
        - name: vault-token
          mountPath: /var/run/secrets/boostport.com
        - name: cosoinit
          mountPath: "/pulsar/bin/coso-init.sh"
          subPath: coso-init.sh
        
        - name: "itomdipulsar-zookeeper-zookeeper-data"
          mountPath: "/pulsar/data"
        
        - name: keytool
          mountPath: "/pulsar/keytool/keytool.sh"
          subPath: keytool.sh
        - name: "itomdipulsar-zookeeper-log4j2"
          mountPath: "/pulsar/conf/log4j2.yaml"
          subPath: log4j2.yaml
        - name: "itomdipulsar-zookeeper-genzkconf"
          mountPath: "/pulsar/bin/gen-zk-conf.sh"
          subPath: gen-zk-conf.sh
      volumes:
      - name: tmp
        emptyDir: {}
      - name: conf
        emptyDir: {}
      - name: ssl
        emptyDir: {}
      - name: logs
        emptyDir: {}
      - name: vault-token
        emptyDir: {}
      - name: cosoinit
        configMap:
          name: "itomdipulsar-cosoinit-configmap"
          defaultMode: 0755
      
      
      - name: keytool
        configMap:
          name: "itomdipulsar-keytool-configmap"
          defaultMode: 0755
      - name: "itomdipulsar-zookeeper-log4j2"
        configMap:
          name: "itomdipulsar-zookeeper"
      - name: "itomdipulsar-zookeeper-genzkconf"
        configMap:
          name: "itomdipulsar-genzkconf-configmap"
          defaultMode: 0755
  volumeClaimTemplates:
  - metadata:
      name: "itomdipulsar-zookeeper-zookeeper-data"
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 19Gi
      storageClassName: fast-disks
---
# Source: nomultimate/charts/itomnomcosodataaccess/templates/itom-coso-dl-data-access.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: itom-coso-dl-data-access
  labels:
    name: itom-coso-dl-data-access
  annotations:
    deployment.microfocus.com/default-replica-count: "1"
    deployment.microfocus.com/runlevel: UP
spec:
  selector:
    matchLabels:
      name: itom-coso-dl-data-access
  serviceName: itom-coso-dl-data-access
  template:
    metadata:
      labels:
        name: itom-coso-dl-data-access
      annotations:
        pod.boostport.com/vault-approle: default-default
        pod.boostport.com/vault-init-container: install
    spec:
      serviceAccountName: itom-nom-coso-dl
      nodeSelector:
        Worker: label

      securityContext:
        runAsNonRoot: true
        runAsUser: 1999
        runAsGroup: 1999
        fsGroup: 1999
      terminationGracePeriodSeconds: 30
      initContainers:
        - name: waitfor-itom-vault-8200
          image: atf.intranet.bb.com.br:5001/hpeswitom/opensuse-base:15.3-0032
          imagePullPolicy: IfNotPresent
          command: [ "sh", "-c", "until nc -z itom-vault 8200 -w 5 ; do echo waiting for itom-vault:8200...; sleep 5; done; exit 0"]
          resources: {}
          securityContext:
            runAsNonRoot: true
            runAsUser: 1999
            runAsGroup: 1999
        - name: install
          image: atf.intranet.bb.com.br:5001/hpeswitom/kubernetes-vault-init:0.15.0-0038
          env:
            - name: CERT_COMMON_NAME
              value: itom-coso-dl-data-access
          volumeMounts:
            - mountPath: /var/run/secrets/boostport.com
              name: vault-token
      containers:
        
        
        - name: itom-coso-dl-data-access
          image: atf.intranet.bb.com.br:5001/hpeswitom/itom-nom-coso-data-access:1.4.46
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 8443
            - containerPort: 8080
          livenessProbe:
            httpGet:
              scheme: HTTPS
              path: /health/
              port: 8443
            initialDelaySeconds: 20
            timeoutSeconds: 10
            periodSeconds: 60
            successThreshold: 1
            failureThreshold: 5
          readinessProbe:
            httpGet:
              scheme: HTTPS
              path: /health/
              port: 8443
            initialDelaySeconds: 5
            timeoutSeconds: 15
            periodSeconds: 5
            failureThreshold: 2
          resources:
            limits:
              cpu: "2"
              memory: "1024Mi"
            requests:
              cpu: "100m"
              memory: "512Mi"
          securityContext:
            readOnlyRootFilesystem: true
            capabilities:
              drop: ["CHOWN", "DAC_OVERRIDE", "FSETID", "FOWNER", "MKNOD", "SETGID", "SETUID", "SETFCAP", "SETPCAP", "NET_BIND_SERVICE", "SYS_CHROOT", "KILL", "AUDIT_WRITE","NET_RAW"]
          env:
            - name: SERVICE_ID
              value: "itom-coso-dl-data-access"
            - name: SERVICE_NAME
              value: "ITOM COSO DL Data Access"
            - name: ZK_HOSTNAME
              value: "nomzk-client-svc.default"
            - name: ZK_PORT
              value: "2181"
            - name: ZK_NAMESPACE
              value: "nom"
            - name: NOM_ZK_ADMIN_PASSWORD_KEY
              value: NOM_ZK_ADMIN_PASSWORD_VAULT_KEY
            - name: APLS_HOST
              value: "itom-autopass-lms"
            - name: APLS_PORT
              value: "80"
            # This product id changes only when there is change in license pd file
            - name: NOM_PRODUCT_ID
              value: 50039_2.0_NOM_2020.11
            - name: LICENSE_CHECK_INTERVAL
              value: "15"
            - name: METRIC_SCHEMA
              value: "mf_shared_provider_default"
            - name: COSO_DATABASE_SCHEMA
              value: "mf_shared_provider_default"
            # Vertica Database Specific Environment Variables
            - name: COSO_DATABASE_HOST
              value: "pxl0nnmi0022.disposistivos.bb.com.br"
            - name: COSO_DATABASE_PORT
              value: "5443"
            - name: COSO_DATABASE_SSL
              value: "false"
            - name: COSO_DATABASE_DB
              value: "verticadb"
            - name: COSO_DATABASE_USER
              value: "vertica_rouser"
            - name: COSO_DATABASE_PASSWORD_KEY
              value: "ITOMDI_ROUSER_PASSWORD_KEY"
            - name: COSO_DATABASE_RW_USER
              value: "vertica_rwuser"
            - name: COSO_DATABASE_RW_PASSWORD_KEY
              value: "ITOMDI_DBA_PASSWORD_KEY"
            - name: COSO_DB_METADATA_SCHEMA
              value: "itom_di_metadata_provider_default"
            - name: COSO_DB_CONFIG_SCHEMA
              value: "itom_di_configuration_provider_default"
            - name: COSO_DATABASE_CONNECTION_MIN_IDLE
              value: "10"
            - name: COSO_DATABASE_CONNECTION_MAX_IDLE
              value: "10"
            - name: COSO_DATABASE_CONNECTION_TOTAL
              value: "20"
            - name: COSO_DATABASE_RW_CONNECTION_MIN_IDLE
              value: "6"
            - name: COSO_DATABASE_RW_CONNECTION_MAX_IDLE
              value: "6"
            - name: COSO_DATABASE_RW_CONNECTION_TOTAL
              value: "12"
            - name: COSO_DATABASE_CONNECTION_TIMEOUT_SEC
              value: "20"
            - name: ITOM_CDF_EXTERNAL_SSL_CERTS_DIR
              value: /var/opt/OV/certs
            - name: LOG_PROPERTIES_LISTENER_ENABLED
              value: "true"
            - name: COSO_MINIO_HOST
              value: 
            - name: COSO_MINIO_PORT
              value: "30006"
            - name: COSO_MINIO_ACCESS_KEY
              valueFrom:
                configMapKeyRef:
                  name: nom-ultimate-cm
                  key: minio.accessKey
            - name: COSO_MINIO_SECRET_KEY
              valueFrom:
                configMapKeyRef:
                  name: nom-ultimate-cm
                  key: minio.secretKey
            - name: JVM_HEAP_MIN
              value: 512m
            - name: JVM_HEAP_MAX
              value: 2048m
            - name: IDM_SVC_SERVICE_HOST
              value: itom-idm-svc.default
            - name: IDM_SVC_SERVICE_PORT
              value: "80"
            - name: NOM_IDM_ADMIN_USER
              value: "nomadmin"
            - name: NOM_ADMIN_PASSWORD_KEY
              value: "idm_nom_admin_password"
          volumeMounts:
          - name: itom-coso-dl-data-access-log-vol
            mountPath: /var/opt/OV/log
            subPath: itom-coso-dl-data-access
          - name: itom-coso-dl-data-access-data-vol
            mountPath: /var/opt/OV
            subPath: itom-coso-dl-data-access
          - name: certs-volume
            mountPath: /var/opt/OV/certs
          - name: vault-token
            mountPath: /var/run/secrets/boostport.com
          - name: temp
            mountPath: /tmp
        - name: kubernetes-vault-renew
          image: atf.intranet.bb.com.br:5001/hpeswitom/kubernetes-vault-renew:0.15.0-0038
          imagePullPolicy: IfNotPresent
          volumeMounts:
            - name: vault-token
              mountPath: /var/run/secrets/boostport.com
      restartPolicy: Always
      volumes:
      - name: itom-coso-dl-data-access-data-vol
        persistentVolumeClaim:
          claimName: data-idm
      - name: itom-coso-dl-data-access-log-vol
        persistentVolumeClaim:
          claimName: log-idm
      - name: certs-volume
        configMap:
          name: default-ca-certificates
      - name: vault-token
        emptyDir: {}
      - name: temp
        emptyDir: {}
---
# Source: nomultimate/charts/nommetricstransform/templates/itom-nom-metric-transformation.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
 # Statefulset name Pod name would be of $StatefulsetName-$ordinal
  name: itom-nom-metric-transformation
  labels:
    name: itom-nom-metric-transformation
  annotations:
    deployment.microfocus.com/default-replica-count: "1"
    deployment.microfocus.com/runlevel: UP
spec:
  #name of the service that governs this Statefuset.This service must exist before the Statefulset and is responsible for the
  #network identity of the set
  selector:
    matchLabels:
      name: itom-nom-metric-transformation
  serviceName: itom-nom-metric-transformation
  replicas: 3
  template:
    metadata:
      labels:
        name: itom-nom-metric-transformation
      annotations:
        pod.boostport.com/vault-approle: default-default
        pod.boostport.com/vault-init-container: install
    spec:
      serviceAccountName: itom-nom-metrics
      nodeSelector:
        Worker: label
      securityContext:
        runAsNonRoot: true
        runAsUser: 1999
        runAsGroup: 1999
        fsGroup: 1999
      terminationGracePeriodSeconds: 30
      initContainers:
      - name: waitfor-itom-vault-8200
        image: atf.intranet.bb.com.br:5001/hpeswitom/opensuse-base:15.3-0032
        imagePullPolicy: IfNotPresent
        command: [ "sh", "-c", "until nc -z itom-vault 8200 -w 5 ; do echo waiting for itom-vault:8200...; sleep 5; done; exit 0"]
        resources: {}
        securityContext:
          runAsNonRoot: true
          runAsUser: 1999
          runAsGroup: 1999
      - name: waitfor-itomdipulsar-proxy-6651
        image: atf.intranet.bb.com.br:5001/hpeswitom/opensuse-base:15.3-0032
        imagePullPolicy: IfNotPresent
        command: [ "sh", "-c", "until nc -z itomdipulsar-proxy 6651 -w 5 ; do echo waiting for itomdipulsar-proxy:6651...; sleep 5; done; exit 0"]
        resources: {}
        securityContext:
          runAsNonRoot: true
          runAsUser: 1999
          runAsGroup: 1999
      - name: install
        image: atf.intranet.bb.com.br:5001/hpeswitom/kubernetes-vault-init:0.15.0-0038
        env:
        - name: CERT_COMMON_NAME
          value: Realm:RE,Common_Name:itom-nom-metric-transformation,Additional_SAN:/itom-nom-metric-transformation/itom-nom-metric-transformation.default
        volumeMounts:
        - mountPath: /var/run/secrets/boostport.com
          name: vault-token
      containers:
      - name: itom-nom-metric-transformation
        image: atf.intranet.bb.com.br:5001/hpeswitom/itom-nom-metric-transformation:1.4.90
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 8443
        - containerPort: 8080
        - containerPort: 8686
        livenessProbe:
          httpGet:
            scheme: HTTPS
            path: /nom/api/metrics/v1/probe/
            port: 8443
          initialDelaySeconds: 90
          timeoutSeconds: 10
        readinessProbe:
          httpGet:
            path: /nom/api/metrics/v1/probe/
            port: 8443
            scheme: HTTPS
          initialDelaySeconds: 30
          timeoutSeconds: 10
        resources:
            limits:
              cpu: "4.0"
              memory: 4000Mi
            requests:
              cpu: "1.0"
              memory: 1000Mi
        securityContext:
          readOnlyRootFilesystem: true
          capabilities:
            drop: ["CHOWN", "DAC_OVERRIDE", "FSETID", "FOWNER", "MKNOD", "SETGID", "SETUID", "SETFCAP", "SETPCAP", "NET_BIND_SERVICE", "SYS_CHROOT", "KILL", "AUDIT_WRITE","NET_RAW"]
        env:
          - name: SERVICE_ID
            value: "itom-nom-metric-transformer"
          - name: SERVICE_NAME
            value: "ITOM NOM Metric Transformer"
          - name: ZK_HOSTNAME
            value: "nomzk-client-svc.default"
          - name: ZK_PORT
            value: "2181"
          - name: ZK_NAMESPACE
            value: "nom"
          - name: NOM_ZK_ADMIN_PASSWORD_KEY
            value: NOM_ZK_ADMIN_PASSWORD_VAULT_KEY
          - name: LOG_PROPERTIES_LISTENER_ENABLED
            value: "true"
          - name: PULSAR_PROXY_SVC_NAME
            value: "itomdipulsar-proxy"
          - name: PULSAR_PROXY_CLIENT_PORT
            value: "6651"
          - name: PULSAR_PROXY_WEB_PORT
            value: "8443"
          - name: PULSAR_TOPIC_NAMESPACE
            value: "public"
          - name: PULSAR_TOPIC_TENANT
            value: "default"
          - name: TOPIC_PARTITION_COUNT
            value: "3"
          - name: CONSUMER_SUBSCRIPTION_TYPE
            value: "Failover"
          - name: BLOCK_IF_QUEUE_FULL
            value: "true"
          - name: COSO_TRANSFORMATION_CONCURRENCY
            value: "10"
          - name: COSO_PULSAR_ACK_TIMEOUT
            value: "40"
          - name: COSO_TRANSFORMATION_QUEUE_LIMIT
            value: "15"
          - name: COSO_CONNECTION_INTERVAL
            value: "300000"
          - name: COSO_CONSUMER_SUBSCRIPTION_NAME
            value: "transformation"
          - name: COSO_PRODUCER_REPROCESSING_ENABLE
            value: "true"
          - name: MAX_MESSAGE_COUNT
            value: "15000"
          - name: VIOLATION_FREE_WINDOW_TO_IGNORE_PROCESSING
            value: "20"
          - name: TRANSFORMATION_DB_MAX_READ_BATCH_SIZE
            value: "2000"
          - name: TRANSFORMATION_DB_MAX_READ_PARALLEL_THREADS
            value: "10"
          - name: TRANSFORMATION_DB_MAX_WRITE_BATCH_SIZE
            value: "10000"
          - name: TRANSFORMATION_DB_MAX_WRITE_PARALLEL_THREADS
            value: "10"
          - name: TRANSFORMATION_DB_CACHE_STATE_DELETE_INTERVAL_MIN
            value: "360"
          - name: TRANSFORMATION_DB_MIN_PERSIST_DELAY_INTERVAL_MIN
            value: "30"
          - name: TRANSFORMATION_DB_PERSIST_INTERLEAVE_INTERVALS_ACROSS_PODS_MIN
            value: "10"
          - name: TRANSFORMATION_DB_PERSIST_CYCLE_INTERVAL
            value: "30"
          - name: TRANSFORMATION_DB_CACHE_QUERY_HISTORY_LIMIT_MIN
            value: "120"
          - name: TRANSFORMATION_DB_CACHE_MAX_CONNECTIONS
            value: "20"
          - name: TRANSFORMATION_IN_MEMORY_INIT_CACHE_SIZE
            value: "10000000"
          - name: TRANSFORMATION_CACHE_SWEEP_ENABLED
            value: "true"
          - name: RE_FAILED_WRITER_FILE_MESSAGE_COUNT
            value: "1000"
          - name: RE_FAILED_WRITER_NEW_FILE_DIFF_INTERVAL_SEC
            value: "10"
          - name: RE_PUBLISH_FILE_MESSAGE_COUNT
            value: "1000"
          - name: RE_PUBLISH_FILE_PAUSE_INTERVAL_MSEC
            value: "1000"
          - name: RE_PUBLISH_MAX_FILE_COMPRESS_DELAY_MSEC
            value: "250"
          - name: RE_PUBLISH_MAX_FILES_TO_PROCESS
            value: "10000"
          - name: CACHE_DB_HOST
            value: "silo12-master.postgresql.bdh.servicos.bb.com.br"
          - name: CACHE_DB_PORT
            value: "5432"
          - name: CACHE_DB_DATABASE
            value: "btcd"
          - name: CACHE_DB_USER
            value: "user_btcd"
          - name: CACHE_DB_PASSWORD_KEY
            value: "BTCD_DB_PASSWD_KEY"
          - name: CACHE_DB_IS_SSL
            value: "false"
          - name: CACHE_DB_SSL_MODE
            value: "verify-ca"
          - name: JVM_HEAP_MIN
            value: "512m"
          - name: JVM_HEAP_MAX
            value: "3072m"
          - name: PARALLEL_GC_THREADS
            value: "4"
          - name: CONCURRENT_GC_THREADS
            value: "2"
          - name: MAX_GC_PAUSE_MSEC
            value: "500"
          - name: REPROCESS_INTERVAL
            value: "300000"
          - name: REPROCESS_DELAY
            value: "300000"
          - name: MCAST_ENABLED
            value: "false"
          - name: QA_ENABLED
            value: "true"
          - name: REINIT_TRANFORMATION_CONFIG_RULES
            value: "true"
          - name: ITOM_CDF_EXTERNAL_SSL_CERTS_DIR
            value: /var/opt/OV/certs
        volumeMounts:
        - name: metric-transformation-log-vol
          mountPath: /var/opt/OV/log
          subPath: metric-transformation
        - name: metric-transformation-data-vol
          mountPath: /var/opt/OV
          subPath: metric-transformation
        - name: vault-token
          mountPath: /var/run/secrets/boostport.com
        - name: temp
          mountPath: /tmp
        - name: certs-volume
          mountPath: /var/opt/OV/certs
      - env:
        - name: CERT_FILE_BASE_NAME
          value: RE/server
        - name: PROXY_LISTENING_PORT
          value: "8787"
        - name: PROXY_TARGET_PORT
          value: "8686"
        image: atf.intranet.bb.com.br:5001/hpeswitom/itom-stunnel:11.8.0-0022
        imagePullPolicy: IfNotPresent
        name: stunnel
        resources:
          limits:
            cpu: 100m
            memory: 200Mi
          requests:
            cpu: 1m
            memory: 50Mi
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - name: vault-token
          mountPath: /var/run/secrets/boostport.com
      - name: kubernetes-vault-renew
        image: atf.intranet.bb.com.br:5001/hpeswitom/kubernetes-vault-renew:0.15.0-0038
        imagePullPolicy: IfNotPresent
        volumeMounts:
        - name: vault-token
          mountPath: /var/run/secrets/boostport.com
      restartPolicy: Always
      volumes:
      - name: metric-transformation-data-vol
        persistentVolumeClaim:
          claimName: data-idm
      - name: metric-transformation-log-vol
        persistentVolumeClaim:
          claimName: log-idm
      - name: certs-volume
        configMap:
          name: default-ca-certificates
      - name: vault-token
        emptyDir: {}
      - name: temp
        emptyDir: {}
---
# Source: nomultimate/charts/nomzk/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app.kubernetes.io/name: itom-nom-zookeeper
    app.kubernetes.io/version: 3.6.3.125
  name: itom-nom-zookeeper
  annotations:
    deployment.microfocus.com/default-replica-count: "1"
    deployment.microfocus.com/runlevel: UP
spec:
  serviceName: nomzk
  replicas: 1
  selector:
    matchLabels:
      name: itom-nom-zookeeper
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/name: itom-nom-zookeeper
        app.kubernetes.io/version: 3.6.3.125
        name: itom-nom-zookeeper
    spec:
      serviceAccountName: itom-nom-zookeeper
      terminationGracePeriodSeconds: 30
      containers:
      - name: itom-zookeeper
        imagePullPolicy: IfNotPresent
        image: atf.intranet.bb.com.br:5001/hpeswitom/itom-zookeeper:3.6.3.125
        resources:
          limits:
            cpu: "0.5"
            memory: 512Mi
          requests:
            cpu: 100m
            memory: 256Mi
        ports:
        - containerPort: 2181
          name: client
        - containerPort: 2888
          name: server
        - containerPort: 3888
          name: leader-election
        readinessProbe:
          exec:
            command:
            - sh
            - -c
            - 'CMD_OUT=`echo "ruok" | (exec 3<>/dev/tcp/127.0.0.1/2181; cat >&3; cat <&3; exec 3<&-)`; if [ "$CMD_OUT" != "imok" ]; then exit 1; fi'
          initialDelaySeconds: 10
          timeoutSeconds: 5
        livenessProbe:
          exec:
            command:
            - sh
            - -c
            - 'CMD_OUT=`echo "ruok" | (exec 3<>/dev/tcp/127.0.0.1/2181; cat >&3; cat <&3; exec 3<&-)`; if [ "$CMD_OUT" != "imok" ]; then exit 1; fi'
          initialDelaySeconds: 10
          timeoutSeconds: 5
        env:
        - name: uid
          value: "1999"
        - name: guid
          value: "1999"
        - name : ZK_REPLICAS
          value: "1"
        - name : ZK_MIN_HEAP_SIZE
          valueFrom:
            configMapKeyRef:
                name: nomzk-cm
                key: jvm.min.heap
        - name : ZK_MAX_HEAP_SIZE
          valueFrom:
            configMapKeyRef:
                name: nomzk-cm
                key: jvm.max.heap
        - name : ZK_TICK_TIME
          valueFrom:
            configMapKeyRef:
                name: nomzk-cm
                key: tick
        - name : ZK_INIT_LIMIT
          valueFrom:
            configMapKeyRef:
                name: nomzk-cm
                key: init
        - name : ZK_SYNC_LIMIT
          valueFrom:
            configMapKeyRef:
                name: nomzk-cm
                key: tick
        - name : ZK_MAX_CLIENT_CNXNS
          valueFrom:
            configMapKeyRef:
                name: nomzk-cm
                key: client.cnxns
        - name: ZK_SNAP_RETAIN_COUNT
          valueFrom:
            configMapKeyRef:
                name: nomzk-cm
                key: snap.retain
        - name: ZK_PURGE_INTERVAL
          valueFrom:
            configMapKeyRef:
                name: nomzk-cm
                key: purge.interval
        - name: ZK_CLIENT_PORT
          value: "2181"
        - name: ZK_SERVER_PORT
          value: "2888"
        - name: ZK_ELECTION_PORT
          value: "3888"
        - name: ZK_USER_GROUP
          value: "nom"
        - name: ZK_LOG_TO_FILE
          value: "true"
        - name: ZK_LOG_LEVEL
          value: "INFO"
        - name: ZK_SNAPSHOT_TRUST_EMPTY
          value: "true"  
        - name: SERVICENAME
          value: RELEASE-NAME-zk-svc
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: POD_NAME
          value: itom-nom-zookeeper
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: CONTAINER_NAME
          value: itom-nom-zookeeper
        volumeMounts:
        - name: nom-zk-volume
          mountPath: /opt/zookeeper/data
          subPath: nom-zk/data
        - name: nom-zk-volume
          mountPath: /opt/zookeeper/datalog
          subPath: nom-zk/datalog
        - name: nom-zk-log-vol
          mountPath: /opt/zookeeper/log
          subPath: nom-zk/log
      volumes:
      - name: nom-zk-volume
        persistentVolumeClaim:
          claimName: data-idm
      - name: nom-zk-log-vol
        persistentVolumeClaim:
          claimName: log-idm
      nodeSelector:
        Worker: label
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: name
                  operator: In
                  values:
                  - zookeeper
              topologyKey: kubernetes.io/hostname
            weight: 100
---
# Source: nomultimate/charts/itomdiminio/templates/post-install-create-client-user-job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: "itom-di-minio-make-client-user-job-krtrz1q"
  
spec:
  template:
    metadata:
      name: itom-di-minio-job
      annotations:
        pod.boostport.com/vault-approle: default-default
        pod.boostport.com/vault-init-container: install
    spec:
      securityContext:
        runAsUser: 1999
        runAsGroup: 1999
        fsGroup: 1999
      containers:
        - name: minio
          image: atf.intranet.bb.com.br:5001/hpeswitom/itom-data-ingestion-minio:2.5.0-21
          imagePullPolicy: IfNotPresent
          env:
            - name: MINIO_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: itom-di-minio-secret
                  key: accesskey
            - name: MINIO_SECRET_KEY
              valueFrom:
                secretKeyRef:
                  name: itom-di-minio-secret
                  key: secretkey
            - name: MINIO_CLIENT_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: nom-secret
                  key: ITOMDI_MINIO_CLIENT_ACCESS_KEY

            - name: MINIO_CLIENT_SECRET_KEY
              valueFrom:
                secretKeyRef:
                  name: nom-secret
                  key: ITOMDI_MINIO_CLIENT_SECRET_KEY
          command: ['sh', '-c', "mkdir -p /minio/certs/CAs && ln -s /var/run/secrets/boostport.com/issue_ca.crt /minio/certs/CAs/public.crt &&  ./mc -C /minio alias set clientuser https://:30006   $MINIO_ACCESS_KEY  $MINIO_SECRET_KEY  --api S3v4 && (until ./mc -C /minio admin info clientuser |grep -ow '0 drives offline' ; do echo waiting for minio service ; sleep 2; done ;) && ./mc -C /minio admin user add clientuser  $MINIO_CLIENT_ACCESS_KEY  $MINIO_CLIENT_SECRET_KEY  && ./mc -C /minio admin  policy add  clientuser clientuserpolicy /minio/client-policy.json && ./mc -C /minio admin policy set clientuser clientuserpolicy user=$MINIO_CLIENT_ACCESS_KEY " ]
          
          volumeMounts:
            - name: vault-token
              mountPath: /var/run/secrets/boostport.com    
      serviceAccount: itom-di-minio-sa
      serviceAccountName: itom-di-minio-sa    
      initContainers:
      - name: waitfor-itom-vault-8200
        image: atf.intranet.bb.com.br:5001/hpeswitom/opensuse-base:15.3-0032
        imagePullPolicy: IfNotPresent
        command: [ "sh", "-c", "until nc -z itom-vault 8200 -w 5 ; do echo waiting for itom-vault:8200...; sleep 5; done; exit 0"]
        resources: {}
        securityContext:
          runAsNonRoot: true
          runAsUser: 1999
          runAsGroup: 1999
      - env:
          - name: VAULT_ROLE_ID
            value: 
          - name: CERT_COMMON_NAME
            value: "Realm:RE,Common_Name:itom-di-minio-hook;Realm:RID,Common_Name:itom-di-minio-hook"
        image: atf.intranet.bb.com.br:5001/hpeswitom/kubernetes-vault-init:0.15.0-0038
        imagePullPolicy: IfNotPresent
        name: install
        resources: {}
        securityContext:
          runAsUser: 1999          
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
          - mountPath: /var/run/secrets/boostport.com
            name: vault-token
      restartPolicy: Never
      terminationGracePeriodSeconds: 0
      volumes:
        - name: vault-token
          emptyDir: {}
---
# Source: nomultimate/charts/itomdimonitoring/templates/gen-certs-cronjob.yaml
# Kick off job for initial run (CronJob does not run a job immediately)
apiVersion: batch/v1
kind: Job
metadata:
  name: itomdimonitoring-gen-certs-job
  labels:
    app.kubernetes.io/name: "itomdimonitoring-gen-certs"
    app: itomdimonitoring
    app.kubernetes.io/managed-by: RELEASE-NAME
    app.kubernetes.io/version: 2.5.0-61
    itom.microfocus.com/capability: itom-data-ingestion
    tier.itom.microfocus.com/backend: backend
spec:
  ttlSecondsAfterFinished: 1200
  template:
    metadata:
      annotations:
        pod.boostport.com/vault-approle: default-default
        pod.boostport.com/vault-init-container: generate-certificates
      labels:
        app.kubernetes.io/name: "itomdimonitoring-gen-certs"
        app: itomdimonitoring
        app.kubernetes.io/managed-by: RELEASE-NAME
        app.kubernetes.io/version: 2.5.0-61
        itom.microfocus.com/capability: itom-data-ingestion
        tier.itom.microfocus.com/backend: backend
    spec:
      restartPolicy: OnFailure
      serviceAccount: itomdimonitoring-gen-certs-sa
      serviceAccountName: itomdimonitoring-gen-certs-sa
      securityContext:
        runAsNonRoot: true
        runAsUser: 1999
        runAsGroup: 1999
      initContainers:
      - name: generate-certificates
        image: atf.intranet.bb.com.br:5001/hpeswitom/kubernetes-vault-init:0.15.0-0038
        imagePullPolicy: IfNotPresent
        env:
          - name: CERT_COMMON_NAME
            value: "itomdimonitoring-gen-certs"
        volumeMounts:
        - name: vault-token
          mountPath: /var/run/secrets/boostport.com
      containers:
      - name: itomdimonitoring-gen-certs
        image: atf.intranet.bb.com.br:5001/hpeswitom/itom-data-ingestion-monitoring-gen-certs:2.5.0-61
        imagePullPolicy: IfNotPresent
        env:
        - name: TLS_CERT_FILE_PATH
          value: "/var/run/secrets/boostport.com/server.crt"
        - name: TLS_KEY_FILE_PATH
          value: "/var/run/secrets/boostport.com/server.key"
        - name: NAMESPACE
          value: default
        - name: SECRET_NAME
          value: itom-di-prometheus-scrape-cert
        volumeMounts:
        - name: vault-token
          mountPath: /var/run/secrets/boostport.com
      volumes:
      - name: vault-token
        emptyDir: {}
---
# Source: nomultimate/charts/itomdipulsar/templates/bookkeeper/bookkeeper-cluster-initialize.yaml
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

apiVersion: batch/v1
kind: Job
metadata:
  name: "itomdipulsar-bookkeeper-init-lguaszz"
  namespace: default
  labels:
    app: itomdipulsar
    chart: itomdipulsar-2.8.1-31
    release: RELEASE-NAME
    heritage: Helm
    cluster: itomdipulsar
    app.kubernetes.io/name: itomdipulsar-bookkeeper
    app.kubernetes.io/managed-by: RELEASE-NAME
    app.kubernetes.io/version: 2.8.1-31
    itom.microfocus.com/capability: itom-data-ingestion
    tier.itom.microfocus.com/backend: backend
    component: bookkeeper-init
spec:
  template:
    metadata:
      annotations:
        pod.boostport.com/vault-init-container: generate-certificates
        pod.boostport.com/vault-approle: default-default
    spec:
      securityContext:
        runAsUser: 1999
        runAsGroup: 1999
        fsGroup: 1999
      serviceAccount: "itomdipulsar-sa"
      serviceAccountName: "itomdipulsar-sa"
      initContainers:
      - name: waitfor-itom-vault-8200
        image: atf.intranet.bb.com.br:5001/hpeswitom/opensuse-base:15.3-0032
        imagePullPolicy: IfNotPresent
        command: [ "sh", "-c", "until nc -z itom-vault 8200 -w 5 ; do echo waiting for itom-vault:8200...; sleep 5; done; exit 0"]
        resources: {}
        securityContext:
          runAsNonRoot: true
          runAsUser: 1999
          runAsGroup: 1999
      - name: generate-certificates
        image: atf.intranet.bb.com.br:5001/hpeswitom/kubernetes-vault-init:0.15.0-0038
        imagePullPolicy: IfNotPresent
        env:
          - name: CERT_COMMON_NAME
            value: "itomdipulsar-zookeeper"
        volumeMounts:
        - name: vault-token
          mountPath: /var/run/secrets/boostport.com
      - name: wait-zookeeper-ready
        image: atf.intranet.bb.com.br:5001/hpeswitom/itom-pulsar-core:2.8.1-26
        imagePullPolicy: IfNotPresent
        command: ["sh", "-c"]
        args:
          - >-
            source bin/coso-init.sh;
            until bin/pulsar zookeeper-shell -server itomdipulsar-zookeeper:2281 ls /; do
              sleep 3;
            done;
        volumeMounts:
        - name: cosoinit
          mountPath: "/pulsar/bin/coso-init.sh"
          subPath: coso-init.sh
        
        - name: keytool
          mountPath: "/pulsar/keytool/keytool.sh"
          subPath: keytool.sh
        - name: tmp
          mountPath: /pulsar/tmp
        - name: conf
          mountPath: /pulsar/conf
        - name: logs
          mountPath: /pulsar/logs
        - name: ssl
          mountPath: /pulsar/ssl
        - name: vault-token
          mountPath: /var/run/secrets/boostport.com
      containers:     
      - name: "itomdipulsar-bookkeeper-init"
        image: atf.intranet.bb.com.br:5001/hpeswitom/itom-pulsar-core:2.8.1-26
        imagePullPolicy: IfNotPresent
        command: ["sh", "-c"]
        args:
          - >
            source bin/coso-init.sh;
            bin/apply-config-from-env.py conf/bookkeeper.conf;
            if bin/bookkeeper shell whatisinstanceid; then
                echo "bookkeeper cluster already initialized";
            else
                bin/bookkeeper shell initnewcluster;
            fi
        envFrom:
        - configMapRef:
            name: "itomdipulsar-bookkeeper"
        volumeMounts:
        - name: cosoinit
          mountPath: "/pulsar/bin/coso-init.sh"
          subPath: coso-init.sh
        - name: tmp
          mountPath: /pulsar/tmp
        - name: conf
          mountPath: /pulsar/conf
        - name: logs
          mountPath: /pulsar/logs
        - name: ssl
          mountPath: /pulsar/ssl
        - name: vault-token
          mountPath: /var/run/secrets/boostport.com
        
        - name: keytool
          mountPath: "/pulsar/keytool/keytool.sh"
          subPath: keytool.sh
      volumes:
      - name: cosoinit
        configMap:
          name: "itomdipulsar-cosoinit-configmap"
          defaultMode: 0755
      - name: tmp
        emptyDir: {}
      - name: conf
        emptyDir: {}
      - name: ssl
        emptyDir: {}
      - name: logs
        emptyDir: {}
      - name: vault-token
        emptyDir: {}
      
      - name: keytool
        configMap:
          name: "itomdipulsar-keytool-configmap"
          defaultMode: 0755
      restartPolicy: Never
---
# Source: nomultimate/charts/itomdipulsar/templates/broker/post-upgrade-job-minio-connector.yaml
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

apiVersion: batch/v1
kind: Job
metadata:
  name: "itomdipulsar-minio-connector-post-upgrade-job-cdbamch"
  namespace: default
  labels:
    app: itomdipulsar
    chart: itomdipulsar-2.8.1-31
    release: RELEASE-NAME
    heritage: Helm
    cluster: itomdipulsar
    app.kubernetes.io/name: itomdipulsar-minio-connector-upgrade
    app.kubernetes.io/managed-by: RELEASE-NAME
    app.kubernetes.io/version: 2.8.1-31
    itom.microfocus.com/capability: itom-data-ingestion
    tier.itom.microfocus.com/backend: backend
    component: minio-connector-upgrade
spec:
  template:
    metadata:
      annotations:
        pod.boostport.com/vault-init-container: generate-certificates
        pod.boostport.com/vault-approle: default-default
    spec:
      securityContext:
        runAsUser: 1999
        runAsGroup: 1999
        fsGroup: 1999
      # The init containers will follow the queue from the zookeeper for the node to run on.
      serviceAccount: itomdipulsar-broker-sa
      serviceAccountName: itomdipulsar-broker-sa
      initContainers:
      - name: waitfor-itom-vault-8200
        image: atf.intranet.bb.com.br:5001/hpeswitom/opensuse-base:15.3-0032
        imagePullPolicy: IfNotPresent
        command: [ "sh", "-c", "until nc -z itom-vault 8200 -w 5 ; do echo waiting for itom-vault:8200...; sleep 5; done; exit 0"]
        resources: {}
        securityContext:
          runAsNonRoot: true
          runAsUser: 1999
          runAsGroup: 1999
      - name: generate-certificates
        image: atf.intranet.bb.com.br:5001/hpeswitom/kubernetes-vault-init:0.15.0-0038
        imagePullPolicy: IfNotPresent
        env:
          - name: CERT_COMMON_NAME
            value: "itomdipulsar-zookeeper"
        volumeMounts:
        - name: vault-token
          mountPath: /var/run/secrets/boostport.com

      # This initContainer will wait for bookkeeper initnewcluster to complete
      # before initializing pulsar metadata
      - name: pulsar-bookkeeper-verify-clusterid
        image: atf.intranet.bb.com.br:5001/hpeswitom/itom-pulsar-core:2.8.1-26
        imagePullPolicy: IfNotPresent
        command: ["sh", "-c"]
        args:
        - |
          source bin/coso-init.sh
          bin/apply-config-from-env.py conf/bookkeeper.conf
          until bin/bookkeeper shell whatisinstanceid
          do
            sleep 3
          done
        envFrom:
        - configMapRef:
            name: "itomdipulsar-bookkeeper"
        volumeMounts:
        - name: cosoinit
          mountPath: "/pulsar/bin/coso-init.sh"
          subPath: coso-init.sh
        
        - name: keytool
          mountPath: "/pulsar/keytool/keytool.sh"
          subPath: keytool.sh
        - name: tmp
          mountPath: /pulsar/tmp
        - name: conf
          mountPath: /pulsar/conf
        - name: logs
          mountPath: /pulsar/logs
        - name: ssl
          mountPath: /pulsar/ssl
        - name: vault-token
          mountPath: /var/run/secrets/boostport.com
      containers:
      - name: "itomdipulsar-minio-connector-upgrade"
        image: atf.intranet.bb.com.br:5001/hpeswitom/itom-pulsar-core:2.8.1-26
        imagePullPolicy: IfNotPresent
        command: ["sh", "-c"]
        args:
          - |
            source bin/coso-init.sh
            bin/apply-config-from-env.py conf/client.conf
            bin/apply-config-from-env.py conf/bookkeeper.conf
            TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)
            di_tenant="provider"
            di_deployment="default"
            pulsar_tenant=public
            updatedReplicas=0
            availableReplicas=0
            replicas=3
            echo "number of replicas $replicas"
            namespace=default
            echo "namespace is $namespace"
            # Avoid startup race condition with broker upgrade
            sleep 5
            until [[  $updatedReplicas  ==  $replicas  && $availableReplicas == $replicas ]]
            do
              curl -X GET --cacert /var/run/secrets/kubernetes.io/serviceaccount/ca.crt \
                   -H "Authorization: Bearer $TOKEN"  -H 'Accept: application/json' \
                   https://kubernetes.default.svc.cluster.local/apis/apps/v1/namespaces/$namespace/deployments/itomdipulsar-broker > /tmp/broker_stats.json 2>/dev/null

              updatedReplicas=$(cat /tmp/broker_stats.json| jq .status.updatedReplicas)
              if [ "$updatedReplicas" == "null" ]
              then
                updatedReplicas=0
              fi
              availableReplicas=$(cat /tmp/broker_stats.json| jq .status.availableReplicas)
              if [ "$availableReplicas" == "null" ]
              then
                availableReplicas=0
              fi

              if [[ $updatedReplicas == $replicas  &&  $availableReplicas == $replicas ]]
              then
                echo " broker deployment upgraded"
              else
                echo " Waiting for Broker deployment to be  upgraded: ${updatedReplicas}/${replicas} available: ${availableReplicas}/${replicas}"
                sleep 30
              fi
            done
            if [[ $updatedReplicas ==  $replicas  &&  $availableReplicas == $replicas ]]
            then
              bin/pulsar-admin sources get  --name ${di_tenant} --tenant ${pulsar_tenant} >/tmp/sources.json
              if [ $? == 0 ]
              then
                replicas=1
                until [ "$replicas" -eq 0 ]
                do
                    bin/pulsar-admin sources stop --name ${di_tenant} --tenant ${pulsar_tenant} --namespace ${di_deployment}
                    echo "Waiting for statefulset(pf-${pulsar_tenant}-${di_deployment}-${di_tenant}) to be removed. replicas: $replicas"
                    sleep 30
                    curl -X GET --cacert /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\
                         -H "Authorization: Bearer $TOKEN"  -H 'Accept: application/json' \
                         https://kubernetes.default.svc.cluster.local/apis/apps/v1/namespaces/$namespace/statefulsets/pf-${pulsar_tenant}-${di_deployment}-${di_tenant} > /tmp/sts.json 2>/dev/null
                    notFound=$(cat /tmp/sts.json|jq .code)
                    echo "statefulset statuscode $notFound"
                    if [ $notFound != "404" ] 
                    then
                        replicas=$(cat /tmp/sts.json|jq .status.replicas)
                        echo "statefulset replica $replicas"
                       if [ "${replicas}" == "[]" ] 
                       then
                         replicas=0
                       fi
                    else
                       replicas=0
                   fi
                   # because of inconsistent behavior in pulsar client , below condition has been placed.For the very first time pulsar admin sources stop command is not working.
                   if [ $replicas != "0" ]
                   then
                     echo "Waiting for statefulset(pf-${pulsar_tenant}-${di_deployment}-${di_tenant}) to be removed and trying to start again. replicas: $replicas"
                     bin/pulsar-admin sources start --name ${di_tenant} --tenant ${pulsar_tenant} --namespace ${di_deployment}
                   fi  
                done
                bin/pulsar-admin sources start --name ${di_tenant} --tenant ${pulsar_tenant} --namespace ${di_deployment} 
              else
                echo "No sources found with name ${di_tenant}"
              fi
            fi
        envFrom:
        - configMapRef:
            name: "itomdipulsar-broker-post-upgrade"
        volumeMounts:
        - name: cosoinit
          mountPath: "/pulsar/bin/coso-init.sh"
          subPath: coso-init.sh
        - name: tmp
          mountPath: /pulsar/tmp
        - name: conf
          mountPath: /pulsar/conf
        - name: logs
          mountPath: /pulsar/logs
        - name: ssl
          mountPath: /pulsar/ssl
        - name: vault-token
          mountPath: /var/run/secrets/boostport.com
        
        - name: keytool
          mountPath: "/pulsar/keytool/keytool.sh"
          subPath: keytool.sh
      volumes:
      - name: cosoinit
        configMap:
          name: "itomdipulsar-cosoinit-configmap"
          defaultMode: 0755
      - name: tmp
        emptyDir: {}
      - name: conf
        emptyDir: {}
      - name: ssl
        emptyDir: {}
      - name: logs
        emptyDir: {}
      - name: vault-token
        emptyDir: {}
      
      - name: keytool
        configMap:
          name: "itomdipulsar-keytool-configmap"
          defaultMode: 0755
      restartPolicy: Never
---
# Source: nomultimate/charts/itomdipulsar/templates/pulsar-cluster-initialize.yaml
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

apiVersion: batch/v1
kind: Job
metadata:
  name: "itomdipulsar-zookeeper-metadata-ddhvefz"
  namespace: default
  labels:
    app: itomdipulsar
    chart: itomdipulsar-2.8.1-31
    release: RELEASE-NAME
    heritage: Helm
    cluster: itomdipulsar
    app.kubernetes.io/name: itomdipulsar-zookeeper-metadata
    app.kubernetes.io/managed-by: RELEASE-NAME
    app.kubernetes.io/version: 2.8.1-31
    itom.microfocus.com/capability: itom-data-ingestion
    tier.itom.microfocus.com/backend: backend
    component: zookeeper-metadata
spec:
  template:
    metadata:
      annotations:
        pod.boostport.com/vault-init-container: generate-certificates
        pod.boostport.com/vault-approle: default-default
    spec:
      securityContext:
        runAsUser: 1999
        runAsGroup: 1999
        fsGroup: 1999
      # The init containers will follow the queue from the zookeeper for the node to run on.  
      serviceAccount: itomdipulsar-sa
      serviceAccountName: itomdipulsar-sa   

      initContainers:
      - name: waitfor-itom-vault-8200
        image: atf.intranet.bb.com.br:5001/hpeswitom/opensuse-base:15.3-0032
        imagePullPolicy: IfNotPresent
        command: [ "sh", "-c", "until nc -z itom-vault 8200 -w 5 ; do echo waiting for itom-vault:8200...; sleep 5; done; exit 0"]
        resources: {}
        securityContext:
          runAsNonRoot: true
          runAsUser: 1999
          runAsGroup: 1999
      - name: generate-certificates
        image: atf.intranet.bb.com.br:5001/hpeswitom/kubernetes-vault-init:0.15.0-0038
        imagePullPolicy: IfNotPresent
        env:
          - name: CERT_COMMON_NAME
            value: "itomdipulsar-zookeeper"
        volumeMounts:
        - name: vault-token
          mountPath: /var/run/secrets/boostport.com
      - name: wait-zookeeper-ready
        image: atf.intranet.bb.com.br:5001/hpeswitom/itom-pulsar-core:2.8.1-26
        imagePullPolicy: IfNotPresent
        command: ["sh", "-c"]
        args:
          - >-
            source bin/coso-init.sh;
            until bin/pulsar zookeeper-shell -server itomdipulsar-zookeeper:2281 ls /; do
              sleep 3;
            done;
        volumeMounts:
        - name: cosoinit
          mountPath: "/pulsar/bin/coso-init.sh"
          subPath: coso-init.sh
        
        - name: keytool
          mountPath: "/pulsar/keytool/keytool.sh"
          subPath: keytool.sh
        - name: tmp
          mountPath: /pulsar/tmp
        - name: conf
          mountPath: /pulsar/conf
        - name: logs
          mountPath: /pulsar/logs
        - name: ssl
          mountPath: /pulsar/ssl
        - name: vault-token
          mountPath: /var/run/secrets/boostport.com
      # This initContainer will wait for bookkeeper initnewcluster to complete
      # before initializing pulsar metadata
      - name: pulsar-bookkeeper-verify-clusterid
        image: atf.intranet.bb.com.br:5001/hpeswitom/itom-pulsar-core:2.8.1-26
        imagePullPolicy: IfNotPresent
        command: ["sh", "-c"]
        args:
        - >
          source bin/coso-init.sh;
          bin/apply-config-from-env.py conf/bookkeeper.conf;
          until bin/bookkeeper shell whatisinstanceid; do
            sleep 3;
          done;
        envFrom:
        - configMapRef:
            name: "itomdipulsar-bookkeeper"
        volumeMounts:
        - name: cosoinit
          mountPath: "/pulsar/bin/coso-init.sh"
          subPath: coso-init.sh
        
        - name: keytool
          mountPath: "/pulsar/keytool/keytool.sh"
          subPath: keytool.sh
        - name: tmp
          mountPath: /pulsar/tmp
        - name: conf
          mountPath: /pulsar/conf
        - name: logs
          mountPath: /pulsar/logs
        - name: ssl
          mountPath: /pulsar/ssl
        - name: vault-token
          mountPath: /var/run/secrets/boostport.com
      containers:     
      - name: "itomdipulsar-zookeeper-metadata"
        image: atf.intranet.bb.com.br:5001/hpeswitom/itom-pulsar-core:2.8.1-26
        imagePullPolicy: IfNotPresent
        command: ["sh", "-c"]
        args:
          - >
            source bin/coso-init.sh;
            bin/pulsar initialize-cluster-metadata \
              --cluster itomdipulsar \
              --zookeeper itomdipulsar-zookeeper:2281 \
              --configuration-store itomdipulsar-zookeeper:2281 \
              --web-service-url http://itomdipulsar-broker.default.svc.cluster.local:8080/ \
              --web-service-url-tls https://itomdipulsar-broker.default.svc.cluster.local:8443/ \
              --broker-service-url pulsar://itomdipulsar-broker.default.svc.cluster.local:6650/ \
              --broker-service-url-tls pulsar+ssl://itomdipulsar-broker.default.svc.cluster.local:6651/ || true;
        volumeMounts:
        - name: cosoinit
          mountPath: "/pulsar/bin/coso-init.sh"
          subPath: coso-init.sh
        - name: tmp
          mountPath: /pulsar/tmp
        - name: conf
          mountPath: /pulsar/conf
        - name: logs
          mountPath: /pulsar/logs
        - name: ssl
          mountPath: /pulsar/ssl
        - name: vault-token
          mountPath: /var/run/secrets/boostport.com
        
        - name: keytool
          mountPath: "/pulsar/keytool/keytool.sh"
          subPath: keytool.sh
      volumes:
      - name: cosoinit
        configMap:
          name: "itomdipulsar-cosoinit-configmap"
          defaultMode: 0755
      - name: tmp
        emptyDir: {}
      - name: conf
        emptyDir: {}
      - name: ssl
        emptyDir: {}
      - name: logs
        emptyDir: {}
      - name: vault-token
        emptyDir: {}
      
      - name: keytool
        configMap:
          name: "itomdipulsar-keytool-configmap"
          defaultMode: 0755
      restartPolicy: Never
---
# Source: nomultimate/charts/nomcore/templates/apply-license-job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: apply-license-job-6lfhoew
spec:
  ttlSecondsAfterFinished: 1.2096e+06
  template:
    metadata:
      labels:
        name: apply-license-job
      annotations:
        pod.boostport.com/vault-approle: default-default
        pod.boostport.com/vault-init-container: install
    spec:
      securityContext:
        runAsNonRoot: true
        runAsUser: 1999
        runAsGroup: 1999
        fsGroup: 1999
      serviceAccountName: nom-core
      restartPolicy: OnFailure
      initContainers:
      - name: waitfor-itom-vault-8200
        image: atf.intranet.bb.com.br:5001/hpeswitom/opensuse-base:15.3-0032
        imagePullPolicy: IfNotPresent
        command: [ "sh", "-c", "until nc -z itom-vault 8200 -w 5 ; do echo waiting for itom-vault:8200...; sleep 5; done; exit 0"]
        resources: {}
        securityContext:
          runAsNonRoot: true
          runAsUser: 1999
          runAsGroup: 1999

      
      
      - name: waitfor-itom-autopass-lms-80
        image: atf.intranet.bb.com.br:5001/hpeswitom/opensuse-base:15.3-0032
        imagePullPolicy: IfNotPresent
        command: [ "sh", "-c", "until nc -z itom-autopass-lms 80 -w 5 ; do echo waiting for itom-autopass-lms:80...; sleep 5; done; exit 0"]
        resources: {}
        securityContext:
          runAsNonRoot: true
          runAsUser: 1999
          runAsGroup: 1999
      - name: waitfor-itom-idm-svc-80
        image: atf.intranet.bb.com.br:5001/hpeswitom/opensuse-base:15.3-0032
        imagePullPolicy: IfNotPresent
        command: [ "sh", "-c", "until nc -z itom-idm-svc 80 -w 5 ; do echo waiting for itom-idm-svc:80...; sleep 5; done; exit 0"]
        resources: {}
        securityContext:
          runAsNonRoot: true
          runAsUser: 1999
          runAsGroup: 1999
      

      - name: install
        image: atf.intranet.bb.com.br:5001/hpeswitom/kubernetes-vault-init:0.15.0-0038
        env:
        volumeMounts:
        - mountPath: /var/run/secrets/boostport.com
          name: vault-token

      containers:
      - name: apply-license
        image: atf.intranet.bb.com.br:5001/hpeswitom/itom-nom-config:2.2.73
        args: ["/tasks/license"]
        env:
        - name: APLS_SVC_NAME
          value: "itom-autopass-lms"
        - name: APLS_SVC_PORT
          value: "80"
        - name: PRODUCT_ID
          value: "50039_2.0_NOM_2020.11"
        - name: IDM_HOST
          value: "itom-idm-svc"
        - name: IDM_PORT
          value: "80"
        - name: IDM_USER
          value: "nomadmin"
        - name: IDM_PASSWORD_KEY
          value: "idm_nom_admin_password"
        imagePullPolicy: IfNotPresent
        volumeMounts:
        - mountPath: /var/run/secrets/boostport.com
          name: vault-token
        - name: certs-volume
          mountPath: /data/nom/certificates

      volumes:
      - name: vault-token
        emptyDir: {}
      - name: certs-volume
        configMap:
          name: default-ca-certificates
---
# Source: nomultimate/charts/nomreportingcontent/templates/create-coso-objects-job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: create-coso-objects-job-u6djzvg
spec:
  ttlSecondsAfterFinished: 1.2096e+06
  template:
    metadata:
      labels:
        name: create-coso-objects-job
      annotations:
        pod.boostport.com/vault-approle: default-default
        pod.boostport.com/vault-init-container: install
    spec:
      securityContext:
        runAsNonRoot: true
        runAsUser: 1999
        runAsGroup: 1999
        fsGroup: 1999
      serviceAccountName: itom-nom-reporting
      restartPolicy: OnFailure
      initContainers:
        - name: waitfor-itom-vault-8200
          image: atf.intranet.bb.com.br:5001/hpeswitom/opensuse-base:15.3-0032
          imagePullPolicy: IfNotPresent
          command: [ "sh", "-c", "until nc -z itom-vault 8200 -w 5 ; do echo waiting for itom-vault:8200...; sleep 5; done; exit 0"]
          resources: {}
          securityContext:
            runAsNonRoot: true
            runAsUser: 1999
            runAsGroup: 1999
        - name: waitfor-itom-di-administrati-18443
          image: atf.intranet.bb.com.br:5001/hpeswitom/opensuse-base:15.3-0032
          imagePullPolicy: IfNotPresent
          command: [ "sh", "-c", "until nc -z itom-di-administration-svc 18443 -w 5 ; do echo waiting for itom-di-administration-svc:18443...; sleep 5; done; exit 0"]
          resources: {}
          securityContext:
            runAsNonRoot: true
            runAsUser: 1999
            runAsGroup: 1999
        - name: waitfor-itom-nom-api-server-8443
          image: atf.intranet.bb.com.br:5001/hpeswitom/opensuse-base:15.3-0032
          imagePullPolicy: IfNotPresent
          command: [ "sh", "-c", "until nc -z itom-nom-api-server 8443 -w 5 ; do echo waiting for itom-nom-api-server:8443...; sleep 5; done; exit 0"]
          resources: {}
          securityContext:
            runAsNonRoot: true
            runAsUser: 1999
            runAsGroup: 1999

        
        

        - name: install
          image: atf.intranet.bb.com.br:5001/hpeswitom/kubernetes-vault-init:0.15.0-0038
          env:
          volumeMounts:
            - mountPath: /var/run/secrets/boostport.com
              name: vault-token
 
        - name: create-coso-objects-prereqs
          image: atf.intranet.bb.com.br:5001/hpeswitom/itom-nom-coso-config:1.4.135
          args: ["/tasks/coso"]
          env:
            - name: DI_ADMIN_SVC_HOST
              value: "itom-nom-api-server"
            - name: DI_ADMIN_SVC_PORT
              value: "8443"
            - name: IDP_HOST
              value: "itom-idm-svc"
            - name: IDP_PORT
              value: "80"
            - name: IDP_USER
              value: "nomadmin"
            - name: IDP_PASSWORD_KEY
              value: "idm_nom_admin_password"
            - name: LOG_FILE_NAME
              value: "log/create-coso-objects-prereqs-%u.%g.log"
            - name: LOG_LEVEL
              value: "INFO"
            - name: LOG_FILE_MAX_SIZE
              value: "0"
            - name: LOG_ROLLUP_COUNT
              value: "1"
          imagePullPolicy: IfNotPresent
          volumeMounts:
            - mountPath: /var/run/secrets/boostport.com
              name: vault-token
            - mountPath: /data/nom/certificates
              name: certs-volume
            - mountPath: /data
              name: data-volume
            - mountPath: /log
              name: log-volume
              subPath: create-coso-objects
        - name: create-coso-objects-nnm
          image: atf.intranet.bb.com.br:5001/hpeswitom/itom-nom-coso-config:1.4.135
          args: ["/tasks/coso/nnm"]
          env:
            - name: DI_ADMIN_SVC_HOST
              value: "itom-nom-api-server"
            - name: DI_ADMIN_SVC_PORT
              value: "8443"
            - name: IDP_HOST
              value: "itom-idm-svc"
            - name: IDP_PORT
              value: "80"
            - name: IDP_USER
              value: "nomadmin"
            - name: IDP_PASSWORD_KEY
              value: "idm_nom_admin_password"
            - name: KAFKA_SVC_NAME
              value: "itomdipulsar-proxy"
            - name: KAFKA_SVC_PORT
              value: "6651"
            - name: KAFKA_REPLICAS
              value: "0"
            - name: LOG_FILE_NAME
              value: "log/create-coso-objects-nnm-%u.%g.log"
            - name: LOG_LEVEL
              value: "INFO"
            - name: LOG_FILE_MAX_SIZE
              value: "0"
            - name: LOG_ROLLUP_COUNT
              value: "1"
          imagePullPolicy: IfNotPresent
          volumeMounts:
            - mountPath: /var/run/secrets/boostport.com
              name: vault-token
            - mountPath: /data/nom/certificates
              name: certs-volume
            - mountPath: /data
              name: data-volume
            - mountPath: /log
              name: log-volume
              subPath: create-coso-objects
        - name: create-coso-objects-na
          image: atf.intranet.bb.com.br:5001/hpeswitom/itom-nom-coso-config:1.4.135
          args: ["/tasks/coso/na"]
          env:
            - name: DI_ADMIN_SVC_HOST
              value: "itom-nom-api-server"
            - name: DI_ADMIN_SVC_PORT
              value: "8443"
            - name: IDP_HOST
              value: "itom-idm-svc"
            - name: IDP_PORT
              value: "80"
            - name: IDP_USER
              value: "nomadmin"
            - name: IDP_PASSWORD_KEY
              value: "idm_nom_admin_password"
            - name: LOG_FILE_NAME
              value: "log/create-coso-objects-na-%u.%g.log"
            - name: LOG_LEVEL
              value: "INFO"
            - name: LOG_FILE_MAX_SIZE
              value: "0"
            - name: LOG_ROLLUP_COUNT
              value: "1"
          imagePullPolicy: IfNotPresent
          volumeMounts:
            - mountPath: /var/run/secrets/boostport.com
              name: vault-token
            - mountPath: /data/nom/certificates
              name: certs-volume
            - mountPath: /data
              name: data-volume
            - mountPath: /log
              name: log-volume
              subPath: create-coso-objects
        - name: create-coso-objects-traffic
          image: atf.intranet.bb.com.br:5001/hpeswitom/itom-nom-coso-config:1.4.135
          args: ["/tasks/coso/traffic"]
          env:
            - name: DI_ADMIN_SVC_HOST
              value: "itom-nom-api-server"
            - name: DI_ADMIN_SVC_PORT
              value: "8443"
            - name: IDP_HOST
              value: "itom-idm-svc"
            - name: IDP_PORT
              value: "80"
            - name: IDP_USER
              value: "nomadmin"
            - name: IDP_PASSWORD_KEY
              value: "idm_nom_admin_password"
            - name: LOG_FILE_NAME
              value: "log/create-coso-objects-traffic-%u.%g.log"
            - name: LOG_LEVEL
              value: "INFO"
            - name: LOG_FILE_MAX_SIZE
              value: "0"
            - name: LOG_ROLLUP_COUNT
              value: "1"
          imagePullPolicy: IfNotPresent
          volumeMounts:
            - mountPath: /var/run/secrets/boostport.com
              name: vault-token
            - mountPath: /data
              name: data-volume
            - mountPath: /log
              name: log-volume
              subPath: create-coso-objects
        - name: create-coso-objects-qa
          image: atf.intranet.bb.com.br:5001/hpeswitom/itom-nom-coso-config:1.4.135
          args: ["/tasks/coso/qa"]
          env:
            - name: DI_ADMIN_SVC_HOST
              value: "itom-nom-api-server"
            - name: DI_ADMIN_SVC_PORT
              value: "8443"
            - name: IDP_HOST
              value: "itom-idm-svc"
            - name: IDP_PORT
              value: "80"
            - name: IDP_USER
              value: "nomadmin"
            - name: IDP_PASSWORD_KEY
              value: "idm_nom_admin_password"
            - name: LOG_FILE_NAME
              value: "log/create-coso-objects-qa-%u.%g.log"
            - name: LOG_LEVEL
              value: "INFO"
            - name: LOG_FILE_MAX_SIZE
              value: "0"
            - name: LOG_ROLLUP_COUNT
              value: "1"
          imagePullPolicy: IfNotPresent
          volumeMounts:
            - mountPath: /var/run/secrets/boostport.com
              name: vault-token
            - mountPath: /data
              name: data-volume
            - mountPath: /log
              name: log-volume
              subPath: create-coso-objects

      containers:
# This is a dummy container since all others are moved to init to serially execute them
        - command:
          - sh
          - -c
          - exit 0
          image: atf.intranet.bb.com.br:5001/hpeswitom/itom-nom-coso-config:1.4.135
          imagePullPolicy: IfNotPresent
          name: dummy-container

      volumes:
      - name: vault-token
        emptyDir: {}
      - name: data-volume
        persistentVolumeClaim:
          claimName: data-idm
      - name: log-volume
        persistentVolumeClaim:
          claimName: log-idm
      - name: certs-volume
        configMap:
          name: default-ca-certificates
---
# Source: nomultimate/charts/nomreportingcontent/templates/import-bvd-dashboards.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: import-bvd-dashboards-job-ijgjgqg
spec:
  ttlSecondsAfterFinished: 1.2096e+06
  backoffLimit: 120
  template:
    metadata:
      labels:
        name: import-bvd-dashboards-job
      annotations:
        pod.boostport.com/vault-approle: default-default
        pod.boostport.com/vault-init-container: install

    spec:
      securityContext:
        runAsNonRoot: true
        runAsUser: 1999
        runAsGroup: 1999
        fsGroup: 1999
      serviceAccountName: itom-nom-reporting
      restartPolicy: OnFailure
      initContainers:
      - name: waitfor-itom-vault-8200
        image: atf.intranet.bb.com.br:5001/hpeswitom/opensuse-base:15.3-0032
        imagePullPolicy: IfNotPresent
        command: [ "sh", "-c", "until nc -z itom-vault 8200 -w 5 ; do echo waiting for itom-vault:8200...; sleep 5; done; exit 0"]
        resources: {}
        securityContext:
          runAsNonRoot: true
          runAsUser: 1999
          runAsGroup: 1999
      - name: waitfor-itom-idm-svc-80
        image: atf.intranet.bb.com.br:5001/hpeswitom/opensuse-base:15.3-0032
        imagePullPolicy: IfNotPresent
        command: [ "sh", "-c", "until nc -z itom-idm-svc 80 -w 5 ; do echo waiting for itom-idm-svc:80...; sleep 5; done; exit 0"]
        resources: {}
        securityContext:
          runAsNonRoot: true
          runAsUser: 1999
          runAsGroup: 1999

      
      
      
      - name: waitfor-bvd-www-80
        image: atf.intranet.bb.com.br:5001/hpeswitom/opensuse-base:15.3-0032
        imagePullPolicy: IfNotPresent
        command: [ "sh", "-c", "until nc -z bvd-www 80 -w 5 ; do echo waiting for bvd-www:80...; sleep 5; done; exit 0"]
        resources: {}
        securityContext:
          runAsNonRoot: true
          runAsUser: 1999
          runAsGroup: 1999
      

      - name: install
        image: atf.intranet.bb.com.br:5001/hpeswitom/kubernetes-vault-init:0.15.0-0038
        env:
        volumeMounts:
        - mountPath: /var/run/secrets/boostport.com
          name: vault-token

      containers:
      - name: copy-dashboards
        image: atf.intranet.bb.com.br:5001/hpeswitom/itom-nom-dashboard-content:02.01.116
        command: ["sh", "-c", "/scripts/copy-dashboards.sh"]
        volumeMounts:
        - name: bvd-conf-vol
          mountPath: /bvd-content
          subPath: bvd/var/bvd/dashboards

# To download CLI and import BVD dashboards, the Job uses only the external access host of the cluster where BVD is running
# For standalone NOM deployment, it will default to global.externalAccessHost/Port
# For shared optic reporting, it will use the external/internal host and port
      
      

      - name: import-legacy-bvd-dashboards
        image: atf.intranet.bb.com.br:5001/hpeswitom/itom-nom-dashboard-content:02.01.116
        command: ["sh", "-c", "/scripts/download-bvd-cli.sh; /scripts/import-bvd-dashboards.sh"]
        env:
        - name: NOM_EDITION
          value: ""
        - name: BVD_HOST
          value: 
        - name: BVD_PORT
          value: "80"
        - name: BVD_USER_NAME    #Set value to name of user account that can view BVD dashboards
          value: "integration_admin"
        - name: BVD_PASSWORD_KEY #Set value to name of Vault key containing base64-encoded password
          value: "idm_integration_admin_password"
        - name: DECODE_BVD_PASSWORD #Set value to true if the password in BVD_PASSWORD_KEY uses base64 encoding
          value: "false"
        - name: BVD_DASHBOARD_DIR
          value: "/resources/dashboards/legacy-bvd"
        volumeMounts:
        - name: certs-volume
          mountPath: /var/opt/OV/certs
          subPath: data
        - mountPath: /bvdCliCert
          name: bvdclicert-dir
      - name: import-bvd-dashboards
        image: atf.intranet.bb.com.br:5001/hpeswitom/itom-nom-dashboard-content:02.01.116
        command: ["sh", "-c", "/scripts/download-bvd-cli.sh; /scripts/import-bvd-dashboards.sh"]
        env:
        - name: NOM_EDITION
          value: "ultimate"
        - name: BVD_HOST
          value: 
        - name: BVD_PORT
          value: "80"
        - name: BVD_USER_NAME    #Set value to name of user account that can view BVD dashboards
          value: "integration_admin"
        - name: BVD_PASSWORD_KEY #Set value to name of Vault key containing base64-encoded password
          value: "idm_integration_admin_password"
        - name: DECODE_BVD_PASSWORD #Set value to true if the password in BVD_PASSWORD_KEY uses base64 encoding 
          value: "false"
        - name: BVD_DASHBOARD_DIR
          value: "/resources/dashboards/bvd"

        volumeMounts:
        - name: certs-volume
          mountPath: /var/opt/OV/certs
          subPath: data
        - mountPath: /bvdCliCert
          name: bvdclicert-dir

      volumes:
      - name: vault-token
        emptyDir: {}
      - name: bvdclicert-dir
        emptyDir: {}
      - name: certs-volume
        configMap:
          name: default-ca-certificates
      - name: bvd-conf-vol
        persistentVolumeClaim:
          claimName: config-idm
---
# Source: nomultimate/charts/nomxui/templates/itom-nom-ui-config-job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: itom-nom-ui-config-job-3diagf6
spec:
  ttlSecondsAfterFinished: 1.2096e+06
  backoffLimit: 120
  template:
    metadata:
      labels:
        name: itom-nom-ui-config-job
      annotations:
        pod.boostport.com/vault-approle: default-default
        pod.boostport.com/vault-init-container: install
    spec:
      serviceAccountName: nom-ui
      restartPolicy: OnFailure
      securityContext:
        runAsNonRoot: true
        runAsUser: 1999
        runAsGroup: 1999
        fsGroup: 1999
      initContainers:
        - name: waitfor-itom-vault-8200
          image: atf.intranet.bb.com.br:5001/hpeswitom/opensuse-base:15.3-0032
          imagePullPolicy: IfNotPresent
          command: [ "sh", "-c", "until nc -z itom-vault 8200 -w 5 ; do echo waiting for itom-vault:8200...; sleep 5; done; exit 0"]
          resources: {}
          securityContext:
            runAsNonRoot: true
            runAsUser: 1999
            runAsGroup: 1999
        
        
        - name: waitfor-bvd-explore-4000
          image: atf.intranet.bb.com.br:5001/hpeswitom/opensuse-base:15.3-0032
          imagePullPolicy: IfNotPresent
          command: [ "sh", "-c", "until nc -z bvd-explore 4000 -w 5 ; do echo waiting for bvd-explore:4000...; sleep 5; done; exit 0"]
          resources: {}
          securityContext:
            runAsNonRoot: true
            runAsUser: 1999
            runAsGroup: 1999
        
        
        
        - name: waitfor-itom-idm-svc-80
          image: atf.intranet.bb.com.br:5001/hpeswitom/opensuse-base:15.3-0032
          imagePullPolicy: IfNotPresent
          command: [ "sh", "-c", "until nc -z itom-idm-svc 80 -w 5 ; do echo waiting for itom-idm-svc:80...; sleep 5; done; exit 0"]
          resources: {}
          securityContext:
            runAsNonRoot: true
            runAsUser: 1999
            runAsGroup: 1999
        
        - name: install
          image: atf.intranet.bb.com.br:5001/hpeswitom/kubernetes-vault-init:0.15.0-0038
          env:
          volumeMounts:
            - mountPath: /var/run/secrets/boostport.com
              name: vault-token

      containers:
        - name: itom-nom-ui-config-job
          image: atf.intranet.bb.com.br:5001/hpeswitom/itom-nom-ui:3.4.77
          command: ["/bin/sh", "-c", "/opt/OV/registries/copy-config.sh"]
          env:
          - name: BVD_EXPLORE_SERVICE_HOST
            value: "bvd-explore"
          - name: BVD_EXPLORE_SERVICE_PORT
            value: "4000"
          - name: NOM_ADMIN_USER_KEY
            value: "nomadmin"
          - name: NOM_ADMIN_PASSWORD_KEY
            value: "idm_nom_admin_password"
          - name: isCoso
            value: "true"
          - name: PERF_DATASOURCE
            valueFrom:
              configMapKeyRef:
                name: nom-ultimate-cm
                key: perfTroubleshooting.datasource
          - name: isEpr
            value: "false"
          - name: IDM_HOST
            value: "itom-idm-svc"
          - name: IDM_PORT
            value: "80"
          - name: BASEDIR
            value: "/"
          imagePullPolicy: IfNotPresent
          volumeMounts:
          - name: ui-data-vol
            mountPath: /var/opt/OV
          - mountPath: /var/run/secrets/boostport.com
            name: vault-token
          - name: bvd-explore-var
            mountPath: /var/bvd
            subPath: bvd/var/bvd
          - name: certs-volume
            mountPath: /var/opt/OV/certs
      volumes:
      - name: vault-token
        emptyDir: {}
      - name: temp
        emptyDir: {}
      - name: certs-volume
        configMap:
          name: default-ca-certificates
      - name: bvd-explore-var
        persistentVolumeClaim:
          claimName: config-idm
      - name: ui-data-vol
        persistentVolumeClaim:
          claimName: data-idm
---
# Source: nomultimate/charts/itomdimonitoring/templates/gen-certs-cronjob.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: itomdimonitoring-gen-certs-cronjob
  namespace: default
  labels:
    app.kubernetes.io/name: "itomdimonitoring-gen-certs"
    app: itomdimonitoring
    app.kubernetes.io/managed-by: RELEASE-NAME
    app.kubernetes.io/version: 2.5.0-61
    itom.microfocus.com/capability: itom-data-ingestion
    tier.itom.microfocus.com/backend: backend
spec:
  schedule: "* */24 * * *"
  successfulJobsHistoryLimit: 0
  failedJobsHistoryLimit: 1
  jobTemplate:
    spec:
      ttlSecondsAfterFinished: 1200
      template:
        metadata:
          annotations:
            pod.boostport.com/vault-approle: default-default
            pod.boostport.com/vault-init-container: generate-certificates
          labels:
            app.kubernetes.io/name: "itomdimonitoring-gen-certs"
            app: itomdimonitoring
            app.kubernetes.io/managed-by: RELEASE-NAME
            app.kubernetes.io/version: 2.5.0-61
            itom.microfocus.com/capability: itom-data-ingestion
            tier.itom.microfocus.com/backend: backend
        spec:
          restartPolicy: OnFailure
          serviceAccount: itomdimonitoring-gen-certs-sa
          serviceAccountName: itomdimonitoring-gen-certs-sa
          securityContext:
            runAsNonRoot: true
            runAsUser: 1999
            runAsGroup: 1999
          initContainers:
          - name: generate-certificates
            image: atf.intranet.bb.com.br:5001/hpeswitom/kubernetes-vault-init:0.15.0-0038
            imagePullPolicy: IfNotPresent
            env:
              - name: CERT_COMMON_NAME
                value: "itomdimonitoring-gen-certs"
            volumeMounts:
            - name: vault-token
              mountPath: /var/run/secrets/boostport.com
          containers:
          - name: itomdimonitoring-gen-certs
            image: atf.intranet.bb.com.br:5001/hpeswitom/itom-data-ingestion-monitoring-gen-certs:2.5.0-61
            imagePullPolicy: IfNotPresent
            env:
            - name: TLS_CERT_FILE_PATH
              value: "/var/run/secrets/boostport.com/server.crt"
            - name: TLS_KEY_FILE_PATH
              value: "/var/run/secrets/boostport.com/server.key"
            - name: NAMESPACE
              value: default
            - name: SECRET_NAME
              value: itom-di-prometheus-scrape-cert
            volumeMounts:
            - name: vault-token
              mountPath: /var/run/secrets/boostport.com
          volumes:
          - name: vault-token
            emptyDir: {}
---
# Source: nomultimate/charts/autopass/templates/apls-ingress.yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: itom-autopass-lms
  annotations:
    kubernetes.io/ingress.class: "nginx"
    ingress.kubernetes.io/backend-protocol: "HTTPS"
    ingress.kubernetes.io/secure-backends: "true"
    ingress.kubernetes.io/session-cookie-name: "route"
    ingress.kubernetes.io/affinity: "cookie"
spec:
  rules:
  - http:
      paths:
      - path: /autopass
        backend:
          serviceName: itom-autopass-lms
          servicePort: 5814
    host: autopass.homologacao.nuvem.hm.bb.com.br
  tls:
  - hosts:
    - autopass.homologacao.nuvem.hm.bb.com.br
    secretName: "nginx-default-secret"
---
# Source: nomultimate/charts/bvd/templates/bvd-ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: bvd-ingress
  namespace: default
  labels:
    app: bvd
    app.kubernetes.io/name: bvd-ingress
    app.kubernetes.io/managed-by: bvd-config
    app.kubernetes.io/version: 11.8.11
    itom.microfocus.com/capability: bvd
    tier.itom.microfocus.com/ingress: ingress
  annotations:
    ingress.kubernetes.io/affinity: cookie
    ingress.kubernetes.io/session-cookie-name: OPSB_AFFINITY_BVD
    nginx.org/websocket-services: bvd-www
    ingress.kubernetes.io/backend-protocol: "HTTPS"
    ingress.kubernetes.io/force-ssl-redirect: "true"
    kubernetes.io/ingress.class: "nginx"
    ingress.kubernetes.io/secure-backends: "true" # old annotation for old ingress used by nom
    nginx.ingress.kubernetes.io/proxy-send-timeout: "600" # set time out to 10 minutes to allow long initial state calls
    nginx.ingress.kubernetes.io/proxy-read-timeout: "600"

spec:
  rules:
  - host: bvd.homologacao.nuvem.hm.bb.com.br
    http:
      paths:
      - path: "/bvd"
        pathType: Prefix
        backend:
          service:
            name: bvd-www
            port:
              number: 4000
      - path: /bvd-receiver
        pathType: Prefix
        backend:
          service:
            name: bvd-receiver
            port:
              number: 4000
      - path: "/dashboard"
        pathType: Prefix
        backend:
          service:
            name: bvd-explore
            port:
              number: 4000
      - path: /webtopdf
        pathType: Prefix
        backend:
          service:
            name: webtopdf
            port:
              number: 3000
---
# Source: nomultimate/charts/idm/templates/idm-ing.yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: itom-idm-admin
  namespace: default
  annotations:
    kubernetes.io/ingress.class: "nginx"
    ingress.kubernetes.io/secure-backends: "true"
    ingress.kubernetes.io/affinity: cookie
    ingress.kubernetes.io/backend-protocol: HTTPS
    ingress.kubernetes.io/session-cookie-hash: sha1
    ingress.kubernetes.io/session-cookie-name: idm-admin-route
    ingress.kubernetes.io/session-cookie-path: /idm-admin
spec:
  tls:
    - hosts:
        - idm.homologacao.nuvem.hm.bb.com.br
      secretName: "nginx-default-secret"
  rules:
  - http:
      paths:
      - path: /idm-admin
        backend:
          serviceName: itom-idm-admin
          servicePort: 18443
    host: idm.homologacao.nuvem.hm.bb.com.br
---
# Source: nomultimate/charts/idm/templates/idm-ing.yaml
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: itom-idm-svc
  namespace: default
  annotations:
    kubernetes.io/ingress.class: "nginx"
    ingress.kubernetes.io/secure-backends: "true"
    ingress.kubernetes.io/affinity: "cookie"
    ingress.kubernetes.io/session-cookie-hash: "sha1"
    ingress.kubernetes.io/backend-protocol: "HTTPS"
    ingress.kubernetes.io/session-cookie-name: idm-svc-route
    ingress.kubernetes.io/session-cookie-path: /idm-service
    ingress.kubernetes.io/session-cookie-samesite: None
spec:
  tls:
    - hosts:
        - idm.homologacao.nuvem.hm.bb.com.br
      secretName: "nginx-default-secret"
  rules:
  - http:
      paths:
      - path: /idm-service/idm
        backend:
          serviceName: itom-idm-svc
          servicePort: 18443
      - path: /idm-service/v2.0
        backend:
          serviceName: itom-idm-svc
          servicePort: 18443
      - path: /idm-service/v3.0
        backend:
          serviceName: itom-idm-svc
          servicePort: 18443
      - path: /idm-service/api
        backend:
          serviceName: itom-idm-svc
          servicePort: 18443
      - path: /idm-service/saml
        backend:
          serviceName: itom-idm-svc
          servicePort: 18443
      - path: /idm-service/oidc
        backend:
          serviceName: itom-idm-svc
          servicePort: 18443
    host: idm.homologacao.nuvem.hm.bb.com.br
  # client certificate authentication
  - host: itom-idm-svc
    http:
      paths:
      - path: /idm-service/idm
        backend:
          serviceName: itom-idm-svc
          servicePort: 18444
      - path: /idm-service/v2.0
        backend:
          serviceName: itom-idm-svc
          servicePort: 18444
      - path: /idm-service/v3.0
        backend:
          serviceName: itom-idm-svc
          servicePort: 18444
      - path: /idm-service/api
        backend:
          serviceName: itom-idm-svc
          servicePort: 18444
      - path: /idm-service/saml
        backend:
          serviceName: itom-idm-svc
          servicePort: 18444
      - path: /idm-service/oidc
        backend:
          serviceName: itom-idm-svc
          servicePort: 18444
---
# Source: nomultimate/charts/nomapiserver/templates/itom-nom-api-server-ingress.yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: nom-ingress-api-server
  annotations:
    ingress.kubernetes.io/backend-protocol: HTTPS
    ingress.kubernetes.io/force-ssl-redirect: "true"
    ingress.kubernetes.io/secure-backends: "true"
    kubernetes.io/ingress.class: "nginx"
spec:
  rules:
    - host: nom-api-server.homologacao.nuvem.hm.bb.com.br
      http:
        paths:
          - path: /nom/api 
            backend:
              serviceName: itom-nom-api-server
              servicePort: 8443
          - path: /apiserver 
            backend:
              serviceName: itom-nom-api-server
              servicePort: 8443
          - path: /urest
            backend:
              serviceName: itom-nom-api-server
              servicePort: 8443  
          - path: /nom/ui/crosslaunch
            backend:
              serviceName: itom-nom-api-server
              servicePort: 8443  
          - path: /idp/oauth2/token
            backend:
              serviceName: itom-nom-api-server
              servicePort: 8443  
          - path: /itom/nom/api
            backend:
              serviceName: itom-nom-api-server
              servicePort: 8443  
          - path: /itom/dl/data/access
            backend:
              serviceName: itom-nom-api-server
              servicePort: 8443
---
# Source: nomultimate/charts/nomhttp/templates/itom-nom-default-ingress.yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: nom-default-ingress
  annotations:
    ingress.kubernetes.io/ingress.class: "nginx"
    ingress.kubernetes.io/backend-protocol: HTTP
    kubernetes.io/ingress.class: "nginx"
spec:
  rules:
  - host: nomultimate.homologacao.nuvem.hm.bb.com.br
    http:
      paths:
      - path: /static      
        backend:
          serviceName: itom-nom-default-http-backend
          servicePort: 8080
---
# Source: nomultimate/charts/nomxui/templates/itom-nom-ui-ingress.yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: nom-ingress-ui
  annotations:
    ingress.kubernetes.io/secure-backends: "true"
    ingress.kubernetes.io/backend-protocol: "HTTPS"
    ingress.kubernetes.io/force-ssl-redirect: "true"
    kubernetes.io/ingress.class: "nginx"
spec:
  rules:
  - host:  
    http:
      paths:
      - path: /externalcomponents/nom/widgets      
        backend:
          serviceName: itom-nom-ui
          servicePort: 8443
---
# Source: nomultimate/charts/itomdimonitoring/templates/dashboards/itom-di-dashboard-dataflow.yaml
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
---
# Source: nomultimate/charts/itomdimonitoring/templates/dashboards/itom-di-dashboard-expressload.yaml
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
---
# Source: nomultimate/charts/itomdimonitoring/templates/dashboards/itom-di-dashboard-preload-postload.yaml
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
---
# Source: nomultimate/charts/itomdimonitoring/templates/dashboards/itom-di-dashboard-pulsar.yaml
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
---
# Source: nomultimate/charts/itomdimonitoring/templates/dashboards/itom-di-dashboard-receiver.yaml
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
---
# Source: nomultimate/charts/itomdimonitoring/templates/dashboards/itom-di-dashboard-vertica.yaml
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
---
# Source: nomultimate/charts/itomdipulsar/templates/bookkeeper/bookkeeper-storageclass.yaml
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
---
# Source: nomultimate/charts/itomdipulsar/templates/broker/broker-service-ingress.yaml
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
---
# Source: nomultimate/charts/itomdipulsar/templates/coso/multiaz/pulsar-multiaz-job.yaml
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
---
# Source: nomultimate/charts/itomdipulsar/templates/coso/multiaz/pulsar-multiaz-rbac.yaml
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
---
# Source: nomultimate/charts/itomdipulsar/templates/coso/multiaz/pulsar-set-bookie-rack.yaml
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
---
# Source: nomultimate/charts/itomdipulsar/templates/detector/pulsar-detector-pdb.yaml
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
---
# Source: nomultimate/charts/itomdipulsar/templates/detector/pulsar-detector-service-account.yaml
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
---
# Source: nomultimate/charts/itomdipulsar/templates/detector/pulsar-detector-service.yaml
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
---
# Source: nomultimate/charts/itomdipulsar/templates/detector/pulsar-detector-statefulset.yaml
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
---
# Source: nomultimate/charts/itomdipulsar/templates/external-dns/external-dns-rbac.yaml
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
---
# Source: nomultimate/charts/itomdipulsar/templates/external-dns/external-dns.yaml
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
---
# Source: nomultimate/charts/itomdipulsar/templates/proxy/proxy-service-ingress.yaml
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
---
# Source: nomultimate/charts/itomdipulsar/templates/tls/tls-cert-internal-issuer.yaml
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
---
# Source: nomultimate/charts/itomdipulsar/templates/tls/tls-cert-public-issuer.yaml
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
---
# Source: nomultimate/charts/itomdipulsar/templates/tls/tls-certs-internal.yaml
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
---
# Source: nomultimate/charts/itomdipulsar/templates/tls/tls-certs-public.yaml
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
---
# Source: nomultimate/charts/itomdipulsar/templates/zookeeper/zookeeper-storageclass.yaml
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

# deploy zookeeper only when `components.zookeeper` is true


# define the storage class for data directory
---
# Source: nomultimate/charts/itomdipulsar/templates/zookeeper/zookeeper-storageclass.yaml
# define the storage class for dataLog directory
---
# Source: nomultimate/charts/nommetricstransform/templates/itom-nom-metric-transformation-sm.yaml
#
#  (c) Copyright 2018-2021 Micro Focus or one of its affiliates.
#
#  The only warranties for products and services of Micro Focus and its affiliates and licensors
#  ("Micro Focus") are as may be set forth in the express warranty statements accompanying such
#  products and services. Nothing herein should be construed as constituting an additional
#  warranty. Micro Focus shall not be liable for technical or editorial errors or omissions contained
#  herein. The information contained herein is subject to change without notice.
#
#  Except as specifically indicated otherwise, this document contains confidential information
#  and a valid license is required for possession, use or copying. If this work is provided to the
#  U.S. Government, consistent with FAR 12.211 and 12.212, Commercial Computer Software,
#  Computer Software Documentation, and Technical Data for Commercial Items are licensed
#  to the U.S. Government under vendor's standard commercial license.
#
---
# Source: nomultimate/templates/nom-dashboard-jvm-cm.yaml
#
#  (c) Copyright 2018-2021 Micro Focus or one of its affiliates.
#
#  The only warranties for products and services of Micro Focus and its affiliates and licensors
#  ("Micro Focus") are as may be set forth in the express warranty statements accompanying such
#  products and services. Nothing herein should be construed as constituting an additional
#  warranty. Micro Focus shall not be liable for technical or editorial errors or omissions contained
#  herein. The information contained herein is subject to change without notice.
#
#  Except as specifically indicated otherwise, this document contains confidential information
#  and a valid license is required for possession, use or copying. If this work is provided to the
#  U.S. Government, consistent with FAR 12.211 and 12.212, Commercial Computer Software,
#  Computer Software Documentation, and Technical Data for Commercial Items are licensed
#  to the U.S. Government under vendor's standard commercial license.
#
---
# Source: nomultimate/charts/itomdipulsar/templates/broker/pre-delete-hook-minio-connector.yaml
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

apiVersion: batch/v1
kind: Job
metadata:
  name: "itomdipulsar-minio-connector-pre-delete-hook-dnczson"
  namespace: default
  labels:
    app: itomdipulsar
    chart: itomdipulsar-2.8.1-31
    release: RELEASE-NAME
    heritage: Helm
    cluster: itomdipulsar
    app.kubernetes.io/name: itomdipulsar-minio-connector-delete
    app.kubernetes.io/managed-by: RELEASE-NAME
    app.kubernetes.io/version: 2.8.1-31
    itom.microfocus.com/capability: itom-data-ingestion
    tier.itom.microfocus.com/backend: backend
    component: minio-connector-delete
  annotations:
    "helm.sh/hook": pre-delete
    "helm.sh/hook-delete-policy": before-hook-creation
spec:
  ttlSecondsAfterFinished: 60
  backoffLimit: 1
  completions: 1
  parallelism: 1
  template:
    metadata:
      annotations:
        pod.boostport.com/vault-init-container: generate-certificates
        pod.boostport.com/vault-approle: default-default
    spec:
      securityContext:
        runAsUser: 1999
        runAsGroup: 1999
        fsGroup: 1999
      # The init containers will follow the queue from the zookeeper for the node to run on.
      serviceAccount: itomdipulsar-broker-sa
      serviceAccountName: itomdipulsar-broker-sa

      initContainers:
      - name: waitfor-itom-vault-8200
        image: atf.intranet.bb.com.br:5001/hpeswitom/opensuse-base:15.3-0032
        imagePullPolicy: IfNotPresent
        command: [ "sh", "-c", "until nc -z itom-vault 8200 -w 5 ; do echo waiting for itom-vault:8200...; sleep 5; done; exit 0"]
        resources: {}
        securityContext:
          runAsNonRoot: true
          runAsUser: 1999
          runAsGroup: 1999
      - name: generate-certificates
        image: atf.intranet.bb.com.br:5001/hpeswitom/kubernetes-vault-init:0.15.0-0038
        imagePullPolicy: IfNotPresent
        env:
          - name: CERT_COMMON_NAME
            value: "itomdipulsar-zookeeper"
        volumeMounts:
        - name: vault-token
          mountPath: /var/run/secrets/boostport.com

      # This initContainer will wait for bookkeeper initnewcluster to complete
      # before initializing pulsar metadata
      - name: pulsar-bookkeeper-verify-clusterid
        image: atf.intranet.bb.com.br:5001/hpeswitom/itom-pulsar-core:2.8.1-26
        imagePullPolicy: IfNotPresent
        command: ["sh", "-c"]
        args:
        - |
          source bin/coso-init.sh;
          bin/apply-config-from-env.py conf/bookkeeper.conf;
          until bin/bookkeeper shell whatisinstanceid; do
            sleep 3;
          done;
        envFrom:
        - configMapRef:
            name: "itomdipulsar-bookkeeper"
        volumeMounts:
        - name: cosoinit
          mountPath: "/pulsar/bin/coso-init.sh"
          subPath: coso-init.sh
        
        - name: keytool
          mountPath: "/pulsar/keytool/keytool.sh"
          subPath: keytool.sh
        - name: tmp
          mountPath: /pulsar/tmp
        - name: conf
          mountPath: /pulsar/conf
        - name: logs
          mountPath: /pulsar/logs
        - name: ssl
          mountPath: /pulsar/ssl
        - name: vault-token
          mountPath: /var/run/secrets/boostport.com
      containers:
      - name: "itomdipulsar-minio-connector-delete"
        image: atf.intranet.bb.com.br:5001/hpeswitom/itom-pulsar-core:2.8.1-26
        imagePullPolicy: IfNotPresent
        command: ["sh", "-c"]
        args:
          - |
            #!/bin/sh
            DI_TENANT=provider
            PULSAR_TENANT="public"
            source bin/coso-init.sh
            bin/apply-config-from-env.py conf/client.conf conf/bookkeeper.conf
            bin/pulsar-admin sources get  --name ${DI_TENANT} --tenant ${PULSAR_TENANT}
            if [ $? == 0 ]
            then
              bin/pulsar-admin sources delete --name ${DI_TENANT} --tenant ${PULSAR_TENANT}
            else
              echo "No sources found with name ${DI_TENANT} for delete  "
            fi
        envFrom:
        - configMapRef:
            name: "itomdipulsar-broker-post-upgrade"
        volumeMounts:
        - name: cosoinit
          mountPath: "/pulsar/bin/coso-init.sh"
          subPath: coso-init.sh
        - name: tmp
          mountPath: /pulsar/tmp
        - name: conf
          mountPath: /pulsar/conf
        - name: logs
          mountPath: /pulsar/logs
        - name: ssl
          mountPath: /pulsar/ssl
        - name: vault-token
          mountPath: /var/run/secrets/boostport.com
        
        - name: keytool
          mountPath: "/pulsar/keytool/keytool.sh"
          subPath: keytool.sh
      volumes:
      - name: cosoinit
        configMap:
          name: "itomdipulsar-cosoinit-configmap"
          defaultMode: 0755
      - name: tmp
        emptyDir: {}
      - name: conf
        emptyDir: {}
      - name: ssl
        emptyDir: {}
      - name: logs
        emptyDir: {}
      - name: vault-token
        emptyDir: {}
      
      - name: keytool
        configMap:
          name: "itomdipulsar-keytool-configmap"
          defaultMode: 0755
      restartPolicy: Never
