dipostload:
  probe:
    taskController:
      liveness:
        enabled: true
        initialDelaySeconds: 420
        periodSeconds: 20
        timeoutSeconds: 5
        failureThreshold: 3
        successThreshold: 1

      readiness:
        enabled: true
        initialDelaySeconds: 30
        periodSeconds: 20
        timeoutSeconds: 5
        failureThreshold: 18
        successThreshold: 1

    taskExecutor:
      liveness:
        enabled: true
        initialDelaySeconds: 420
        periodSeconds: 20
        timeoutSeconds: 5
        failureThreshold: 3
        successThreshold: 1

      readiness:
        enabled: true
        initialDelaySeconds: 30
        periodSeconds: 20
        timeoutSeconds: 5
        failureThreshold: 18
        successThreshold: 1
  topologySpreadConstraints:
    taskExecutor:
      enabled: true
      maxSkew: 1
      topologyKey: "topology.kubernetes.io/zone"
      whenUnsatisfiable: ScheduleAnyway
  config:
    taskGenerator:
      replicaCount: "1"
      jvmArgs: "-Xms256m -Xmx1024m -XX:+HeapDumpOnOutOfMemoryError"
      logConfigFromConfigmap: true

    taskExecutor:
      replicaCount: "1"
      jvmArgs: "-Xms256m -Xmx1024m -XX:+HeapDumpOnOutOfMemoryError"
      logConfigFromConfigmap: true

    postload:
      taskTopic: "di_postload_task_topic"
      statusTopic: "di_postload_task_status_topic"
      stateTopic: "di_internal_postload_state"
      taskExecutionIntervalMillis: "60000"
      consumersPerTaskExecutor: "20"
      useReceiveToConsume: "true"
      enableTopicMonitoring: "false"
      enableTeProcessMonitoring : "true"
      timeZone: ""
      enrichmentBatchSize: "2000000000"
      postResourcePool: ""
      # value should be same as prometheus scrape interval specified by scrape_interval in itomdimonitoring-prometheus config map
      scrapeIntervalMillis: "15000"
      pulsarNamespace: "itomdipostload"
      acceptableMissedTriggerDelaySeconds: "7200"


    configServer:
      hostname: "itom-di-administration-svc"
      port: "18443"
      connectRetryDelayMs: "30000"
      clientHeartBeat: "20000"
      serverHeartBeat: "30000"
      messageBufferSizeLimitInMb: "2"

    csvdirectload:
      #csv directload configuration
      compressArchiveFiles: "true"
      compressFailedFiles: "true"
      #set the below flag to false if you want to compress the archived files periodically instead of compressing each file
      compressEachFile: "true"
      cleanupRetentionPeriodDays: "1"
      cleanupRetentionSizeMb: "-1" # retention size in MB, set to -1 to disable

    pulsar:
      serviceName: "itomdipulsar-broker"
      kopServiceName: "itomdipulsar-proxy"
      namespace: "default"
      tenant: "public"
      brokerServicePort: "6650"
      brokerServicePortTls: "6651"
      webServicePort: "8080"
      webServicePortTls: "8443"
      tlsEnable: "true"
      authEnable: "true"
      authClass: "org.apache.pulsar.client.impl.auth.AuthenticationTls"
      tlsHostnameVerification: "false"
      connectionRetryIntervalSeconds: "30"
    vertica:
      connectionRetryIntervalSeconds: "30"

global:
  cluster:
    k8sProvider: cdf
  di:
    tenant: "provider"
    deployment: "default"
    logging:
      useFile: true
  # persistence.enabled=true means that the PVCs are expected to be dynamically created by the composition chart.
  # Otherwise, persistence.dataVolumeClaim is a persistent volume claim for storing data files.
  # persistence.logVolumeClaim is a persistent volume claim for storing log files.
  # If all of the above are undefined, then temporary ephemeral storage will be created (only if isDemo=true)

  persistence:
    enabled: false
    configVolumeClaim:
    logVolumeClaim:
    dataVolumeClaim:


  # RBAC 
  rbac:
    serviceAccountCreate: true
    roleCreate: true

  nodeSelector: {}
  # For custom server CAs (incl. for Vertica). tlsTruststore should be the name of a configMap which contains the certs.
  tlsTruststore:

  # If deployPrometheusConfig is true, CDF Monitoring Framework is assumed to be installed and will use it for metric storage
  prometheus:
    deployPrometheusConfig: true
    prometheusSelector:
      prometheus_config: "1"
    scrapeCertSecretName: "itom-di-prometheus-scrape-cert"

  # If isDemo is true, this will allow use of ephemeral storage and other POC aspects.
  isDemo: false

  instance:
    kafka:
    pulsar: pulsar

  pulsar:
#    serviceName:
    namespace: "default"
    tenant: "public"
    isKopEnabled: false
#    webServicePort:
#    webServicePortTls:
#    brokerServicePort:
#    brokerServicePortTls:
#    tlsEnable: "false"
#    authEnable: "false"
#    authClass: "org.apache.pulsar.client.impl.auth.AuthenticationTls"
#    tlsHostnameVerification: "false"

  #messageBus: "kafka"

  vertica:
    embedded: false
    host:
    rwuser:
    rwuserkey: ITOMDI_DBA_PASSWORD_KEY
    rouser:
    rouserkey: ITOMDI_DBA_PASSWORD_KEY
    db:
    port:
    tlsEnabled: "true"
    tlsMode: "verify_full"
    #purgeAfterDelete: "false"


  docker:
    # set the global.docker.registry and orgName to your internal Docker registry/org
    registry: localhost:5000
    orgName: hpeswitom
    imagePullSecret: ""
    imagePullPolicy: IfNotPresent

  securityContext:
      user: "1999"
      group: "1999"
      groupVault: "0"
      fsGroup: "1999"

  # Global image references for vault
  vaultRenew:
    #registry: localhost:5000
    #orgName: hpeswitom
    image: kubernetes-vault-renew
    imageTag: 0.10.0-0019
  vaultInit:
    #registry: localhost:5000
    #orgName: hpeswitom
    image: kubernetes-vault-init
    imageTag: 0.10.0-0019
  busybox:
    image: itom-busybox
    imageTag: 1.32.0-006

resources:
  taskGenerator:
    limits:
      cpu: "4"
      memory: "1024Mi"
    requests:
      cpu: "0.5"
      memory: "512Mi"
  taskExecutor:
    limits:
      cpu: "4"
      memory: "2048Mi"
    requests:
      cpu: "0.5"
      memory: "512Mi"


affinity:
  taskGenerator:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
              - key: "app"
                operator: In
                values:
                  - itom-di-postload-processor-taskgenerator
          topologyKey: "kubernetes.io/hostname"
  taskExecutor:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            labelSelector:
              matchExpressions:
                - key: "app"
                  operator: In
                  values:
                    - itom-di-postload-processor-taskexecutor
            topologyKey: "kubernetes.io/hostname"
  # Global image references

nodeSelector: {}
deployment:
  rbac:
    serviceAccount: ""

#####################
# Image definitions
#####################

postload:
  taskGenerator:
    image: itom-data-ingestion-postload-taskcontroller
    imageTag: 2.5.0-50
  taskExecutor:
    image: itom-data-ingestion-postload-taskexecutor
    imageTag: 2.5.0-50
enrichment:
  image: itom-data-ingestion-enrichment
  imageTag: 2.5.0-14
customTaskType:
  # To plugin custom task type set enabled to true and
  # replace CUSTOM_TASK_TYPE_IMG_NAME, CUSTOM_TASK_TYPE_IMG_TAG with the custom task type image name and tag.
  # Configure registry and orgName if you want to override the corresponding global docker values.
  enabled: false
  #registry: localhost:5000
  #orgName: hpeswitom
  image: CUSTOM_TASK_TYPE_IMG_NAME
  imageTag: CUSTOM_TASK_TYPE_IMG_TAG

